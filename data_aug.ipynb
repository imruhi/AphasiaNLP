{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [],
   "source": [
    "# task: turn text into non-fluent aphasic speech, based on Misra et al (2022) \n",
    "# current: basic code, maybe try to speed up transformation"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T15:52:37.759039800Z",
     "start_time": "2024-01-21T15:52:37.722539Z"
    }
   },
   "id": "6e5928ec082fcfd2"
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "import numpy as np\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-21T15:52:37.291884300Z"
    }
   },
   "id": "53d51e52434633de"
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [],
   "source": [
    "text1 = \"I thought I was going to finish the 3rd season of the Wire tonight.\"\n",
    "#  But there was a commentary on episode 11, so I had to re-watch Middle Ground with the commentary. Hopefully I can finish the season next weekend.\n",
    "\n",
    "vp_pattern = [ [{'POS': 'VERB', 'OP': '?'},\n",
    "           {'POS': 'ADV', 'OP': '*'},\n",
    "           {'POS': 'AUX', 'OP': '*'},\n",
    "           {'POS': 'VERB', 'OP': '+'}] ] \n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"Verb phrase\", vp_pattern)\n",
    "\n",
    "doc1 = nlp(text1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-21T15:52:37.694826900Z"
    }
   },
   "id": "19cdffd5065cd02"
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "def aphasic_speech(text, doc):\n",
    "    n = 15\n",
    "    aphasic_utt = \"\"\n",
    "    \n",
    "    if len(text.split()) <= n:  \n",
    "        # get NPs\n",
    "        noun_phrases = set()\n",
    "        for nc in doc.noun_chunks:\n",
    "            for nop in [nc, doc[nc.root.left_edge.i:nc.root.right_edge.i+1]]:\n",
    "                noun_phrases.add(nop.text.strip())                   \n",
    "        print(noun_phrases)      \n",
    "        \n",
    "        # get VPs\n",
    "        verb_phrases = matcher(doc)\n",
    "        verb_phrases = [doc[start:end] for _, start, \n",
    "                        end in verb_phrases]\n",
    "        print(verb_phrases)\n",
    "        \n",
    "        ratio = len(noun_phrases)/len(verb_phrases)\n",
    "        print(ratio)\n",
    "        \n",
    "        X = np.random.uniform(0,1)\n",
    "        \n",
    "        if ratio > 2 and X <= 0.8:\n",
    "            # skip sentence\n",
    "            return aphasic_utt\n",
    "        \n",
    "        else:\n",
    "            # dont skip sentence\n",
    "            for tok in doc:\n",
    "                if tok.dep_ in [\"det\", \"prep\", \"cop\", \"aux\"]: \n",
    "                    # determiners, prepositions, copulas\n",
    "                    Y = np.random.uniform(0,1)\n",
    "                    if Y > 0.9:\n",
    "                        aphasic_utt += tok.text + \" \"\n",
    "                        \n",
    "                elif tok.pos_ in [\"ADJ\", \"ADV\"]:      \n",
    "                    # adjectives, adverbs\n",
    "                    Z = np.random.uniform(0,1)\n",
    "                    if Z > 0.5:\n",
    "                        aphasic_utt += tok.text + \" \"\n",
    "                \n",
    "                elif tok.pos_ == \"VERB\":\n",
    "                    # verbs\n",
    "                    aphasic_utt += tok.lemma_ + \" \"\n",
    "                \n",
    "                else:\n",
    "                    # all other pos\n",
    "                    aphasic_utt += tok.text + \" \"\n",
    "    \n",
    "    return aphasic_utt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T15:52:37.759039800Z",
     "start_time": "2024-01-21T15:52:37.741936300Z"
    }
   },
   "id": "941a3422465576bd"
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "data": {
      "text/plain": "'auxiliary'"
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('aux')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T15:52:37.768746800Z",
     "start_time": "2024-01-21T15:52:37.753816900Z"
    }
   },
   "id": "970a9c7b4bbe7d21"
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the Wire', 'the 3rd season of the Wire', 'the 3rd season', 'I'}\n",
      "[thought, was going, going, finish]\n",
      "1.0\n",
      "I think I go finish season Wire tonight . \n"
     ]
    }
   ],
   "source": [
    "print(aphasic_speech(text1, doc1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T15:52:37.812997Z",
     "start_time": "2024-01-21T15:52:37.769757900Z"
    }
   },
   "id": "a3402831c9362bbc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
