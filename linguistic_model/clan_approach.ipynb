{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "##### NOTE: In CLAN the C-NNLA command was used to get the distribution parameters. However, error outputs from C-NNLA were not very useful (like % correct ..., % incorrect, % grammatical etc) since some CHA files do not annotate errors like [* p], [* s] (but just replace it with xxx, yyy, zzz), making those measures unreliable\n",
    "\n",
    "##### However, I am trying to include the 5 most common word errors from the C-NNLA measures and basing their occurance on the Salis and Edwards (2004) paper\n",
    "\n",
    "##### This approach also does not consider any phonetic errors/substitutions (marked as @u or [* p])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4fa55da3a7037ee"
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from canonical_sents import get_canonical_sentences\n",
    "import os\n",
    "import scipy.stats as stats\n",
    "import random\n",
    "import re\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:32:16.662541200Z",
     "start_time": "2024-02-27T20:32:16.204279900Z"
    }
   },
   "id": "fd32d434bf5e298a"
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "pronouns = {'Art': ['a', 'an', 'the', ''],\n",
    "           'Dem': ['this', 'that', 'these', 'those', ''],\n",
    "           'Poss': ['my', 'your', 'his', 'her', 'its', 'our', 'their', '']}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:32:16.676860100Z",
     "start_time": "2024-02-27T20:32:16.664540800Z"
    }
   },
   "id": "6d1fac05d4a9f743"
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [],
   "source": [
    "def pro_sub(pro):\n",
    "    for _, pro1 in pronouns.items():\n",
    "        if pro.lower() in pro1:\n",
    "            y = [j for j in pro1 if pro!=j]\n",
    "            return random.choice(y)\n",
    "    return \"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:32:16.703874700Z",
     "start_time": "2024-02-27T20:32:16.678860600Z"
    }
   },
   "id": "53da41a352bbf2cf"
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [],
   "source": [
    "def get_truncnorm(mean, std, min, max):\n",
    "    # mean, std, min, max parameters dependent on their extracted normal\n",
    "    # distributions\n",
    "    a, b = (min - mean) / std, (max - mean) / std\n",
    "    return stats.truncnorm(a, b, loc=mean, \n",
    "                                   scale=std).rvs(size=1)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:32:16.716628500Z",
     "start_time": "2024-02-27T20:32:16.692860500Z"
    }
   },
   "id": "f2f021bc59a96b52"
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [],
   "source": [
    "def get_curr_nv_ratio(nouns, verbs):\n",
    "    if len(nouns) != 0 and len(verbs) != 0:\n",
    "        curr_ratio_nv = len(nouns)/len(verbs)\n",
    "    else:\n",
    "        curr_ratio_nv = 0\n",
    "    return curr_ratio_nv"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:32:16.726627500Z",
     "start_time": "2024-02-27T20:32:16.710109500Z"
    }
   },
   "id": "349210d24a5ce11d"
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "# TODO: add verb form handling \n",
    "def aphasic_speech(text):\n",
    "    doc = nlp(text)\n",
    "    # max length of modified sentences (log norm dist with log(27) being longest)\n",
    "    # determine how long this sentence will be\n",
    "    n = np.random.lognormal(mean=0.9162787, sigma=0.7350323)\n",
    "    while n > 3.295:\n",
    "        n = np.random.lognormal(mean=0.9162787, sigma=0.7350323)\n",
    "        \n",
    "    utt = \"\"\n",
    "    # length of original text\n",
    "    length = len(re.findall(\"[a-zA-Z_]+\", text))\n",
    "    \n",
    "    if length != 0:\n",
    "        # get possible n/v ratio for this sentence\n",
    "        # n/v ratio is log-norm\n",
    "        ratio_nv = get_truncnorm(1.461698, 1, 0.228, 6.818)\n",
    "        # get the possible percentage of all POS\n",
    "        percent_noun = get_truncnorm(20.68179, 9.794099, 5.991, 50)\n",
    "        percent_verb = get_truncnorm(16.59888, 5.036936, 0, 26.267)\n",
    "        percent_det = get_truncnorm(7.494686, 5.927496, 0, 27.79661)\n",
    "        percent_prep = get_truncnorm(3.104844, 2.325505, 0, 15.05682)\n",
    "        percent_adj = get_truncnorm(4.34104, 3.509376, 0, 21.05263)\n",
    "        percent_adv = get_truncnorm(5.751157, 2.958618, 0, 15.88448)\n",
    "        nouns = []\n",
    "        verbs = []\n",
    "        determiners = []\n",
    "        prepositions = []\n",
    "        adjectives = []\n",
    "        adverbs = []\n",
    "        \n",
    "        # count no. of respective POS\n",
    "        for tok in doc:\n",
    "            if tok.pos_ == \"NOUN\":\n",
    "                nouns.append(tok.text)\n",
    "            elif tok.pos_ == \"VERB\" or tok.dep_ == \"cop\" or tok.tag_ in [\"VBD\", \"VBN\"]:\n",
    "                verbs.append(tok.text)\n",
    "            elif tok.dep_ == \"det\":\n",
    "                determiners.append(tok.text)\n",
    "            elif tok.dep_ == \"prep\":\n",
    "                prepositions.append(tok.text)\n",
    "            elif tok.pos_ == \"ADJ\":\n",
    "                adjectives.append(tok.text)\n",
    "            elif tok.pos_ == \"ADV\":\n",
    "                adverbs.append(tok.text)\n",
    "                \n",
    "        for tok in doc:\n",
    "            # possible percentage of keeping respective POS\n",
    "            curr_ratio_nv = get_curr_nv_ratio(nouns, verbs)\n",
    "            \n",
    "            if tok.pos_ == \"NOUN\": \n",
    "                # if possible noun percent in sentence less than current\n",
    "                # percent or if current n/v ratio is too big, remove noun \n",
    "                # from sentence\n",
    "                if (percent_noun <= (len(nouns)/length) * 100 \n",
    "                        or curr_ratio_nv > ratio_nv) :\n",
    "                    utt += ' '\n",
    "                    nouns.remove(tok.text)\n",
    "                else:\n",
    "                    utt += tok.text + ' '\n",
    "                \n",
    "            elif  tok.pos_ == \"VERB\" or tok.dep_ == \"cop\" or tok.tag_ in [\"VBD\", \"VBN\"]:\n",
    "                # if possible verb percent in sentence less than current \n",
    "                # percent or if current n/v ratio too big remove noun \n",
    "                # from sentence\n",
    "                if (percent_verb <= (len(verbs)/length) * 100 \n",
    "                        or curr_ratio_nv > ratio_nv):\n",
    "                   utt += ' '\n",
    "                   verbs.remove(tok.text)\n",
    "                else:\n",
    "                    utt += tok.text + ' '\n",
    "                    \n",
    "            elif tok.dep_ == \"det\":\n",
    "                if percent_det <= (len(determiners)/length) * 100:\n",
    "                   utt += ' '\n",
    "                   determiners.remove(tok.text)\n",
    "                else:\n",
    "                    utt += tok.text + ' '\n",
    "                    \n",
    "            elif tok.dep_ == \"prep\":\n",
    "                if percent_prep <= (len(prepositions)/length) * 100:\n",
    "                   utt += ' '\n",
    "                   prepositions.remove(tok.text)\n",
    "                else:\n",
    "                    utt += tok.text + ' '\n",
    "                    \n",
    "            elif tok.pos_ == \"ADJ\":\n",
    "                if percent_adj <= (len(adjectives)/length) * 100:\n",
    "                   utt += ' '\n",
    "                   adjectives.remove(tok.text)\n",
    "                else:\n",
    "                    utt += tok.text + ' '\n",
    "                    \n",
    "            elif tok.pos_ == \"ADV\":\n",
    "                if percent_adj <= (len(adverbs)/length) * 100:\n",
    "                   utt += ' '\n",
    "                   adverbs.remove(tok.text)\n",
    "                else:\n",
    "                    utt += tok.text + ' '\n",
    "                    \n",
    "            elif tok.pos_ in [\"DET\", \"PRON\"]:\n",
    "                # based on Salis and Edwards (2004)\n",
    "                # CLAN error code s:r:gc:pro\n",
    "                x = np.random.uniform(0,1)\n",
    "                                   \n",
    "                # 3% of determiners were substituted\n",
    "                if x >= 0.97:\n",
    "                    if tok.pos_ == \"DET\" or \"Dem\" in tok.morph.get('PronType') or \"Yes\" in tok.morph.get('Poss'):\n",
    "                        utt += pro_sub(tok.text) + \" \"\n",
    "                # 19% determiners were omitted\n",
    "                elif x >= 0.81:\n",
    "                    utt += \" \"\n",
    "                else:\n",
    "                    utt += tok.text + ' '\n",
    "           \n",
    "           # all other words with respective POS unaffected\n",
    "            else:\n",
    "                utt += tok.text + ' '\n",
    "\n",
    "        utt = \" \".join(utt.split()) # remove trailing whitespaces\n",
    "        utt = re.sub(r'\\s+([?.!\",])', r'\\1', utt)\n",
    "        # print(\"Possible utt: \", utt)\n",
    "        # only return sentences which are short enough\n",
    "        if np.exp(n) >= len(re.findall(\"[a-zA-Z_]+\", utt)) and len(re.findall(\"[a-zA-Z_]+\", utt)) <= 27:\n",
    "            return utt, True\n",
    "        else:\n",
    "            return '', False\n",
    "        \n",
    "    # skipped sentence due to length = 0\n",
    "    else:\n",
    "        return \"\", False\n",
    "    \n",
    "def augment(filepath, save_path, include_canonical=False):\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:32:16.745723400Z",
     "start_time": "2024-02-27T20:32:16.732628400Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final utter: I the brass that you sent me, and all looks.\n",
      "-----------------------------------------------\n",
      "Final utter: you much your trouble and extra 3 pieces.\n",
      "-----------------------------------------------\n",
      "-----------------------------------------------\n",
      "-----------------------------------------------\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "para = \"\"\"I received the brass that you sent me, and all looks well. Thank you very much for all your trouble and the extra 3 pieces. I feel that you have an outstanding company and are striving the best that you can to achieve customer satisfaction. I will be certain to tell my friends about US Reloading Supply. ~ Michelle Admin Note: By accident we shorted them two shells, so we sent them replacements.\"\"\"\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentences = sent_tokenize(para)\n",
    "for sent in sentences:\n",
    "    aphasic, changed = aphasic_speech(sent)\n",
    "    if changed:\n",
    "        print(\"Final utter:\", \" \".join(aphasic.split()).strip())\n",
    "        print(\"-----------------------------------------------\")\n",
    "    else:\n",
    "        print(\"-----------------------------------------------\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:32:16.805163200Z",
     "start_time": "2024-02-27T20:32:16.741730Z"
    }
   },
   "id": "82580e7ec167ce03"
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Her poss PRON PRP$\n",
      "flowers nsubj NOUN NNS\n",
      "are ROOT AUX VBP\n",
      "pretty acomp ADV RB\n"
     ]
    }
   ],
   "source": [
    "sent = \"Her flowers are pretty\"\n",
    "\n",
    "for tok in nlp(sent):\n",
    "    print(tok.text, tok.dep_, tok.pos_, tok.tag_)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:32:16.847175800Z",
     "start_time": "2024-02-27T20:32:16.804163700Z"
    }
   },
   "id": "5dd30f4c9175db25"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test on 10k C4"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91db62f38951bdb0"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "ds = load_dataset('stas/c4-en-10k')\n",
    "print(ds)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T21:05:17.677435100Z",
     "start_time": "2024-02-27T21:05:10.913495400Z"
    }
   },
   "id": "254d6418351d0f9"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m sents \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m text \u001B[38;5;129;01min\u001B[39;00m texts:\n\u001B[1;32m----> 4\u001B[0m     sentences \u001B[38;5;241m=\u001B[39m \u001B[43msent_tokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m sent \u001B[38;5;129;01min\u001B[39;00m sentences:\n\u001B[0;32m      6\u001B[0m         sents\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(sent\u001B[38;5;241m.\u001B[39msplit()))\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\thesis\\lib\\site-packages\\nltk\\tokenize\\__init__.py:107\u001B[0m, in \u001B[0;36msent_tokenize\u001B[1;34m(text, language)\u001B[0m\n\u001B[0;32m     97\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     98\u001B[0m \u001B[38;5;124;03mReturn a sentence-tokenized copy of *text*,\u001B[39;00m\n\u001B[0;32m     99\u001B[0m \u001B[38;5;124;03musing NLTK's recommended sentence tokenizer\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    104\u001B[0m \u001B[38;5;124;03m:param language: the model name in the Punkt corpus\u001B[39;00m\n\u001B[0;32m    105\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    106\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m load(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtokenizers/punkt/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlanguage\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.pickle\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 107\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\thesis\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1281\u001B[0m, in \u001B[0;36mPunktSentenceTokenizer.tokenize\u001B[1;34m(self, text, realign_boundaries)\u001B[0m\n\u001B[0;32m   1277\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtokenize\u001B[39m(\u001B[38;5;28mself\u001B[39m, text: \u001B[38;5;28mstr\u001B[39m, realign_boundaries: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[0;32m   1278\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1279\u001B[0m \u001B[38;5;124;03m    Given a text, returns a list of the sentences in that text.\u001B[39;00m\n\u001B[0;32m   1280\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1281\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msentences_from_text\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrealign_boundaries\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\thesis\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1341\u001B[0m, in \u001B[0;36mPunktSentenceTokenizer.sentences_from_text\u001B[1;34m(self, text, realign_boundaries)\u001B[0m\n\u001B[0;32m   1332\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msentences_from_text\u001B[39m(\n\u001B[0;32m   1333\u001B[0m     \u001B[38;5;28mself\u001B[39m, text: \u001B[38;5;28mstr\u001B[39m, realign_boundaries: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m   1334\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[0;32m   1335\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1336\u001B[0m \u001B[38;5;124;03m    Given a text, generates the sentences in that text by only\u001B[39;00m\n\u001B[0;32m   1337\u001B[0m \u001B[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001B[39;00m\n\u001B[0;32m   1338\u001B[0m \u001B[38;5;124;03m    True, includes in the sentence closing punctuation that\u001B[39;00m\n\u001B[0;32m   1339\u001B[0m \u001B[38;5;124;03m    follows the period.\u001B[39;00m\n\u001B[0;32m   1340\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1341\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [text[s:e] \u001B[38;5;28;01mfor\u001B[39;00m s, e \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\thesis\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1341\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m   1332\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msentences_from_text\u001B[39m(\n\u001B[0;32m   1333\u001B[0m     \u001B[38;5;28mself\u001B[39m, text: \u001B[38;5;28mstr\u001B[39m, realign_boundaries: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m   1334\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[0;32m   1335\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1336\u001B[0m \u001B[38;5;124;03m    Given a text, generates the sentences in that text by only\u001B[39;00m\n\u001B[0;32m   1337\u001B[0m \u001B[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001B[39;00m\n\u001B[0;32m   1338\u001B[0m \u001B[38;5;124;03m    True, includes in the sentence closing punctuation that\u001B[39;00m\n\u001B[0;32m   1339\u001B[0m \u001B[38;5;124;03m    follows the period.\u001B[39;00m\n\u001B[0;32m   1340\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1341\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [text[s:e] \u001B[38;5;28;01mfor\u001B[39;00m s, e \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\thesis\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1329\u001B[0m, in \u001B[0;36mPunktSentenceTokenizer.span_tokenize\u001B[1;34m(self, text, realign_boundaries)\u001B[0m\n\u001B[0;32m   1327\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m realign_boundaries:\n\u001B[0;32m   1328\u001B[0m     slices \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_realign_boundaries(text, slices)\n\u001B[1;32m-> 1329\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m sentence \u001B[38;5;129;01min\u001B[39;00m slices:\n\u001B[0;32m   1330\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m (sentence\u001B[38;5;241m.\u001B[39mstart, sentence\u001B[38;5;241m.\u001B[39mstop)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\thesis\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1461\u001B[0m, in \u001B[0;36mPunktSentenceTokenizer._realign_boundaries\u001B[1;34m(self, text, slices)\u001B[0m\n\u001B[0;32m   1459\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m sentence1, sentence2 \u001B[38;5;129;01min\u001B[39;00m _pair_iter(slices):\n\u001B[0;32m   1460\u001B[0m     sentence1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mslice\u001B[39m(sentence1\u001B[38;5;241m.\u001B[39mstart \u001B[38;5;241m+\u001B[39m realign, sentence1\u001B[38;5;241m.\u001B[39mstop)\n\u001B[1;32m-> 1461\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m sentence2:\n\u001B[0;32m   1462\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m text[sentence1]:\n\u001B[0;32m   1463\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m sentence1\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "texts = ds[\"train\"][\"text\"]\n",
    "sents = []\n",
    "for text in texts:\n",
    "    sentences = sent_tokenize(text)\n",
    "    for sent in sentences:\n",
    "        sents.append(\" \".join(sent.split()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T21:05:19.730140500Z",
     "start_time": "2024-02-27T21:05:17.666780900Z"
    }
   },
   "id": "45c43d9f2a09e8a7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(sents)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-27T21:05:19.722824100Z"
    }
   },
   "id": "4c7a6c44422921ff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_sents = sents[:1000]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-27T21:05:19.726075Z"
    }
   },
   "id": "8eb950222550ce9e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "aphasic_sents = []\n",
    "for sent in test_sents:\n",
    "    aphasic_speech(sent)\n",
    "    aphasic, changed = aphasic_speech(sent)\n",
    "    if changed and aphasic !=\".\":\n",
    "        print(aphasic)\n",
    "        aphasic_sents.append(aphasic)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-27T21:05:19.729084700Z"
    }
   },
   "id": "7b116c75e017a103"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.DataFrame(data={\"modified\": aphasic_sents}).to_csv(\"synthetic.csv\", sep=\",\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-27T21:05:19.731179300Z"
    }
   },
   "id": "e4ba83ae8965e8c6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "aphasic_sents"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-27T21:05:19.733178900Z"
    }
   },
   "id": "7034bde713f32ba5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Post process of aphasic sentences\n",
    "Also adding some \"control\" sentences"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ab67054aaaee979"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset('stas/c4-en-10k')\n",
    "print(ds)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T21:08:50.937868900Z",
     "start_time": "2024-02-27T21:08:45.539725100Z"
    }
   },
   "id": "a232e0b1491df977"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"synthetic.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T21:08:50.987372600Z",
     "start_time": "2024-02-27T21:08:50.943197600Z"
    }
   },
   "id": "b979318b075b845f"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "sentences = df['modified']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T21:08:51.003315800Z",
     "start_time": "2024-02-27T21:08:50.956988200Z"
    }
   },
   "id": "346435140a64874a"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "0             Beginners BBQ Class Taking Place Missoula!\n1                          You will, this your calendar.\n2      Thursday, September 22nd join World Class BBQ ...\n3                        I 've 500 drive and 240 gb SSD.\n4                          I 've SSD and I up SSD drive.\n                             ...                        \n377                                            Grace is.\n378                    the stomach, and it takes breath.\n379    I God ’s our, getting married, and living our ...\n380    it feels a raging fight going on me, making de...\n381                                    Where does Grace?\nName: modified, Length: 382, dtype: object"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T21:08:51.003315800Z",
     "start_time": "2024-02-27T21:08:50.971448100Z"
    }
   },
   "id": "2995138dc0660fcb"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import re\n",
    "remove_startswiths = [\" \", \",\", \"!\", \".\", \"?\", \".\", \"/\"]\n",
    "new_sents = []\n",
    "for sent in sentences:\n",
    "    sent = re.sub(r'\\s+([?.!\",\\'])', r'\\1', sent)\n",
    "    sent = re.sub(r\"^\\W+\", \"\", sent)\n",
    "    sent = re.sub(r\"\\(\\s*(.*?)\\s*\\)\",r'(\\1)', sent)\n",
    "    new_sents.append(sent)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T21:09:01.261709800Z",
     "start_time": "2024-02-27T21:09:01.245691100Z"
    }
   },
   "id": "6ea26a0b735f444c"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "texts = ds[\"train\"][\"text\"]\n",
    "sents = []\n",
    "for text in texts:\n",
    "    sentences = sent_tokenize(text)\n",
    "    for sent in sentences:\n",
    "        sents.append(\" \".join(sent.split()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T21:09:29.913975900Z",
     "start_time": "2024-02-27T21:09:25.538332900Z"
    }
   },
   "id": "e4682d97a579cf7e"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "control_sents = sents[1000:2000]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T21:09:33.534284100Z",
     "start_time": "2024-02-27T21:09:33.510966600Z"
    }
   },
   "id": "621e4424aaaa2c50"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "broca_data = pd.DataFrame(data={\"modified\": new_sents, \"label\": [1]*len(new_sents)})\n",
    "control_data = pd.DataFrame(data={\"modified\": control_sents, \"label\": [0]*len(control_sents)})\n",
    "data_full_scenario = pd.concat([broca_data, control_data], ignore_index=True)\n",
    "data_full_scenario = data_full_scenario.sample(frac=1).reset_index(drop=True)\n",
    "data_full_scenario.to_csv(\"gen_data.csv\", sep=\",\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T21:09:35.043517600Z",
     "start_time": "2024-02-27T21:09:35.022596300Z"
    }
   },
   "id": "de47ecaf3bc979de"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "                                               modified  label\n0     A tooth-colored composite placed on the lingua...      0\n1     Serve in bowls topped with toasted sesame seed...      0\n2     in Anthropology from Loyola University of Chic...      0\n3     Copper Chef is, and we are a look at their bak...      1\n4                 Disturbia Clothing coupon code gift !      0\n...                                                 ...    ...\n1377  It’s effective both for people who simply want...      0\n1378                                                13.      0\n1379                        EPA 21 MPG Hwy/15 MPG City!      0\n1380  The Security Council has adopted such statemen...      0\n1381                                                1σ.      1\n\n[1382 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>modified</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A tooth-colored composite placed on the lingua...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Serve in bowls topped with toasted sesame seed...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>in Anthropology from Loyola University of Chic...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Copper Chef is, and we are a look at their bak...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Disturbia Clothing coupon code gift !</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1377</th>\n      <td>It’s effective both for people who simply want...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1378</th>\n      <td>13.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1379</th>\n      <td>EPA 21 MPG Hwy/15 MPG City!</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1380</th>\n      <td>The Security Council has adopted such statemen...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1381</th>\n      <td>1σ.</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>1382 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_full_scenario"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T21:09:42.840090Z",
     "start_time": "2024-02-27T21:09:42.836027200Z"
    }
   },
   "id": "e18e6bd624f8eb25"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
