{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## NOTE: In CLAN the C-NNLA command was used to get the distribution parameters. However, error outputs from C-NNLA were not very useful (like % correct ..., % incorrect, % grammatical etc) since some CHA files do not annotate errors like [* p], [* s] (but just replace it with xxx, yyy, zzz), making those measures unreliable\n",
    "\n",
    "## This approach also does not consider any phonetic errors/substitutions (marked as @u or [* p]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4fa55da3a7037ee"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from canonical_sents import get_canonical_sentences\n",
    "import os\n",
    "import scipy.stats as stats\n",
    "import random\n",
    "import re\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T13:41:53.055012300Z",
     "start_time": "2024-02-27T13:41:43.305677Z"
    }
   },
   "id": "fd32d434bf5e298a"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def get_truncnorm(mean, std, min, max):\n",
    "    # mean, std, min, max parameters dependent on their extracted normal\n",
    "    # distributions\n",
    "    a, b = (min - mean) / std, (max - mean) / std\n",
    "    return stats.truncnorm(a, b, loc=mean, \n",
    "                                   scale=std).rvs(size=1)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T13:41:53.065817Z",
     "start_time": "2024-02-27T13:41:53.057021Z"
    }
   },
   "id": "f2f021bc59a96b52"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def get_curr_nv_ratio(nouns, verbs):\n",
    "    if len(nouns) != 0 and len(verbs) != 0:\n",
    "        curr_ratio_nv = len(nouns)/len(verbs)\n",
    "    else:\n",
    "        curr_ratio_nv = 0\n",
    "    return curr_ratio_nv"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T13:41:53.085828900Z",
     "start_time": "2024-02-27T13:41:53.069827900Z"
    }
   },
   "id": "349210d24a5ce11d"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# DONE noun and verb handling?\n",
    "# TODO: check distributions of sentence types and only allow those, handle determiners, articles, copulas, prepositions, adjectives, adverbs \n",
    "def aphasic_speech(text):\n",
    "    doc = nlp(text)\n",
    "    # max length of modified sentences \n",
    "    # determine how long this sentence will be\n",
    "    n = np.random.lognormal(mean=0.9162787, sigma=0.7350323)\n",
    "    while n > 3.295:\n",
    "        n = np.random.lognormal(mean=0.9162787, sigma=0.7350323)\n",
    "        \n",
    "    utt = \"\"\n",
    "    # length of original text\n",
    "    length = len(re.findall(\"[a-zA-Z_]+\", text))\n",
    "    \n",
    "    # TODO: remove when whole stuff implemented: max no. words in sentence in aphasiabank for brocas was 27\n",
    "    if length <= 27:\n",
    "        # get possible n/v ratio for this sentence\n",
    "        # n/v ratio is log-norm\n",
    "        ratio_nv = get_truncnorm(1.461698, 1, 0.228, 6.818)\n",
    "        # get the possible percentage of all POS\n",
    "        percent_noun = get_truncnorm(20.68179, 9.794099, 5.991, 50)\n",
    "        percent_verb = get_truncnorm(16.59888, 5.036936, 0, 26.267)\n",
    "        percent_det = get_truncnorm(7.494686, 5.927496, 0, 27.79661)\n",
    "        percent_prep = get_truncnorm(3.104844, 2.325505, 0, 15.05682)\n",
    "        percent_adj = get_truncnorm(4.34104, 3.509376, 0, 21.05263)\n",
    "        percent_adv = get_truncnorm(5.751157, 2.958618, 0, 15.88448)\n",
    "        nouns = []\n",
    "        verbs = []\n",
    "        determiners = []\n",
    "        prepositions = []\n",
    "        adjectives = []\n",
    "        adverbs = []\n",
    "        \n",
    "        # count no. of respective POS\n",
    "        for tok in doc:\n",
    "            if tok.pos_ == \"NOUN\":\n",
    "                nouns.append(tok.text)\n",
    "            elif tok.pos_ == \"VERB\" or tok.dep_ == \"cop\" or tok.tag_ in [\"VBD\", \"VBN\"]:\n",
    "                verbs.append(tok.text)\n",
    "            elif tok.dep_ == \"det\":\n",
    "                determiners.append(tok.text)\n",
    "            elif tok.dep_ == \"prep\":\n",
    "                prepositions.append(tok.text)\n",
    "            elif tok.pos_ == \"ADJ\":\n",
    "                adjectives.append(tok.text)\n",
    "            elif tok.pos_ == \"ADV\":\n",
    "                adverbs.append(tok.text)\n",
    "                \n",
    "        for tok in doc:\n",
    "            # possible percentage of keeping respective POS\n",
    "            \n",
    "            curr_ratio_nv = get_curr_nv_ratio(nouns, verbs)\n",
    "            \n",
    "            if tok.pos_ == \"NOUN\": \n",
    "                # if possible noun percent in sentence less than current\n",
    "                # percent or if current n/v ratio is too big, remove noun \n",
    "                # from sentence\n",
    "                if (percent_noun <= (len(nouns)/length) * 100 \n",
    "                        or curr_ratio_nv > ratio_nv) :\n",
    "                    utt += ' '\n",
    "                    nouns.remove(tok.text)\n",
    "                else:\n",
    "                    utt += tok.text + ' '\n",
    "                \n",
    "            elif  tok.pos_ == \"VERB\" or tok.dep_ == \"cop\" or tok.tag_ in [\"VBD\", \"VBN\"]:\n",
    "                # if possible verb percent in sentence less than current \n",
    "                # percent or if current n/v ratio too big remove noun \n",
    "                # from sentence\n",
    "                if (percent_verb <= (len(verbs)/length) * 100 \n",
    "                        or curr_ratio_nv > ratio_nv):\n",
    "                   utt += ' '\n",
    "                   verbs.remove(tok.text)\n",
    "                else:\n",
    "                    utt += tok.text + ' '\n",
    "                    \n",
    "            elif tok.dep_ == \"det\":\n",
    "                if percent_det <= (len(determiners)/length) * 100:\n",
    "                   utt += ' '\n",
    "                   determiners.remove(tok.text)\n",
    "                else:\n",
    "                    utt += tok.text + ' '\n",
    "                    \n",
    "            elif tok.dep_ == \"prep\":\n",
    "                if percent_prep <= (len(prepositions)/length) * 100:\n",
    "                   utt += ' '\n",
    "                   prepositions.remove(tok.text)\n",
    "                else:\n",
    "                    utt += tok.text + ' '\n",
    "                    \n",
    "            elif tok.pos_ == \"ADJ\":\n",
    "                if percent_adj <= (len(adjectives)/length) * 100:\n",
    "                   utt += ' '\n",
    "                   adjectives.remove(tok.text)\n",
    "                else:\n",
    "                    utt += tok.text + ' '\n",
    "                    \n",
    "            elif tok.pos_ == \"ADV\":\n",
    "                if percent_adj <= (len(adverbs)/length) * 100:\n",
    "                   utt += ' '\n",
    "                   adverbs.remove(tok.text)\n",
    "                else:\n",
    "                    utt += tok.text + ' '\n",
    "                    \n",
    "           # all other words with respective POS unaffected\n",
    "            else:\n",
    "                utt += tok.text + ' '\n",
    "\n",
    "        utt = \" \".join(utt.split()) # remove trailing whitespaces\n",
    "        utt = re.sub(r'\\s+([?.!\",])', r'\\1', utt)\n",
    "        print(\"Possible utt: \", utt)\n",
    "        # only return sentences which are short enough\n",
    "        if np.exp(n) >= len(re.findall(\"[a-zA-Z_]+\", utt)):\n",
    "            return utt, True\n",
    "        else:\n",
    "            return '', False\n",
    "        \n",
    "    # skipped sentence due to length of it being too big?\n",
    "    # TODO: remove when adding other stuff mentioned in todo at start\n",
    "    else:\n",
    "        return \"\", False\n",
    "    \n",
    "def augment(filepath, save_path, include_canonical=False):\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T13:41:53.171991200Z",
     "start_time": "2024-02-27T13:41:53.098159Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible utt:  I brass that you me, and all looks.\n",
      "-----------------------------------------------\n",
      "Possible utt:  you your trouble and 3 pieces.\n",
      "Final utter: you your trouble and 3 pieces.\n",
      "-----------------------------------------------\n",
      "Possible utt:  I that you have outstanding company and are striving the best that you can to achieve customer satisfaction.\n",
      "-----------------------------------------------\n",
      "Possible utt:  I will be to tell my friends US Reloading Supply.\n",
      "-----------------------------------------------\n",
      "Possible utt:  ~ Michelle Admin Note : we them two, we them replacements.\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "para = \"\"\"I received the brass that you sent me, and all looks well. Thank you very much for all your trouble and the extra 3 pieces. I feel that you have an outstanding company and are striving the best that you can to achieve customer satisfaction. I will be certain to tell my friends about US Reloading Supply. ~ Michelle Admin Note: By accident we shorted them two shells, so we sent them replacements.\"\"\"\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentences = sent_tokenize(para)\n",
    "for sent in sentences:\n",
    "    aphasic, changed = aphasic_speech(sent)\n",
    "    if changed:\n",
    "        print(\"Final utter:\", \" \".join(aphasic.split()).strip())\n",
    "        print(\"-----------------------------------------------\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T13:43:12.291680500Z",
     "start_time": "2024-02-27T13:43:12.215100800Z"
    }
   },
   "id": "82580e7ec167ce03"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All det DET DT\n",
      "flowers nsubj NOUN NNS\n",
      "are ROOT AUX VBP\n",
      "pretty acomp ADV RB\n"
     ]
    }
   ],
   "source": [
    "sent = \"All flowers are pretty\"\n",
    "\n",
    "for tok in nlp(sent):\n",
    "    print(tok.text, tok.dep_, tok.pos_, tok.tag_)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T13:41:54.010022900Z",
     "start_time": "2024-02-27T13:41:53.993904500Z"
    }
   },
   "id": "5dd30f4c9175db25"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
