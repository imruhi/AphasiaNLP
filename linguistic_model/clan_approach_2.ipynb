{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "##### NOTE: In CLAN the C-NNLA command was used to get the distribution parameters. However, error outputs from C-NNLA were not very useful (like % correct ..., % incorrect, % grammatical etc) since some CHA files do not annotate errors like [* p], [* s] (but just replace it with xxx, yyy, zzz), making those measures unreliable\n",
    "\n",
    "##### However, I am trying to include the 5 most common word errors from the C-NNLA measures \n",
    "\n",
    "##### This approach also does not consider any utterance errors/substitutions (marked as @u or [* p:n] in AphasiaBank)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4fa55da3a7037ee"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import random\n",
    "import re\n",
    "from pattern.text.en import singularize, pluralize, conjugate\n",
    "import enchant\n",
    "from preprocess import preprocess\n",
    "from spacy.matcher import Matcher\n",
    "from string import printable\n",
    "\n",
    "d = enchant.Dict(\"en_US\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T10:48:41.611238900Z",
     "start_time": "2024-04-10T10:48:34.959184200Z"
    }
   },
   "id": "fd32d434bf5e298a"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "dets = {'Art': ['a', 'an', 'the', ''],\n",
    "           'Dem': ['this', 'that', 'these', 'those', ''],\n",
    "           'Poss': ['my', 'your', 'his', 'her', 'its', 'our', 'their', '']}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T10:48:41.626809200Z",
     "start_time": "2024-04-10T10:48:41.611238900Z"
    }
   },
   "id": "6d1fac05d4a9f743"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def det_sub(x):\n",
    "    for _, det in dets.items():\n",
    "        if x.lower() in det:\n",
    "            y = [j for j in det if x!=j]\n",
    "            return random.choice(y)\n",
    "    return \"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T10:48:41.662889900Z",
     "start_time": "2024-04-10T10:48:41.630031600Z"
    }
   },
   "id": "53da41a352bbf2cf"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def get_truncnorm(mean, std, min, max):\n",
    "    # mean, std, min, max parameters dependent on their extracted normal\n",
    "    # distributions\n",
    "    a, b = (min - mean) / std, (max - mean) / std\n",
    "    return stats.truncnorm(a, b, loc=mean, \n",
    "                                   scale=std).rvs(size=1)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T10:48:41.664118Z",
     "start_time": "2024-04-10T10:48:41.640702700Z"
    }
   },
   "id": "f2f021bc59a96b52"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def get_curr_nv_ratio(nouns, verbs):\n",
    "    if len(nouns) != 0 and len(verbs) != 0:\n",
    "        curr_ratio_nv = len(nouns)/len(verbs)\n",
    "    else:\n",
    "        curr_ratio_nv = 0\n",
    "    return curr_ratio_nv"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T10:48:41.679850300Z",
     "start_time": "2024-04-10T10:48:41.663310300Z"
    }
   },
   "id": "349210d24a5ce11d"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def get_alt_word(tok):\n",
    "    # get a new word which is the same length as old word\n",
    "    # to simulate p:w errors\n",
    "    possible_words = [x for x in d.suggest(tok.text) if len(x) ==\n",
    "                      len(tok.text) and d.check(x) and x != tok.text] \n",
    "    if possible_words:\n",
    "        new_word = random.choice(possible_words)\n",
    "        if new_word:\n",
    "            return new_word\n",
    "    \n",
    "    # if we cannot find new word just return current word\n",
    "    return tok.text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T10:48:41.699629500Z",
     "start_time": "2024-04-10T10:48:41.685599Z"
    }
   },
   "id": "bb61dd53862bfa26"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "m0sa_lim = 0.7      # m:0s:a             (30%)\n",
    "ms_lim = 0.7        # m:+s(:a)           (30%)\n",
    "m0s_lim = 0.7       # m:0s               (30%)\n",
    "m03_lim = 0.6       # m:03s:a            (40%)\n",
    "mvsg_lim = 0.6      # m:vsg:a            (40%)\n",
    "pw_lim = 0.7        # p:w                (30%)\n",
    "sgc_lim = 0.6       # s:r:gc             (40%)\n",
    "suk_lim = 0.7       # s:uk               (30%)\n",
    "sr_lim = 0.6        # s:r (for pronouns) (40%)\n",
    "rep_lim = 0.4"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T10:48:41.721806300Z",
     "start_time": "2024-04-10T10:48:41.699629500Z"
    }
   },
   "id": "9e073b00e916e0db"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "vp_pattern = [[{'POS': 'VERB', 'OP': '?'},\n",
    "               {'POS': 'ADV', 'OP': '*'},\n",
    "               {'POS': 'AUX', 'OP': '*'},\n",
    "               {'POS': 'VERB', 'OP': '+'}]]\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"Verb phrase\", vp_pattern)\n",
    "aphasic_utt = \"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T10:48:41.758012900Z",
     "start_time": "2024-04-10T10:48:41.716435800Z"
    }
   },
   "id": "d08adb6c35fe3b1e"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def check_complexity(doc):\n",
    "    # remove too complex sentences, from Misra et al. \n",
    "    # get NPs\n",
    "    noun_phrases = set()\n",
    "    for nc in doc.noun_chunks:\n",
    "        for nop in [nc, doc[nc.root.left_edge.i:nc.root.right_edge.i + 1]]:\n",
    "            noun_phrases.add(nop.text.strip())\n",
    "    # get VPs\n",
    "    verb_phrases = matcher(doc)\n",
    "    verb_phrases = [doc[start:end] for _, start, end in verb_phrases]\n",
    "\n",
    "    try:\n",
    "        ratio = len(noun_phrases) / len(verb_phrases)\n",
    "    except:\n",
    "        # Division by zero\n",
    "        return 0, True\n",
    "\n",
    "    X = np.random.uniform(0, 1)\n",
    "    \n",
    "    # if too complex or going to reject sentence\n",
    "    # return true for too complex\n",
    "    return ratio, ratio > 2 and X <= 0.8"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T10:48:41.758012900Z",
     "start_time": "2024-04-10T10:48:41.728588400Z"
    }
   },
   "id": "ca434c8d68b59b0d"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "extra = '\"#$%&()*+-/:;<=>@[\\]^_`{|}~\"'\n",
    "\n",
    "for x in extra:\n",
    "    printable = printable.replace(x,'',)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T10:48:41.758012900Z",
     "start_time": "2024-04-10T10:48:41.745120900Z"
    }
   },
   "id": "cc3eb0758daad61d"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def aphasic_speech(text):\n",
    "    doc = nlp(text)\n",
    "    # keep 0.n of the text, min length is 5 words\n",
    "    # n = max(5, round(len(re.findall(\"[a-zA-Z_]+\", text))*0.3))\n",
    "    \n",
    "    n = np.random.gamma(shape=7.859547, scale=1/1.029798)\n",
    "    n = round(n)\n",
    "    \n",
    "    while n > 47 or n < 5:\n",
    "        n = np.random.gamma(shape=7.859547, scale=1/1.029798)\n",
    "        n = round(n)\n",
    "        \n",
    "    utt = \"\"\n",
    "    # length of original text\n",
    "    length = len(re.findall(\"[a-zA-Z_]+\", text))\n",
    "    \n",
    "    if length != 0:\n",
    "        # do not modify sentences with special characters\n",
    "        if set(text).difference(printable):\n",
    "            return \"\", False\n",
    "        \n",
    "        # get possible n/v ratio for this sentence\n",
    "        ratio_nv = np.random.gamma(shape=2.180031, scale=1/1.498104)\n",
    "        \n",
    "        # get the possible percentage of all POS\n",
    "        # values below are %-ages, not ratios\n",
    "        # noun and verb distributions are gamma, rest are truncated normal\n",
    "        percent_noun = np.random.gamma(shape=4.0047683, scale=1/0.1944749)\n",
    "        percent_verb = np.random.gamma(shape=9.9920204, scale=1/0.5973042)\n",
    "        open_close = np.random.gamma(shape=4.99415, scale=1/3.558095)\n",
    "        percent_det = get_truncnorm(7.55312, 6.004386, 0, 27.79661)\n",
    "        percent_prep = get_truncnorm(3.15664, 2.386052, 0, 15.05682)\n",
    "        percent_adj = get_truncnorm(4.258013, 3.460436, 0, 21.05263)\n",
    "        percent_adv = get_truncnorm(5.808547, 2.911826, 0, 15.88448)\n",
    "        # print(percent_noun)\n",
    "        # print(percent_verb)\n",
    "        nouns = []\n",
    "        verbs = []\n",
    "        determiners = []\n",
    "        prepositions = []\n",
    "        adjectives = []\n",
    "        adverbs = []\n",
    "        interjections = []\n",
    "        nv_control = np.random.gamma(shape=49.65696, scale=1/50.32268)\n",
    "        \n",
    "        # count no. of respective POS\n",
    "        for tok in doc:\n",
    "            if tok.pos_ == \"NOUN\":\n",
    "                nouns.append(tok.text)\n",
    "            elif tok.pos_ == \"VERB\" or tok.dep_ == \"cop\" or tok.tag_ in [\"VBD\", \"VBN\"]:\n",
    "                verbs.append(tok.text)\n",
    "            # det:art and det:dem only\n",
    "            elif tok.dep_ == \"det\" and (\"Dem\" in tok.morph.get('PronType') or \"Art\" in tok.morph.get('PronType')):\n",
    "                determiners.append(tok.text)\n",
    "            elif tok.dep_ == \"prep\":\n",
    "                prepositions.append(tok.text)\n",
    "            elif tok.pos_ == \"ADJ\":\n",
    "                adjectives.append(tok.text)\n",
    "            elif tok.pos_ == \"ADV\":\n",
    "                adverbs.append(tok.text)\n",
    "            elif tok.pos_ == \"INTJ\":\n",
    "                interjections.append(tok.pos_)\n",
    "        \n",
    "        if len(verbs) == 0:\n",
    "            return \"\", False\n",
    "        \n",
    "        if nv_control < len(nouns)/len(verbs):\n",
    "            return \"\", False\n",
    "        \n",
    "        open_class_num = len(nouns) + len(verbs) + len(adjectives) + len(adverbs)\n",
    "        closed_class_num = length - open_class_num - len(interjections)\n",
    "        \n",
    "        for tok in doc:\n",
    "            # current percentage of nouns and verbs in broca utterance\n",
    "            curr_ratio_nv = get_curr_nv_ratio(nouns, verbs)\n",
    "            \n",
    "            open_class_num = len(nouns) + len(verbs) + len(adjectives) + len(adverbs)\n",
    "            closed_class_num = length - open_class_num - len(interjections)\n",
    "             \n",
    "            # if length is 0 then no utterance\n",
    "            if length == 0 or closed_class_num == 0:\n",
    "                return \"\", False\n",
    "             \n",
    "            m0sa_prob = random.uniform(0,1)     #m:0s:a\n",
    "            ms_prob = random.uniform(0,1)       #m:+s(:a)\n",
    "            m0s_prob = random.uniform(0,1)      #m:0s\n",
    "            m03_prob = random.uniform(0,1)      # m:03s:a\n",
    "            mvsg_prob = random.uniform(0,1)     # m:vsg:a\n",
    "            pw_prob = random.uniform(0,1)       # p:w\n",
    "            sgc_prob = random.uniform(0,1)      # s:r:gc\n",
    "            suk_prob = random.uniform(0,1)      # s:uk  \n",
    "            sr_prob = random.uniform(0,1)       #s:r (for pronouns)\n",
    "            rep_prob = random.uniform(0,1)      # for repetition of                                         \n",
    "                                                # pronouns and \n",
    "                                                # interjections\n",
    "            remove = None\n",
    "            add = False\n",
    "            \n",
    "            if open_close < open_class_num/closed_class_num:\n",
    "                remove = np.random.choice([\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"])\n",
    "            if open_close > open_class_num/closed_class_num:\n",
    "                add = True\n",
    "                \n",
    "            # Handle nouns\n",
    "            if tok.pos_ == \"NOUN\": \n",
    "                # if possible noun percent in sentence less than current\n",
    "                # percent or if current n/v ratio is too big, remove noun \n",
    "                # from sentence\n",
    "                if (percent_noun <= (len(nouns)/length) * 100\n",
    "                        or curr_ratio_nv > ratio_nv or remove == tok.pos_) :\n",
    "                    utt += ' '\n",
    "                    length -= 1 \n",
    "                    nouns.remove(tok.text)  \n",
    "                # m:0s:a, m:+s, p:w, s:uk errors\n",
    "                elif m0sa_prob >= m0sa_lim or ms_prob >= ms_lim:\n",
    "                    if \"Plur\" in tok.morph.get(\"Number\"):\n",
    "                        utt += singularize(tok.text) + ' '\n",
    "                    elif \"Sing\" in tok.morph.get(\"Number\"):\n",
    "                        utt += pluralize(tok.text) + ' '  \n",
    "                elif pw_prob >= pw_lim or suk_prob >= suk_lim:\n",
    "                    utt += get_alt_word(tok) + ' '     \n",
    "                else:\n",
    "                    utt += tok.text + ' '\n",
    "                    \n",
    "            # Handle verbs (copula and gerund/participles counted as verb)\n",
    "            elif  tok.pos_ == \"VERB\" or tok.dep_ == \"cop\" or tok.tag_ in [\"VBD\", \"VBN\"]:\n",
    "                # if possible verb percent in sentence less than current \n",
    "                # percent or if current n/v ratio too big remove noun \n",
    "                # from sentence\n",
    "                if (percent_verb <= (len(verbs)/length) * 100 or \n",
    "                        remove == tok.pos_):\n",
    "                   utt += ' '\n",
    "                   length -= 1\n",
    "                   verbs.remove(tok.text)\n",
    "                \n",
    "                # m:03s:a, m:vsg:a error\n",
    "                elif m03_prob >= m03_lim or mvsg_prob >= mvsg_lim:\n",
    "                    # lemmatize reg+irr 3rd sing\n",
    "                    if '3' in tok.morph.get(\"Person\") and 'Sing' in tok.morph.get(\"Number\"):\n",
    "                        utt += tok.lemma_ + \" \"\n",
    "                    else:\n",
    "                        utt += tok.text + \" \"\n",
    "                # p:w error\n",
    "                elif pw_prob >= pw_lim:\n",
    "                    utt += get_alt_word(tok) + ' '\n",
    "                else:\n",
    "                    x = np.random.uniform(0,1)\n",
    "                    if x >= 0.5:\n",
    "                        # print(tok.text)\n",
    "                        utt += conjugate(tok.text, '3sg') + ' '\n",
    "                    else:\n",
    "                        utt += tok.text + ' '\n",
    "            \n",
    "            # Handle determiners (art and dem)\n",
    "            elif tok.dep_ == \"det\" and (\"Dem\" in tok.morph.get('PronType') or \"Art\" in tok.morph.get('PronType')) :\n",
    "                # if possible determiner percent in sentence less than current, remove determiner\n",
    "                if percent_det <= (len(determiners)/length) * 100:\n",
    "                   utt += ' '\n",
    "                   length -= 1\n",
    "                   determiners.remove(tok.text)\n",
    "                else:\n",
    "                    utt += tok.text + ' '\n",
    "                    \n",
    "            # Handle pronouns\n",
    "            elif tok.pos_ == \"PRON\":\n",
    "                # s:r:gc:pro  and s:r error (same for pronouns)\n",
    "                # but clan uses both versions \n",
    "                if sgc_prob >= sgc_lim or sr_prob >= sr_lim:\n",
    "                    if tok.pos_ == \"DET\" or \"Dem\" in tok.morph.get('PronType') or \"Yes\" in tok.morph.get('Poss'):\n",
    "                        sub = det_sub(tok.text) \n",
    "                        utt += sub + \" \"\n",
    "                        if rep_prob >= rep_lim:\n",
    "                            length += 1\n",
    "                            utt += sub + \" \"\n",
    "                    else:\n",
    "                        if rep_prob >= rep_lim:\n",
    "                            length += 1\n",
    "                            utt += tok.text + \" \"\n",
    "                        utt += tok.text + \" \"\n",
    "                else:\n",
    "                    if rep_prob >= rep_lim:\n",
    "                        length += 1\n",
    "                        utt += tok.text + \" \"\n",
    "                    utt += tok.text + \" \"\n",
    "                \n",
    "\n",
    "            # Handle prepositions\n",
    "            elif tok.dep_ == \"prep\":\n",
    "                if percent_prep <= (len(prepositions)/length) * 100:\n",
    "                   utt += ' '\n",
    "                   length -= 1\n",
    "                   prepositions.remove(tok.text)\n",
    "                elif suk_prob >= suk_lim:\n",
    "                    utt += get_alt_word(tok) + ' ' \n",
    "                else:\n",
    "                    utt += tok.text + ' '\n",
    "            \n",
    "            # Handle adjectives                 \n",
    "            elif tok.pos_ == \"ADJ\":\n",
    "                if (percent_adj <= (len(adjectives)/length) * 100 or \n",
    "                        remove == tok.pos_):\n",
    "                   utt += ' '\n",
    "                   length -= 1\n",
    "                   adjectives.remove(tok.text)\n",
    "                elif suk_prob >= suk_lim:\n",
    "                    utt += get_alt_word(tok) + ' '\n",
    "                else:\n",
    "                    utt += tok.text + ' '\n",
    "            \n",
    "            # Handle adverbs\n",
    "            elif tok.pos_ == \"ADV\":\n",
    "                if (percent_adv <= (len(adverbs)/length) * 100\n",
    "                        or remove == tok.pos_):\n",
    "                   utt += ' '\n",
    "                   length -= 1\n",
    "                   adverbs.remove(tok.text)\n",
    "                # p:w and s:uk errors\n",
    "                elif pw_prob >= pw_lim or suk_prob >= suk_lim:\n",
    "                    utt += get_alt_word(tok) + ' '\n",
    "                else:\n",
    "                    utt += tok.text + ' '\n",
    "            \n",
    "            # Handle particles ('s, not etc)\n",
    "            elif tok.pos_ == \"PART\":\n",
    "                x = np.random.uniform(0,1)\n",
    "                # m:0s error 50% times\n",
    "                # missing plural suffix\n",
    "                if tok.text.startswith(\"'s\") :\n",
    "                    if m0s_prob >= m0s_lim:\n",
    "                        utt = utt[:-1] + tok.text + ' ' \n",
    "                elif (tok.text.startswith(\"'\") or tok.text.startswith(\"n't\") \n",
    "                      or tok.text.startswith(\"nt\") or tok.text.startswith(\"v'e\")\n",
    "                      or tok.text.startswith(\"ve\")):\n",
    "                        utt = utt[:-1] + ' ' \n",
    "                else:\n",
    "                    utt += tok.text + ' '\n",
    "            \n",
    "            # Handle auxillaries ('ve in i have)\n",
    "            elif tok.pos_ == \"AUX\":\n",
    "                if tok.text.startswith(\"'\"):\n",
    "                    utt = utt[:-1] + tok.text + ' '\n",
    "\n",
    "                else:\n",
    "                    utt += tok.text + ' '\n",
    "            \n",
    "            # Handling punctuation (like :, .)\n",
    "            elif tok.pos == \"PUNCT\":                \n",
    "                utt = utt[:-1] + tok.text+ ' '\n",
    "            \n",
    "            elif tok.pos == \"INTJ\":\n",
    "                if rep_prob >= rep_lim or add:\n",
    "                    length += 1\n",
    "                    interjections.append(tok.pos_)\n",
    "                    utt += tok.text + \" \" \n",
    "                utt += tok.text + \" \"\n",
    "                \n",
    "            # all other words with respective POS have a chance of s:uk\n",
    "            else:\n",
    "                utt += tok.text + ' '\n",
    "\n",
    "            \n",
    "        utt = \" \".join(utt.split()) # remove trailing whitespaces\n",
    "        utt = re.sub(r'\\s+([?.!\",])', r'\\1', utt)\n",
    "        \n",
    "        # only return sentences which are short enough\n",
    "        if (5 < len(re.findall(\"[a-zA-Z_]+\", utt)) <= n \n",
    "                and len(re.findall(\"[a-zA-Z_]+\", utt)) <= 47): \n",
    "            return utt, True\n",
    "        else:\n",
    "            return '', False\n",
    "        \n",
    "    # skipped sentence due to original length = 0\n",
    "    else:\n",
    "        return \"\", False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T10:48:41.951887900Z",
     "start_time": "2024-04-10T10:48:41.933430700Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test area"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "245fdb8ba2c59c76"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence:  I received the brass that you sent me.\n",
      "-----------------------------------------------\n",
      "Original sentence:  Thank you very much for all your trouble and the extra 3 pieces.\n",
      "-----------------------------------------------\n",
      "Original sentence:  I feel that you have an outstanding company and are striving the best that you can to achieve customer satisfaction.\n",
      "-----------------------------------------------\n",
      "Original sentence:  I will be certain to tell my friends about US Reloading Supply.\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "para = \"\"\"I received the brass that you sent me. Thank you very much for all your trouble and the extra 3 pieces. I feel that you have an outstanding company and are striving the best that you can to achieve customer satisfaction. I will be certain to tell my friends about US Reloading Supply.\"\"\"\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentences = sent_tokenize(para)\n",
    "for sent in sentences:\n",
    "    print(\"Original sentence: \", sent)\n",
    "    aphasic, changed = aphasic_speech(sent)\n",
    "    if changed:\n",
    "        print(\"Final utter:\", \" \".join(aphasic.split()).strip())\n",
    "        print(\"-----------------------------------------------\")\n",
    "    else:\n",
    "        print(\"-----------------------------------------------\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T10:48:41.999181100Z",
     "start_time": "2024-04-10T10:48:41.955498100Z"
    }
   },
   "id": "82580e7ec167ce03"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ruhis ADJ\n",
      "notebook NOUN\n"
     ]
    }
   ],
   "source": [
    "for x in nlp(\"Ruhis notebook\"):\n",
    "    print(x.text, x.pos_)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T10:48:42.014305300Z",
     "start_time": "2024-04-10T10:48:41.999181100Z"
    }
   },
   "id": "8535e070e5517404"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test on some of the IMDB dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91db62f38951bdb0"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'timestamp', 'url', 'meta', 'text_length', 'domain', 'perplexity', 'dup_ratio', 'pairs', 'repetitions', 'cluster'],\n",
      "        num_rows: 100000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "ds = load_dataset(\"datablations/c4-filter-small\")\n",
    "print(ds)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T10:48:47.977198300Z",
     "start_time": "2024-04-10T10:48:42.014305300Z"
    }
   },
   "id": "254d6418351d0f9"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "texts = ds[\"train\"][\"text\"]\n",
    "sents = []\n",
    "for text in texts:\n",
    "    text = re.sub(r'\\<.*?\\>', \" \", text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    for sent in sentences:\n",
    "        if isinstance(sent, str):\n",
    "            sent = re.sub(r'\\<.*?\\>', \" \", sent)\n",
    "            if not sent.rstrip().isdigit() and len(sent.rstrip()) >= 1:\n",
    "                sents.append(sent)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T10:49:18.020341100Z",
     "start_time": "2024-04-10T10:48:47.985608800Z"
    }
   },
   "id": "45c43d9f2a09e8a7"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "total_num_sents = 40000 # 1000\n",
    "test_sents = sents[:total_num_sents]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T10:49:18.036139300Z",
     "start_time": "2024-04-10T10:49:18.026722100Z"
    }
   },
   "id": "b96867011ab3dbfa"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000 sentences, broca utts: 44\n",
      "Processed 2000 sentences, broca utts: 92\n",
      "Processed 3000 sentences, broca utts: 126\n",
      "Processed 4000 sentences, broca utts: 168\n",
      "Processed 5000 sentences, broca utts: 197\n",
      "Processed 6000 sentences, broca utts: 240\n",
      "Processed 7000 sentences, broca utts: 269\n",
      "Processed 8000 sentences, broca utts: 306\n",
      "Processed 9000 sentences, broca utts: 358\n",
      "Processed 10000 sentences, broca utts: 403\n",
      "Processed 11000 sentences, broca utts: 432\n",
      "Processed 12000 sentences, broca utts: 475\n",
      "Processed 13000 sentences, broca utts: 523\n",
      "Processed 14000 sentences, broca utts: 549\n",
      "Processed 15000 sentences, broca utts: 586\n",
      "Processed 16000 sentences, broca utts: 618\n",
      "Processed 17000 sentences, broca utts: 636\n",
      "Processed 18000 sentences, broca utts: 669\n",
      "Processed 19000 sentences, broca utts: 714\n",
      "Processed 20000 sentences, broca utts: 746\n",
      "Processed 21000 sentences, broca utts: 785\n",
      "Processed 22000 sentences, broca utts: 830\n",
      "Processed 23000 sentences, broca utts: 854\n",
      "Processed 24000 sentences, broca utts: 884\n",
      "Processed 25000 sentences, broca utts: 912\n",
      "Processed 26000 sentences, broca utts: 948\n",
      "Processed 27000 sentences, broca utts: 978\n",
      "Processed 28000 sentences, broca utts: 1008\n",
      "Processed 29000 sentences, broca utts: 1031\n",
      "Processed 30000 sentences, broca utts: 1058\n",
      "Processed 31000 sentences, broca utts: 1087\n",
      "Processed 32000 sentences, broca utts: 1137\n",
      "Processed 33000 sentences, broca utts: 1166\n",
      "Processed 34000 sentences, broca utts: 1197\n",
      "Processed 35000 sentences, broca utts: 1224\n",
      "Processed 36000 sentences, broca utts: 1251\n",
      "Processed 37000 sentences, broca utts: 1272\n",
      "Processed 38000 sentences, broca utts: 1334\n",
      "Processed 39000 sentences, broca utts: 1373\n",
      "Processed 40000 sentences, broca utts: 1399\n"
     ]
    }
   ],
   "source": [
    "aphasic_sents = []\n",
    "normal_sents = []\n",
    "\n",
    "import inflect\n",
    "from preprocess import postprocess\n",
    "p = inflect.engine()\n",
    "count = 0\n",
    "\n",
    "for sent in test_sents:\n",
    "    # no digits like in aphasiabank\n",
    "    # print(sent)\n",
    "    b = re.findall(\"[0-9]+\", sent)\n",
    "    for i in b:\n",
    "        sent = sent.replace(i, p.number_to_words(i))\n",
    "    \n",
    "    s = preprocess(sent)\n",
    "    aphasic, changed = aphasic_speech(s)\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(f\"Processed {count} sentences, broca utts: {len(aphasic_sents)}\")\n",
    "    if changed and aphasic !=\".\":\n",
    "        # print(sent)\n",
    "        # print(s)\n",
    "        # print(postprocess(aphasic))\n",
    "        # print()\n",
    "        normal_sents.append(s)\n",
    "        aphasic_sents.append(aphasic)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T10:55:48.894222600Z",
     "start_time": "2024-04-10T10:49:43.114276Z"
    }
   },
   "id": "7b116c75e017a103"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Post process of aphasic sentences\n",
    "Also adding some \"control\" sentences"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ab67054aaaee979"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from datasets import load_dataset\n",
    "# \n",
    "# # ds = load_dataset('stas/c4-en-10k')\n",
    "# ds = load_dataset(\"imdb\")\n",
    "# print(ds)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T10:49:19.581908800Z",
     "start_time": "2024-04-10T10:49:19.566269800Z"
    }
   },
   "id": "a232e0b1491df977"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "sentences = aphasic_sents\n",
    "original = normal_sents"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T10:55:48.932642200Z",
     "start_time": "2024-04-10T10:55:48.900766900Z"
    }
   },
   "id": "346435140a64874a"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from preprocess import postprocess\n",
    "broca_sents = []\n",
    "original_sents = []\n",
    "for sent, o in zip(sentences, original):\n",
    "    x = postprocess(sent)\n",
    "    if x != \"\":\n",
    "        broca_sents.append(x)\n",
    "        original_sents.append(o)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T10:55:48.952552800Z",
     "start_time": "2024-04-10T10:55:48.920445100Z"
    }
   },
   "id": "6ea26a0b735f444c"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "pd.DataFrame(data={\"modified\": broca_sents, \"original\": original_sents}).to_csv(\"data/test.csv\", sep=\",\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T10:55:48.968618300Z",
     "start_time": "2024-04-10T10:55:48.955349400Z"
    }
   },
   "id": "6577bcdc8e5cfc7e"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "broca_sents = pd.read_csv(\"data/test.csv\", sep=\",\")[\"modified\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T10:55:49.001350500Z",
     "start_time": "2024-04-10T10:55:48.968618300Z"
    }
   },
   "id": "c8cd793b87630a2c"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "import inflect\n",
    "from preprocess import preprocess, postprocess\n",
    "p = inflect.engine()\n",
    "import re\n",
    "\n",
    "# post process and pre-process in same way\n",
    "from nltk.tokenize import sent_tokenize\n",
    "new_sents = []\n",
    "sentences = sents[total_num_sents:]\n",
    "for sent in sentences:\n",
    "    if len(new_sents) >= round(len(broca_sents)*2.5755):\n",
    "        break\n",
    "    if isinstance(sent, str):\n",
    "        sent = re.sub(r'\\<.*?\\>', \" \", sent)\n",
    "        if not sent.rstrip().isdigit() and len(sent.rstrip()) >= 1:\n",
    "            # no digits\n",
    "            b = re.findall(\"[0-9]+\", sent)\n",
    "            for i in b:\n",
    "                sent = sent.replace(i, p.number_to_words(i))\n",
    "            sent = preprocess(sent)\n",
    "            sent = postprocess(sent)\n",
    "            if sent != \"\":\n",
    "                new_sents.append(sent)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T10:55:49.381934600Z",
     "start_time": "2024-04-10T10:55:48.996177200Z"
    }
   },
   "id": "e4682d97a579cf7e"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "3603"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_sents)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T10:55:49.394750900Z",
     "start_time": "2024-04-10T10:55:49.383983900Z"
    }
   },
   "id": "6f90655ea524e9ce"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "1399"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(broca_sents)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T10:55:49.442555300Z",
     "start_time": "2024-04-10T10:55:49.394750900Z"
    }
   },
   "id": "446fb41e5a3ab5d1"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "control_sents = new_sents[:total_num_sents+round(len(broca_sents)*2.5755)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T10:55:49.458554400Z",
     "start_time": "2024-04-10T10:55:49.415286100Z"
    }
   },
   "id": "621e4424aaaa2c50"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1399 3603\n"
     ]
    }
   ],
   "source": [
    "print(len(broca_sents), len(control_sents))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T10:55:49.458554400Z",
     "start_time": "2024-04-10T10:55:49.426628200Z"
    }
   },
   "id": "8f614650609a6194"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "broca_data = pd.DataFrame(data={\"preprocessed_text\": broca_sents, \"label\": [1]*len(broca_sents)})\n",
    "control_data = pd.DataFrame(data={\"preprocessed_text\": control_sents, \"label\": [0]*len(control_sents)})\n",
    "data_full_scenario = pd.concat([broca_data, control_data], ignore_index=True)\n",
    "data_full_scenario = data_full_scenario.sample(frac=1).reset_index(drop=True)\n",
    "# data_full_scenario.to_csv(\"data/synthetic_clan_test.csv\", sep=\",\", index=False)\n",
    "data_full_scenario.to_csv(\"data/test_merge.csv\", sep=\",\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T10:55:49.521972300Z",
     "start_time": "2024-04-10T10:55:49.447541500Z"
    }
   },
   "id": "de47ecaf3bc979de"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "                                      preprocessed_text  label\n0                             as i hoped they would be.      1\n1     i walked away from the dinner party thinking i...      0\n2                                how do we we usu them?      1\n3     we are committed to efficient safe and simple ...      0\n4     ​​picking winners is our business ​​copyright ...      0\n...                                                 ...    ...\n4997                  let us keep the good ones around.      0\n4998  get your original tractor or combine parts her...      0\n4999  on may zeroth of zerozero call it arson releas...      0\n5000  in june zero the united states marine corps an...      0\n5001                     what what you you are wearing.      1\n\n[5002 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>preprocessed_text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>as i hoped they would be.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>i walked away from the dinner party thinking i...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>how do we we usu them?</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>we are committed to efficient safe and simple ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>​​picking winners is our business ​​copyright ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4997</th>\n      <td>let us keep the good ones around.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4998</th>\n      <td>get your original tractor or combine parts her...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4999</th>\n      <td>on may zeroth of zerozero call it arson releas...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5000</th>\n      <td>in june zero the united states marine corps an...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5001</th>\n      <td>what what you you are wearing.</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>5002 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_full_scenario"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T10:55:49.529236800Z",
     "start_time": "2024-04-10T10:55:49.512341500Z"
    }
   },
   "id": "e18e6bd624f8eb25"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
