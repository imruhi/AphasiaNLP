{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Testing the trained model (trained on synthetic data) on broca data from aphasiabank gives 69% to 70% accuracy for correctly predicting aphasiabank sentences as broca, and 88-90% accuracy for correctly predicting control sentences as control"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "516400c5dd06a2b6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### NOTE: In CLAN the C-NNLA command was used to get the distribution parameters. However, error outputs from C-NNLA were not very useful (like % correct ..., % incorrect, % grammatical etc) since some CHA files do not annotate errors like [* p], [* s] (but just replace it with xxx, yyy, zzz), making those measures unreliable\n",
    "\n",
    "##### However, I am trying to include the 5 most common word errors from the C-NNLA measures \n",
    "\n",
    "##### This approach also does not consider any utterance errors/substitutions (marked as @u or [* p:n] in AphasiaBank)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4fa55da3a7037ee"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import random\n",
    "import re\n",
    "from pattern.text.en import singularize, pluralize, conjugate\n",
    "import enchant\n",
    "from preprocess import preprocess\n",
    "from spacy.matcher import Matcher\n",
    "from string import printable\n",
    "\n",
    "d = enchant.Dict(\"en_US\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T21:28:19.900572500Z",
     "start_time": "2024-04-09T21:28:19.388751900Z"
    }
   },
   "id": "fd32d434bf5e298a"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "dets = {'Art': ['a', 'an', 'the', ''],\n",
    "           'Dem': ['this', 'that', 'these', 'those', ''],\n",
    "           'Poss': ['my', 'your', 'his', 'her', 'its', 'our', 'their', '']}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T21:28:19.915510900Z",
     "start_time": "2024-04-09T21:28:19.902796600Z"
    }
   },
   "id": "6d1fac05d4a9f743"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "def det_sub(x):\n",
    "    for _, det in dets.items():\n",
    "        if x.lower() in det:\n",
    "            y = [j for j in det if x!=j]\n",
    "            return random.choice(y)\n",
    "    return \"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T21:28:19.936512100Z",
     "start_time": "2024-04-09T21:28:19.917511100Z"
    }
   },
   "id": "53da41a352bbf2cf"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "def get_truncnorm(mean, std, min, max):\n",
    "    # mean, std, min, max parameters dependent on their extracted normal\n",
    "    # distributions\n",
    "    a, b = (min - mean) / std, (max - mean) / std\n",
    "    return stats.truncnorm(a, b, loc=mean, \n",
    "                                   scale=std).rvs(size=1)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T21:28:19.947511900Z",
     "start_time": "2024-04-09T21:28:19.935512300Z"
    }
   },
   "id": "f2f021bc59a96b52"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "def get_curr_nv_ratio(nouns, verbs):\n",
    "    if len(nouns) != 0 and len(verbs) != 0:\n",
    "        curr_ratio_nv = len(nouns)/len(verbs)\n",
    "    else:\n",
    "        curr_ratio_nv = 0\n",
    "    return curr_ratio_nv"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T21:28:19.975511Z",
     "start_time": "2024-04-09T21:28:19.950512700Z"
    }
   },
   "id": "349210d24a5ce11d"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "def get_alt_word(tok):\n",
    "    # get a new word which is the same length as old word\n",
    "    # to simulate p:w errors\n",
    "    possible_words = [x for x in d.suggest(tok.text) if len(x) ==\n",
    "                      len(tok.text) and d.check(x) and x != tok.text] \n",
    "    if possible_words:\n",
    "        new_word = random.choice(possible_words)\n",
    "        if new_word:\n",
    "            return new_word\n",
    "    \n",
    "    # if we cannot find new word just return current word\n",
    "    return tok.text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T21:28:19.987511Z",
     "start_time": "2024-04-09T21:28:19.962511500Z"
    }
   },
   "id": "bb61dd53862bfa26"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "m0sa_lim = 0.7      # m:0s:a             (30%)\n",
    "ms_lim = 0.7        # m:+s(:a)           (30%)\n",
    "m0s_lim = 0.7       # m:0s               (30%)\n",
    "m03_lim = 0.6       # m:03s:a            (40%)\n",
    "mvsg_lim = 0.6      # m:vsg:a            (40%)\n",
    "pw_lim = 0.7        # p:w                (30%)\n",
    "sgc_lim = 0.6       # s:r:gc             (40%)\n",
    "suk_lim = 0.7       # s:uk               (30%)\n",
    "sr_lim = 0.6        # s:r (for pronouns) (40%)\n",
    "rep_lim = 0.4"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T21:28:19.997513Z",
     "start_time": "2024-04-09T21:28:19.981514600Z"
    }
   },
   "id": "9e073b00e916e0db"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "vp_pattern = [[{'POS': 'VERB', 'OP': '?'},\n",
    "               {'POS': 'ADV', 'OP': '*'},\n",
    "               {'POS': 'AUX', 'OP': '*'},\n",
    "               {'POS': 'VERB', 'OP': '+'}]]\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"Verb phrase\", vp_pattern)\n",
    "aphasic_utt = \"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T21:28:20.022822300Z",
     "start_time": "2024-04-09T21:28:19.997513Z"
    }
   },
   "id": "d08adb6c35fe3b1e"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "def check_complexity(doc):\n",
    "    # remove too complex sentences, from Misra et al. \n",
    "    # get NPs\n",
    "    noun_phrases = set()\n",
    "    for nc in doc.noun_chunks:\n",
    "        for nop in [nc, doc[nc.root.left_edge.i:nc.root.right_edge.i + 1]]:\n",
    "            noun_phrases.add(nop.text.strip())\n",
    "    # get VPs\n",
    "    verb_phrases = matcher(doc)\n",
    "    verb_phrases = [doc[start:end] for _, start, end in verb_phrases]\n",
    "\n",
    "    try:\n",
    "        ratio = len(noun_phrases) / len(verb_phrases)\n",
    "    except:\n",
    "        # Division by zero\n",
    "        return 0, True\n",
    "\n",
    "    X = np.random.uniform(0, 1)\n",
    "    \n",
    "    # if too complex or going to reject sentence\n",
    "    # return true for too complex\n",
    "    return ratio, ratio > 2 and X <= 0.8"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T21:28:20.033824900Z",
     "start_time": "2024-04-09T21:28:20.013511600Z"
    }
   },
   "id": "ca434c8d68b59b0d"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "def aphasic_speech(text):\n",
    "    doc = nlp(text)\n",
    "    # keep 0.n of the text, min length is 5 words\n",
    "    # n = max(5, round(len(re.findall(\"[a-zA-Z_]+\", text))*0.3))\n",
    "    \n",
    "    n = np.random.gamma(shape=7.859547, scale=1/1.029798)\n",
    "    n = round(n)\n",
    "    \n",
    "    while n > 47 or n < 5:\n",
    "        n = np.random.gamma(shape=7.859547, scale=1/1.029798)\n",
    "        n = round(n)\n",
    "        \n",
    "    utt = \"\"\n",
    "    # length of original text\n",
    "    length = len(re.findall(\"[a-zA-Z_]+\", text))\n",
    "    \n",
    "    if length != 0:\n",
    "        # do not modify sentences with special characters\n",
    "        if set(text).difference(printable):\n",
    "            return \"\", False\n",
    "        \n",
    "        # get possible n/v ratio for this sentence\n",
    "        ratio_nv = np.random.gamma(shape=2.180031, scale=1/1.498104)\n",
    "        \n",
    "        # get the possible percentage of all POS\n",
    "        # values below are %-ages, not ratios\n",
    "        # noun and verb distributions are gamma, rest are truncated normal\n",
    "        percent_noun = np.random.gamma(shape=4.0047683, scale=1/0.1944749)\n",
    "        percent_verb = np.random.gamma(shape=9.9920204, scale=1/0.5973042)\n",
    "        open_close = np.random.gamma(shape=4.99415, scale=1/3.558095)\n",
    "        percent_det = get_truncnorm(7.55312, 6.004386, 0, 27.79661)\n",
    "        percent_prep = get_truncnorm(3.15664, 2.386052, 0, 15.05682)\n",
    "        percent_adj = get_truncnorm(4.258013, 3.460436, 0, 21.05263)\n",
    "        percent_adv = get_truncnorm(5.808547, 2.911826, 0, 15.88448)\n",
    "        # print(percent_noun)\n",
    "        # print(percent_verb)\n",
    "        nouns = []\n",
    "        verbs = []\n",
    "        determiners = []\n",
    "        prepositions = []\n",
    "        adjectives = []\n",
    "        adverbs = []\n",
    "        interjections = []\n",
    "        \n",
    "        # count no. of respective POS\n",
    "        for tok in doc:\n",
    "            if tok.pos_ == \"NOUN\":\n",
    "                nouns.append(tok.text)\n",
    "            elif tok.pos_ == \"VERB\" or tok.dep_ == \"cop\" or tok.tag_ in [\"VBD\", \"VBN\"]:\n",
    "                verbs.append(tok.text)\n",
    "            # det:art and det:dem only\n",
    "            elif tok.dep_ == \"det\" and (\"Dem\" in tok.morph.get('PronType') or \"Art\" in tok.morph.get('PronType')):\n",
    "                determiners.append(tok.text)\n",
    "            elif tok.dep_ == \"prep\":\n",
    "                prepositions.append(tok.text)\n",
    "            elif tok.pos_ == \"ADJ\":\n",
    "                adjectives.append(tok.text)\n",
    "            elif tok.pos_ == \"ADV\":\n",
    "                adverbs.append(tok.text)\n",
    "            elif tok.pos_ == \"INTJ\":\n",
    "                interjections.append(tok.pos_)\n",
    "        \n",
    "        open_class_num = len(nouns) + len(verbs) + len(adjectives) + len(adverbs)\n",
    "        closed_class_num = length - open_class_num - len(interjections)\n",
    "        \n",
    "        for tok in doc:\n",
    "            # current percentage of nouns and verbs in broca utterance\n",
    "            curr_ratio_nv = get_curr_nv_ratio(nouns, verbs)\n",
    "            \n",
    "            open_class_num = len(nouns) + len(verbs) + len(adjectives) + len(adverbs)\n",
    "            closed_class_num = length - open_class_num - len(interjections)\n",
    "             \n",
    "            # if length is 0 then no utterance\n",
    "            if length == 0 or closed_class_num == 0:\n",
    "                return \"\", False\n",
    "             \n",
    "            m0sa_prob = random.uniform(0,1)     #m:0s:a\n",
    "            ms_prob = random.uniform(0,1)       #m:+s(:a)\n",
    "            m0s_prob = random.uniform(0,1)      #m:0s\n",
    "            m03_prob = random.uniform(0,1)      # m:03s:a\n",
    "            mvsg_prob = random.uniform(0,1)     # m:vsg:a\n",
    "            pw_prob = random.uniform(0,1)       # p:w\n",
    "            sgc_prob = random.uniform(0,1)      # s:r:gc\n",
    "            suk_prob = random.uniform(0,1)      # s:uk  \n",
    "            sr_prob = random.uniform(0,1)       #s:r (for pronouns)\n",
    "            rep_prob = random.uniform(0,1)      # for repetition of                                         \n",
    "                                                # pronouns and \n",
    "                                                # interjections\n",
    "            remove = None\n",
    "            add = False\n",
    "            \n",
    "            if open_close < open_class_num/closed_class_num:\n",
    "                remove = np.random.choice([\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"])\n",
    "            if open_close > open_class_num/closed_class_num:\n",
    "                add = True\n",
    "                \n",
    "            # Handle nouns\n",
    "            if tok.pos_ == \"NOUN\": \n",
    "                # if possible noun percent in sentence less than current\n",
    "                # percent or if current n/v ratio is too big, remove noun \n",
    "                # from sentence\n",
    "                if (percent_noun <= (len(nouns)/length) * 100\n",
    "                        or curr_ratio_nv > ratio_nv or remove == tok.pos_) :\n",
    "                    utt += ' '\n",
    "                    length -= 1 \n",
    "                    nouns.remove(tok.text)  \n",
    "                # m:0s:a, m:+s, p:w, s:uk errors\n",
    "                elif m0sa_prob >= m0sa_lim or ms_prob >= ms_lim:\n",
    "                    if \"Plur\" in tok.morph.get(\"Number\"):\n",
    "                        utt += singularize(tok.text) + ' '\n",
    "                    elif \"Sing\" in tok.morph.get(\"Number\"):\n",
    "                        utt += pluralize(tok.text) + ' '  \n",
    "                elif pw_prob >= pw_lim or suk_prob >= suk_lim:\n",
    "                    utt += get_alt_word(tok) + ' '     \n",
    "                else:\n",
    "                    utt += tok.text + ' '\n",
    "                    \n",
    "            # Handle verbs (copula and gerund/participles counted as verb)\n",
    "            elif  tok.pos_ == \"VERB\" or tok.dep_ == \"cop\" or tok.tag_ in [\"VBD\", \"VBN\"]:\n",
    "                # if possible verb percent in sentence less than current \n",
    "                # percent or if current n/v ratio too big remove noun \n",
    "                # from sentence\n",
    "                if (percent_verb <= (len(verbs)/length) * 100 or \n",
    "                        remove == tok.pos_):\n",
    "                   utt += ' '\n",
    "                   length -= 1\n",
    "                   verbs.remove(tok.text)\n",
    "                \n",
    "                # m:03s:a, m:vsg:a error\n",
    "                elif m03_prob >= m03_lim or mvsg_prob >= mvsg_lim:\n",
    "                    # lemmatize reg+irr 3rd sing\n",
    "                    if '3' in tok.morph.get(\"Person\") and 'Sing' in tok.morph.get(\"Number\"):\n",
    "                        utt += tok.lemma_ + \" \"\n",
    "                    else:\n",
    "                        utt += tok.text + \" \"\n",
    "                # p:w error\n",
    "                elif pw_prob >= pw_lim:\n",
    "                    utt += get_alt_word(tok) + ' '\n",
    "                else:\n",
    "                    x = np.random.uniform(0,1)\n",
    "                    if x >= 0.5:\n",
    "                        # print(tok.text)\n",
    "                        utt += conjugate(tok.text, '3sg') + ' '\n",
    "                    else:\n",
    "                        utt += tok.text + ' '\n",
    "            \n",
    "            # Handle determiners (art and dem)\n",
    "            elif tok.dep_ == \"det\" and (\"Dem\" in tok.morph.get('PronType') or \"Art\" in tok.morph.get('PronType')) :\n",
    "                # if possible determiner percent in sentence less than current, remove determiner\n",
    "                if percent_det <= (len(determiners)/length) * 100:\n",
    "                   utt += ' '\n",
    "                   length -= 1\n",
    "                   determiners.remove(tok.text)\n",
    "                else:\n",
    "                    utt += tok.text + ' '\n",
    "                    \n",
    "            # Handle pronouns\n",
    "            elif tok.pos_ == \"PRON\":\n",
    "                # s:r:gc:pro  and s:r error (same for pronouns)\n",
    "                # but clan uses both versions \n",
    "                if sgc_prob >= sgc_lim or sr_prob >= sr_lim:\n",
    "                    if tok.pos_ == \"DET\" or \"Dem\" in tok.morph.get('PronType') or \"Yes\" in tok.morph.get('Poss'):\n",
    "                        sub = det_sub(tok.text) \n",
    "                        utt += sub + \" \"\n",
    "                        if rep_prob >= rep_lim:\n",
    "                            length += 1\n",
    "                            utt += sub + \" \"\n",
    "                    else:\n",
    "                        if rep_prob >= rep_lim:\n",
    "                            length += 1\n",
    "                            utt += tok.text + \" \"\n",
    "                        utt += tok.text + \" \"\n",
    "                else:\n",
    "                    if rep_prob >= rep_lim:\n",
    "                        length += 1\n",
    "                        utt += tok.text + \" \"\n",
    "                    utt += tok.text + \" \"\n",
    "                \n",
    "\n",
    "            # Handle prepositions\n",
    "            elif tok.dep_ == \"prep\":\n",
    "                if percent_prep <= (len(prepositions)/length) * 100:\n",
    "                   utt += ' '\n",
    "                   length -= 1\n",
    "                   prepositions.remove(tok.text)\n",
    "                elif suk_prob >= suk_lim:\n",
    "                    utt += get_alt_word(tok) + ' ' \n",
    "                else:\n",
    "                    utt += tok.text + ' '\n",
    "            \n",
    "            # Handle adjectives                 \n",
    "            elif tok.pos_ == \"ADJ\":\n",
    "                if (percent_adj <= (len(adjectives)/length) * 100 or \n",
    "                        remove == tok.pos_):\n",
    "                   utt += ' '\n",
    "                   length -= 1\n",
    "                   adjectives.remove(tok.text)\n",
    "                elif suk_prob >= suk_lim:\n",
    "                    utt += get_alt_word(tok) + ' '\n",
    "                else:\n",
    "                    utt += tok.text + ' '\n",
    "            \n",
    "            # Handle adverbs\n",
    "            elif tok.pos_ == \"ADV\":\n",
    "                if (percent_adv <= (len(adverbs)/length) * 100\n",
    "                        or remove == tok.pos_):\n",
    "                   utt += ' '\n",
    "                   length -= 1\n",
    "                   adverbs.remove(tok.text)\n",
    "                # p:w and s:uk errors\n",
    "                elif pw_prob >= pw_lim or suk_prob >= suk_lim:\n",
    "                    utt += get_alt_word(tok) + ' '\n",
    "                else:\n",
    "                    utt += tok.text + ' '\n",
    "            \n",
    "            # Handle particles ('s, not etc)\n",
    "            elif tok.pos_ == \"PART\":\n",
    "                x = np.random.uniform(0,1)\n",
    "                # m:0s error 50% times\n",
    "                # missing plural suffix\n",
    "                if tok.text.startswith(\"'s\") :\n",
    "                    if m0s_prob >= m0s_lim:\n",
    "                        utt = utt[:-1] + tok.text + ' ' \n",
    "                elif (tok.text.startswith(\"'\") or tok.text.startswith(\"n't\") \n",
    "                      or tok.text.startswith(\"nt\") or tok.text.startswith(\"v'e\")\n",
    "                      or tok.text.startswith(\"ve\")):\n",
    "                        utt = utt[:-1] + ' ' \n",
    "                else:\n",
    "                    utt += tok.text + ' '\n",
    "            \n",
    "            # Handle auxillaries ('ve in i have)\n",
    "            elif tok.pos_ == \"AUX\":\n",
    "                if tok.text.startswith(\"'\"):\n",
    "                    utt = utt[:-1] + tok.text + ' '\n",
    "\n",
    "                else:\n",
    "                    utt += tok.text + ' '\n",
    "            \n",
    "            # Handling punctuation (like :, .)\n",
    "            elif tok.pos == \"PUNCT\":                \n",
    "                utt = utt[:-1] + tok.text+ ' '\n",
    "            \n",
    "            elif tok.pos == \"INTJ\":\n",
    "                if rep_prob >= rep_lim or add:\n",
    "                    length += 1\n",
    "                    interjections.append(tok.pos_)\n",
    "                    utt += tok.text + \" \" \n",
    "                utt += tok.text + \" \"\n",
    "                \n",
    "            # all other words with respective POS have a chance of s:uk\n",
    "            else:\n",
    "                utt += tok.text + ' '\n",
    "\n",
    "            \n",
    "        utt = \" \".join(utt.split()) # remove trailing whitespaces\n",
    "        utt = re.sub(r'\\s+([?.!\",])', r'\\1', utt)\n",
    "        \n",
    "        # only return sentences which are short enough\n",
    "        # minimum 40% of the sentence is kept\n",
    "        if (5 < len(re.findall(\"[a-zA-Z_]+\", utt)) <= n \n",
    "                and len(re.findall(\"[a-zA-Z_]+\", utt)) <= 47): \n",
    "            return utt, True\n",
    "        else:\n",
    "            return '', False\n",
    "        \n",
    "    # skipped sentence due to original length = 0\n",
    "    else:\n",
    "        return \"\", False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T21:28:20.068820600Z",
     "start_time": "2024-04-09T21:28:20.051820600Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test area"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "245fdb8ba2c59c76"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence:  I received the brass that you sent me.\n",
      "Final utter: I I that that you me.\n",
      "-----------------------------------------------\n",
      "Original sentence:  Thank you very much for all your trouble and the extra 3 pieces.\n",
      "-----------------------------------------------\n",
      "Original sentence:  I feel that you have an outstanding company and are striving the best that you can to achieve customer satisfaction.\n",
      "-----------------------------------------------\n",
      "Original sentence:  I will be certain to tell my friends about US Reloading Supply.\n",
      "Final utter: I will be to tell my US Reloading Supply.\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "para = \"\"\"I received the brass that you sent me. Thank you very much for all your trouble and the extra 3 pieces. I feel that you have an outstanding company and are striving the best that you can to achieve customer satisfaction. I will be certain to tell my friends about US Reloading Supply.\"\"\"\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentences = sent_tokenize(para)\n",
    "for sent in sentences:\n",
    "    print(\"Original sentence: \", sent)\n",
    "    aphasic, changed = aphasic_speech(sent)\n",
    "    if changed:\n",
    "        print(\"Final utter:\", \" \".join(aphasic.split()).strip())\n",
    "        print(\"-----------------------------------------------\")\n",
    "    else:\n",
    "        print(\"-----------------------------------------------\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T21:28:20.575564300Z",
     "start_time": "2024-04-09T21:28:20.058821700Z"
    }
   },
   "id": "82580e7ec167ce03"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ruhis ADJ\n",
      "notebook NOUN\n"
     ]
    }
   ],
   "source": [
    "for x in nlp(\"Ruhis notebook\"):\n",
    "    print(x.text, x.pos_)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T21:28:20.585564400Z",
     "start_time": "2024-04-09T21:28:20.570563800Z"
    }
   },
   "id": "8535e070e5517404"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test on some of the IMDB dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91db62f38951bdb0"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'timestamp', 'url', 'meta', 'text_length', 'domain', 'perplexity', 'dup_ratio', 'pairs', 'repetitions', 'cluster'],\n",
      "        num_rows: 100000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "ds = load_dataset(\"datablations/c4-filter-small\")\n",
    "print(ds)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T21:28:24.948244300Z",
     "start_time": "2024-04-09T21:28:20.587563500Z"
    }
   },
   "id": "254d6418351d0f9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "texts = ds[\"train\"][\"text\"]\n",
    "sents = []\n",
    "for text in texts:\n",
    "    text = re.sub(r'\\<.*?\\>', \" \", text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    for sent in sentences:\n",
    "        if isinstance(sent, str):\n",
    "            sent = re.sub(r'\\<.*?\\>', \" \", sent)\n",
    "            if not sent.rstrip().isdigit() and len(sent.rstrip()) >= 1:\n",
    "                sents.append(sent)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-04-09T21:28:24.952741200Z"
    }
   },
   "id": "45c43d9f2a09e8a7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total_num_sents = 24000 # 1000\n",
    "test_sents = sents[:total_num_sents]"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "b96867011ab3dbfa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "aphasic_sents = []\n",
    "normal_sents = []\n",
    "\n",
    "import inflect\n",
    "from preprocess import postprocess\n",
    "p = inflect.engine()\n",
    "count = 0\n",
    "\n",
    "for sent in test_sents:\n",
    "    # no digits like in aphasiabank\n",
    "    # print(sent)\n",
    "    b = re.findall(\"[0-9]+\", sent)\n",
    "    for i in b:\n",
    "        sent = sent.replace(i, p.number_to_words(i))\n",
    "    \n",
    "    s = preprocess(sent)\n",
    "    aphasic, changed = aphasic_speech(s)\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(f\"Processed {count} sentences, broca utts: {len(aphasic_sents)}\")\n",
    "    if changed and aphasic !=\".\":\n",
    "        # print(sent)\n",
    "        # print(s)\n",
    "        # print(postprocess(aphasic))\n",
    "        # print()\n",
    "        normal_sents.append(s)\n",
    "        aphasic_sents.append(aphasic)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "7b116c75e017a103"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Post process of aphasic sentences\n",
    "Also adding some \"control\" sentences"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ab67054aaaee979"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from datasets import load_dataset\n",
    "# \n",
    "# # ds = load_dataset('stas/c4-en-10k')\n",
    "# ds = load_dataset(\"imdb\")\n",
    "# print(ds)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "a232e0b1491df977"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sentences = aphasic_sents\n",
    "original = normal_sents"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "346435140a64874a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from preprocess import postprocess\n",
    "broca_sents = []\n",
    "original_sents = []\n",
    "for sent, o in zip(sentences, original):\n",
    "    x = postprocess(sent)\n",
    "    if x != \"\":\n",
    "        broca_sents.append(x)\n",
    "        original_sents.append(o)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "6ea26a0b735f444c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.DataFrame(data={\"modified\": broca_sents, \"original\": original_sents}).to_csv(\"data/synthetic_clan_c4.csv\", sep=\",\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "6577bcdc8e5cfc7e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "broca_sents = pd.read_csv(\"data/synthetic_clan_c4.csv\", sep=\",\")[\"modified\"]"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "c8cd793b87630a2c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import inflect\n",
    "from preprocess import preprocess, postprocess\n",
    "p = inflect.engine()\n",
    "import re\n",
    "\n",
    "# post process and pre-process in same way\n",
    "from nltk.tokenize import sent_tokenize\n",
    "new_sents = []\n",
    "sentences = sents[total_num_sents:]\n",
    "for sent in sentences:\n",
    "    if len(new_sents) >= round(len(broca_sents)*2.5755):\n",
    "        break\n",
    "    if isinstance(sent, str):\n",
    "        sent = re.sub(r'\\<.*?\\>', \" \", sent)\n",
    "        if not sent.rstrip().isdigit() and len(sent.rstrip()) >= 1:\n",
    "            # no digits\n",
    "            b = re.findall(\"[0-9]+\", sent)\n",
    "            for i in b:\n",
    "                sent = sent.replace(i, p.number_to_words(i))\n",
    "            sent = preprocess(sent)\n",
    "            sent = postprocess(sent)\n",
    "            if sent != \"\":\n",
    "                new_sents.append(sent)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "e4682d97a579cf7e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(new_sents)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "6f90655ea524e9ce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(broca_sents)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "446fb41e5a3ab5d1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "control_sents = new_sents[:total_num_sents+round(len(broca_sents)*2.5755)]"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "621e4424aaaa2c50"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(broca_sents), len(control_sents))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "8f614650609a6194"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "broca_data = pd.DataFrame(data={\"preprocessed_text\": broca_sents, \"label\": [1]*len(broca_sents)})\n",
    "control_data = pd.DataFrame(data={\"preprocessed_text\": control_sents, \"label\": [0]*len(control_sents)})\n",
    "data_full_scenario = pd.concat([broca_data, control_data], ignore_index=True)\n",
    "data_full_scenario = data_full_scenario.sample(frac=1).reset_index(drop=True)\n",
    "# data_full_scenario.to_csv(\"data/synthetic_clan_test.csv\", sep=\",\", index=False)\n",
    "data_full_scenario.to_csv(\"data/synthetic_clan_merge_c4.csv\", sep=\",\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "de47ecaf3bc979de"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_full_scenario"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "e18e6bd624f8eb25"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
