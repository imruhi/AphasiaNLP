{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# TODO"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4b7abeb946bf588"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import random\n",
    "import re\n",
    "from pattern.text.en import singularize, pluralize, conjugate\n",
    "import enchant\n",
    "from preprocess import preprocess\n",
    "from spacy.matcher import Matcher\n",
    "from string import printable\n",
    "from preprocess import postprocess\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "d = enchant.Dict(\"en_US\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T14:41:54.166119Z",
     "start_time": "2024-05-06T14:41:53.683102700Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "extra = '!\"#$%&\\'()*+-/:;<=>?@[\\\\]^_`{|}~'\n",
    "\n",
    "for x in extra:\n",
    "    printable = printable.replace(x,'',)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T14:41:54.183102700Z",
     "start_time": "2024-05-06T14:41:54.170657600Z"
    }
   },
   "id": "bd77d6afd505a31c"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ,. \\t\\n\\r\\x0b\\x0c'"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "printable"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T14:41:54.234709600Z",
     "start_time": "2024-05-06T14:41:54.184609600Z"
    }
   },
   "id": "cf038f52952eb41c"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "dets = {'Art': ['a', 'an', 'the'],\n",
    "           'Dem': ['this', 'that', 'these', 'those'],\n",
    "           'Poss': ['my', 'your', 'his', 'her', 'its', 'our', 'their']}\n",
    "\n",
    "def det_sub(x):\n",
    "    for _, det in dets.items():\n",
    "        if x.lower() in det:\n",
    "            y = [j for j in det if x!=j]\n",
    "            return random.choice(y)\n",
    "    return \"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T14:41:54.246681500Z",
     "start_time": "2024-05-06T14:41:54.200649300Z"
    }
   },
   "id": "93aae4fad150933"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "dataset_filename = \"../linguistic_model/data/spoken corpus/preprocessed_test_merge.csv\"\n",
    "ds = pd.read_csv(dataset_filename, encoding='utf8', index_col=False).drop(['Unnamed: 0',\"label\"], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T14:41:54.288392600Z",
     "start_time": "2024-05-06T14:41:54.214573500Z"
    }
   },
   "id": "ba74d2f1ffdf9890"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "texts = ds[\"preprocessed_text\"]\n",
    "sents = []\n",
    "for text in texts:\n",
    "    text = re.sub(r'\\<.*?\\>', \" \", text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    for sent in sentences:\n",
    "        if isinstance(sent, str):\n",
    "            sent = re.sub(r'\\<.*?\\>', \" \", sent)\n",
    "            if not sent.rstrip().isdigit() and len(sent.rstrip()) >= 1:\n",
    "                sents.append(sent)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T14:41:54.698632200Z",
     "start_time": "2024-05-06T14:41:54.283537800Z"
    }
   },
   "id": "26d6b39a368e9ca0"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "23207"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sents)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T14:41:54.719989900Z",
     "start_time": "2024-05-06T14:41:54.695122300Z"
    }
   },
   "id": "dcd1727f7d680616"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "total_num_sents = len(sents) # 1000\n",
    "test_sents = sents[:total_num_sents]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T14:41:54.724897900Z",
     "start_time": "2024-05-06T14:41:54.717167200Z"
    }
   },
   "id": "d012d17dee191198"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "m0sa_lim = 0.7      # m:0s:a             (30%) done\n",
    "ms_lim = 0.7        # m:+s(:a)           (30%) done\n",
    "sgc_lim = 0.6       # s:r:gc             (40%) done \n",
    "rep_lim = 0.9       # repetition         (10%) done"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T14:41:54.775006300Z",
     "start_time": "2024-05-06T14:41:54.724897900Z"
    }
   },
   "id": "1e38b4281b3f84b4"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def aphasic_speech(text):\n",
    "    doc = nlp(text)\n",
    "    vp_pattern = [[{'POS': 'VERB', 'OP': '?'},\n",
    "                   {'POS': 'ADV', 'OP': '*'},\n",
    "                   {'POS': 'AUX', 'OP': '*'},\n",
    "                   {'POS': 'VERB', 'OP': '+'}]]\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add(\"Verb phrase\", vp_pattern)\n",
    "    n = 15\n",
    "    aphasic_utt = \"\"\n",
    "    length = len(re.findall(\"[a-zA-Z_]+\", text))\n",
    "    \n",
    "    nouns = []\n",
    "    verbs = []\n",
    "    determiners = []\n",
    "    prepositions = []\n",
    "    adjectives = []\n",
    "    adverbs = []\n",
    "    interjections = []\n",
    "    open_close = np.random.gamma(shape=4.99415, scale=1/3.558095)\n",
    "    add = False\n",
    "    \n",
    "    # count no. of respective POS\n",
    "    for tok in doc:\n",
    "        if tok.pos_ == \"NOUN\":\n",
    "            nouns.append(tok.text)\n",
    "        elif tok.pos_ == \"VERB\" or tok.dep_ == \"cop\" or tok.tag_ in [\"VBD\", \"VBN\"]:\n",
    "            verbs.append(tok.text)\n",
    "        # det:art and det:dem only\n",
    "        elif tok.dep_ == \"det\" and (\"Dem\" in tok.morph.get('PronType') or \"Art\" in tok.morph.get('PronType')):\n",
    "            determiners.append(tok.text)\n",
    "        elif tok.dep_ == \"prep\":\n",
    "            prepositions.append(tok.text)\n",
    "        elif tok.pos_ == \"ADJ\":\n",
    "            adjectives.append(tok.text)\n",
    "        elif tok.pos_ == \"ADV\":\n",
    "            adverbs.append(tok.text)\n",
    "        elif tok.pos_ == \"INTJ\":\n",
    "            interjections.append(tok.pos_)\n",
    "            \n",
    "    open_class_num = len(nouns) + len(verbs) + len(adjectives) + len(adverbs)\n",
    "    closed_class_num = length - open_class_num - len(interjections)\n",
    "    \n",
    "    # acc to frank, not removing only adding\n",
    "    if closed_class_num != 0:\n",
    "        if open_close > open_class_num/closed_class_num:\n",
    "            add = True\n",
    "    \n",
    "    # discard sentences of 15 and above length\n",
    "    # and with symbols\n",
    "    if length <= n and not set(text).difference(printable):\n",
    "        m0sa_prob = random.uniform(0,1)     # m:0s:a\n",
    "        ms_prob = random.uniform(0,1)       # m:+s(:a)\n",
    "        sgc_prob = random.uniform(0,1)      # s:r:gc\n",
    "        rep_prob = random.uniform(0,1)      # repetitions\n",
    "        \n",
    "        # get NPs\n",
    "        noun_phrases = set()\n",
    "\n",
    "        for nc in doc.noun_chunks:\n",
    "            for nop in [nc, doc[nc.root.left_edge.i:nc.root.right_edge.i + 1]]:\n",
    "                noun_phrases.add(nop.text.strip())\n",
    "                # get VPs\n",
    "        verb_phrases = matcher(doc)\n",
    "        verb_phrases = [doc[start:end] for _, start, end in verb_phrases]\n",
    "\n",
    "        try:\n",
    "            ratio = len(noun_phrases) / len(verb_phrases)\n",
    "        except:\n",
    "            return False, aphasic_utt\n",
    "\n",
    "        X = np.random.uniform(0, 1)\n",
    "        \n",
    "        if ratio > 2 and X < 0.8:\n",
    "            # skip sentence if np/vp too big with prob of 80%\n",
    "            return False, aphasic_utt\n",
    "        else:\n",
    "            # dont skip sentence\n",
    "            for tok in doc:\n",
    "                \n",
    "                if tok.pos_ == \"NOUN\":\n",
    "                    # m:0s:a and m:+s(:a)\n",
    "                    if m0sa_prob >= m0sa_lim or ms_prob >= ms_lim:\n",
    "                        if \"Plur\" in tok.morph.get(\"Number\"):\n",
    "                            aphasic_utt += singularize(tok.text) + ' '\n",
    "                        elif \"Sing\" in tok.morph.get(\"Number\"):\n",
    "                            aphasic_utt += pluralize(tok.text) + ' ' \n",
    "                    else:\n",
    "                        aphasic_utt += tok.text + \" \"\n",
    "                \n",
    "                # Handle pronouns\n",
    "                elif tok.pos_ == \"PRON\":\n",
    "                    # s:r:gc:pro  \n",
    "                    if sgc_prob >= sgc_lim:\n",
    "                        if tok.pos_ == \"DET\" or \"Dem\" in tok.morph.get('PronType') or \"Yes\" in tok.morph.get('Poss'):\n",
    "                            sub = det_sub(tok.text) \n",
    "                            aphasic_utt += sub + \" \"\n",
    "                            if rep_prob >= rep_lim:\n",
    "                                aphasic_utt += sub + \" \"\n",
    "                        else:\n",
    "                            if rep_prob >= rep_lim:\n",
    "                                aphasic_utt += tok.text + \" \"\n",
    "                            aphasic_utt += tok.text + \" \"\n",
    "                    else:\n",
    "                        aphasic_utt += tok.text + \" \"\n",
    "                        if rep_prob >= rep_lim:\n",
    "                            aphasic_utt += tok.text + \" \"\n",
    "                elif tok.pos_ in [\"DET\", \"PART\"] or tok.dep_ in [\"prep\"]:\n",
    "                    # determiners, prepositions, particle discard with 60-70%\n",
    "                    Y = np.random.uniform(0, 1)\n",
    "                    prob = np.random.uniform(0.6,0.7)\n",
    "                    if Y > prob:\n",
    "                        aphasic_utt += tok.text + \" \"\n",
    "                        \n",
    "                elif tok.pos_ in [\"ADJ\", \"ADV\"]:\n",
    "                    # adjectives, adverbs discard with 50%\n",
    "                    Z = np.random.uniform(0, 1)\n",
    "                    if Z < 0.5:\n",
    "                        aphasic_utt += tok.text + \" \"\n",
    "                        \n",
    "                elif tok.pos_ == \"VERB\":\n",
    "                    # verbs lemmatize with 50%\n",
    "                    Z = np.random.uniform(0, 1)\n",
    "                    if Z < 0.5:\n",
    "                        aphasic_utt += tok.lemma_ + \" \"\n",
    "                        \n",
    "                elif tok.pos == \"INTJ\":\n",
    "                    # close class from PC analysis \n",
    "                    if rep_prob >= rep_lim or add:\n",
    "                        aphasic_utt += tok.text + \" \"\n",
    "                    aphasic_utt += tok.text + \" \"\n",
    "                \n",
    "                else:\n",
    "                    # all other pos remain\n",
    "                    aphasic_utt += tok.text + \" \"\n",
    "                \n",
    "        # exclusion criterias\n",
    "        lower = round(length * (1/3))\n",
    "        higher = round(length * (2/3))\n",
    "        aphasic_utt = postprocess(aphasic_utt)\n",
    "        new_length = len(re.findall(\"[a-zA-Z_]+\", aphasic_utt))\n",
    "        \n",
    "        if lower <= new_length <= higher:\n",
    "            return True, aphasic_utt\n",
    "        else:\n",
    "            return False, aphasic_utt\n",
    "    \n",
    "    else:\n",
    "        return False, aphasic_utt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T14:41:54.775898200Z",
     "start_time": "2024-05-06T14:41:54.753927400Z"
    }
   },
   "id": "a662af15af5493c7"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000 sentences, broca utts: 193\n",
      "Processed 2000 sentences, broca utts: 412\n",
      "Processed 3000 sentences, broca utts: 592\n",
      "Processed 4000 sentences, broca utts: 803\n",
      "Processed 5000 sentences, broca utts: 989\n",
      "Processed 6000 sentences, broca utts: 1169\n",
      "Processed 7000 sentences, broca utts: 1382\n",
      "Processed 8000 sentences, broca utts: 1603\n",
      "Processed 9000 sentences, broca utts: 1798\n",
      "Processed 10000 sentences, broca utts: 2008\n",
      "Processed 11000 sentences, broca utts: 2209\n",
      "Processed 12000 sentences, broca utts: 2420\n",
      "Processed 13000 sentences, broca utts: 2610\n",
      "Processed 14000 sentences, broca utts: 2766\n",
      "Processed 15000 sentences, broca utts: 2984\n",
      "Processed 16000 sentences, broca utts: 3140\n",
      "Processed 17000 sentences, broca utts: 3346\n",
      "Processed 18000 sentences, broca utts: 3589\n",
      "Processed 19000 sentences, broca utts: 3793\n",
      "Processed 20000 sentences, broca utts: 3976\n",
      "Processed 21000 sentences, broca utts: 4170\n",
      "Processed 22000 sentences, broca utts: 4411\n",
      "Processed 23000 sentences, broca utts: 4618\n"
     ]
    }
   ],
   "source": [
    "aphasic_sents = []\n",
    "normal_sents = []\n",
    "new_sents = []\n",
    "\n",
    "import inflect\n",
    "from preprocess import postprocess\n",
    "p = inflect.engine()\n",
    "count = 0\n",
    "num_sents = 9000 # how many aphasic sentences?\n",
    "\n",
    "for sent in test_sents:\n",
    "    # no digits like in aphasiabank\n",
    "    # print(sent)\n",
    "    b = re.findall(\"[0-9]+\", sent)\n",
    "    for i in b:\n",
    "        sent = sent.replace(i, p.number_to_words(i))\n",
    "    \n",
    "    s = preprocess(sent)\n",
    "    \n",
    "    if len(aphasic_sents) >= num_sents:\n",
    "        changed = False\n",
    "        aphasic = \"\"\n",
    "    else:\n",
    "        changed, aphasic = aphasic_speech(s)\n",
    "        count += 1\n",
    "    \n",
    "    if len(aphasic_sents) >= num_sents and len(new_sents) >= num_sents * 2.5911:\n",
    "        break\n",
    "        \n",
    "    if count % 1000 == 0:\n",
    "        print(f\"Processed {count} sentences, broca utts: {len(aphasic_sents)}\")\n",
    "    \n",
    "    # min length is 3?\n",
    "    if changed and aphasic !=\".\" and (postprocess(s) != aphasic): #and 3 <= len(re.findall(\"[a-zA-Z_]+\", aphasic)):\n",
    "        # print(sent)\n",
    "        # print(postprocess(s))\n",
    "        # print(postprocess(aphasic))\n",
    "        # print()\n",
    "        normal_sents.append(postprocess(s))\n",
    "        aphasic_sents.append(aphasic)\n",
    "    if not changed:\n",
    "        new_sents.append(postprocess(s))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T14:43:31.538608Z",
     "start_time": "2024-05-06T14:41:54.770988Z"
    }
   },
   "id": "22912266114be251"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "sentences = aphasic_sents\n",
    "original = normal_sents"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T14:43:31.549037200Z",
     "start_time": "2024-05-06T14:43:31.538608Z"
    }
   },
   "id": "48d2a87faba3ccf0"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from preprocess import postprocess\n",
    "broca_sents = []\n",
    "original_sents = []\n",
    "for sent, o in zip(sentences, original):\n",
    "    x = postprocess(sent)\n",
    "    if x != \"\":\n",
    "        broca_sents.append(x)\n",
    "        original_sents.append(o)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T14:43:31.610663700Z",
     "start_time": "2024-05-06T14:43:31.552462200Z"
    }
   },
   "id": "c51b6447aa272856"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "pd.DataFrame(data={\"modified\": broca_sents, \"original\": original_sents}).to_csv(\"data/new_3.csv\", sep=\",\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T14:43:31.647044800Z",
     "start_time": "2024-05-06T14:43:31.614638200Z"
    }
   },
   "id": "5ad1eafde2b154dd"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "broca_sents = pd.read_csv(\"data/new_3.csv\", sep=\",\")[\"modified\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T14:43:31.669653800Z",
     "start_time": "2024-05-06T14:43:31.638717200Z"
    }
   },
   "id": "46bc092a7122b513"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "broca_sents = broca_sents[:num_sents]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T14:43:31.684696800Z",
     "start_time": "2024-05-06T14:43:31.668828100Z"
    }
   },
   "id": "9bd0b1671dcaa5d1"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "12033"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(len(broca_sents)*2.5911)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T14:43:31.697168800Z",
     "start_time": "2024-05-06T14:43:31.680327500Z"
    }
   },
   "id": "4b54c89b88e9dd27"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "control_sents = new_sents[:round(len(broca_sents)*2.5911)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T14:43:31.709944300Z",
     "start_time": "2024-05-06T14:43:31.693155600Z"
    }
   },
   "id": "8ec64bced1906d27"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "broca_data = pd.DataFrame(data={\"preprocessed_text\": broca_sents, \"label\": [1]*len(broca_sents)})\n",
    "control_data = pd.DataFrame(data={\"preprocessed_text\": control_sents, \"label\": [0]*len(control_sents)})\n",
    "data_full_scenario = pd.concat([broca_data, control_data], ignore_index=True)\n",
    "data_full_scenario = data_full_scenario.sample(frac=1).reset_index(drop=True)\n",
    "data_full_scenario[\"preprocessed_text\"] = [re.sub(r'[^\\w\\s]','',x) for x in data_full_scenario[\"preprocessed_text\"]]\n",
    "data_full_scenario.to_csv(\"data/new_3_merge.csv\", sep=\",\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T14:43:31.747177500Z",
     "start_time": "2024-05-06T14:43:31.706198Z"
    }
   },
   "id": "6b6635d33f70b4a8"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "                                       preprocessed_text  label\n0                             i can give you a rice cake      0\n1                                       he used the four      0\n2                 god has created us made us like we are      0\n3           you smoked it down into the cork did not you      0\n4                                     ll demos cratos ll      0\n...                                                  ...    ...\n16672                 what shall i the righteousness god      1\n16673                                        what i mean      0\n16674  well i do not think henry was the horrible per...      0\n16675                 that is where we bought the car at      0\n16676  jewelry and rings and things would be missing ...      0\n\n[16677 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>preprocessed_text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>i can give you a rice cake</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>he used the four</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>god has created us made us like we are</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>you smoked it down into the cork did not you</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ll demos cratos ll</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>16672</th>\n      <td>what shall i the righteousness god</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>16673</th>\n      <td>what i mean</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>16674</th>\n      <td>well i do not think henry was the horrible per...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>16675</th>\n      <td>that is where we bought the car at</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>16676</th>\n      <td>jewelry and rings and things would be missing ...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>16677 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_full_scenario"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T14:43:31.767370400Z",
     "start_time": "2024-05-06T14:43:31.751361Z"
    }
   },
   "id": "7675b36856541d1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
