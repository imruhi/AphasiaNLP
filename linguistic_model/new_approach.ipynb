{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# TODO\n",
    "add code for the extra errors, run it and check results for both classifiers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4b7abeb946bf588"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import random\n",
    "import re\n",
    "from pattern.text.en import singularize, pluralize, conjugate\n",
    "import enchant\n",
    "from preprocess import preprocess\n",
    "from spacy.matcher import Matcher\n",
    "from string import printable\n",
    "from preprocess import postprocess\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "d = enchant.Dict(\"en_US\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T11:51:59.293695100Z",
     "start_time": "2024-05-06T11:51:58.549523100Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "extra = '!\"#$%&\\'()*+-/:;<=>?@[\\\\]^_`{|}~'\n",
    "\n",
    "for x in extra:\n",
    "    printable = printable.replace(x,'',)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T11:51:59.309759400Z",
     "start_time": "2024-05-06T11:51:59.143329700Z"
    }
   },
   "id": "bd77d6afd505a31c"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ,. \\t\\n\\r\\x0b\\x0c'"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "printable"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T11:51:59.309759400Z",
     "start_time": "2024-05-06T11:51:59.156118100Z"
    }
   },
   "id": "cf038f52952eb41c"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "dataset_filename = \"../linguistic_model/data/spoken corpus/preprocessed_test_merge.csv\"\n",
    "ds = pd.read_csv(dataset_filename, encoding='utf8', index_col=False).drop(['Unnamed: 0',\"label\"], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T11:51:59.313835200Z",
     "start_time": "2024-05-06T11:51:59.180770600Z"
    }
   },
   "id": "ba74d2f1ffdf9890"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "texts = ds[\"preprocessed_text\"]\n",
    "sents = []\n",
    "for text in texts:\n",
    "    text = re.sub(r'\\<.*?\\>', \" \", text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    for sent in sentences:\n",
    "        if isinstance(sent, str):\n",
    "            sent = re.sub(r'\\<.*?\\>', \" \", sent)\n",
    "            if not sent.rstrip().isdigit() and len(sent.rstrip()) >= 1:\n",
    "                sents.append(sent)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T11:51:59.680971100Z",
     "start_time": "2024-05-06T11:51:59.237291600Z"
    }
   },
   "id": "26d6b39a368e9ca0"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "23207"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sents)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T11:51:59.725612300Z",
     "start_time": "2024-05-06T11:51:59.680971100Z"
    }
   },
   "id": "dcd1727f7d680616"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "total_num_sents = len(sents) # 1000\n",
    "test_sents = sents[:total_num_sents]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T11:51:59.737776400Z",
     "start_time": "2024-05-06T11:51:59.692975800Z"
    }
   },
   "id": "d012d17dee191198"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "m0sa_lim = 0.7      # m:0s:a             (30%) done\n",
    "ms_lim = 0.7        # m:+s(:a)           (30%) done\n",
    "sgc_lim = 0.6       # s:r:gc             (40%)\n",
    "suk_lim = 0.7       # s:uk               (30%)\n",
    "rep_lim = 0.9       # repetition          (10%)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T11:51:59.743248100Z",
     "start_time": "2024-05-06T11:51:59.709217400Z"
    }
   },
   "id": "1e38b4281b3f84b4"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "def aphasic_speech(text):\n",
    "    doc = nlp(text)\n",
    "    vp_pattern = [[{'POS': 'VERB', 'OP': '?'},\n",
    "                   {'POS': 'ADV', 'OP': '*'},\n",
    "                   {'POS': 'AUX', 'OP': '*'},\n",
    "                   {'POS': 'VERB', 'OP': '+'}]]\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add(\"Verb phrase\", vp_pattern)\n",
    "    n = 15\n",
    "    aphasic_utt = \"\"\n",
    "    length = len(re.findall(\"[a-zA-Z_]+\", text))\n",
    "    \n",
    "    nouns = []\n",
    "    verbs = []\n",
    "    determiners = []\n",
    "    prepositions = []\n",
    "    adjectives = []\n",
    "    adverbs = []\n",
    "    interjections = []\n",
    "    open_close = np.random.gamma(shape=4.99415, scale=1/3.558095)\n",
    "    add = False\n",
    "    \n",
    "    # count no. of respective POS\n",
    "    for tok in doc:\n",
    "        if tok.pos_ == \"NOUN\":\n",
    "            nouns.append(tok.text)\n",
    "        elif tok.pos_ == \"VERB\" or tok.dep_ == \"cop\" or tok.tag_ in [\"VBD\", \"VBN\"]:\n",
    "            verbs.append(tok.text)\n",
    "        # det:art and det:dem only\n",
    "        elif tok.dep_ == \"det\" and (\"Dem\" in tok.morph.get('PronType') or \"Art\" in tok.morph.get('PronType')):\n",
    "            determiners.append(tok.text)\n",
    "        elif tok.dep_ == \"prep\":\n",
    "            prepositions.append(tok.text)\n",
    "        elif tok.pos_ == \"ADJ\":\n",
    "            adjectives.append(tok.text)\n",
    "        elif tok.pos_ == \"ADV\":\n",
    "            adverbs.append(tok.text)\n",
    "        elif tok.pos_ == \"INTJ\":\n",
    "            interjections.append(tok.pos_)\n",
    "            \n",
    "    open_class_num = len(nouns) + len(verbs) + len(adjectives) + len(adverbs)\n",
    "    closed_class_num = length - open_class_num - len(interjections)\n",
    "    \n",
    "    # acc to frank, not removing only adding\n",
    "    if closed_class_num != 0:\n",
    "        if open_close > open_class_num/closed_class_num:\n",
    "            add = True\n",
    "    \n",
    "    # discard sentences of 15 and above length\n",
    "    # and with symbols\n",
    "    if length <= n and not set(text).difference(printable):\n",
    "        m0sa_prob = random.uniform(0,1)     # m:0s:a\n",
    "        ms_prob = random.uniform(0,1)       # m:+s(:a)\n",
    "        sgc_prob = random.uniform(0,1)      # s:r:gc\n",
    "        suk_prob = random.uniform(0,1)      # s:uk  \n",
    "        sr_prob = random.uniform(0,1)       # s:r (for pronouns)\n",
    "        rep_prob = random.uniform(0,1)      # repetitions\n",
    "        \n",
    "        # get NPs\n",
    "        noun_phrases = set()\n",
    "\n",
    "        for nc in doc.noun_chunks:\n",
    "            for nop in [nc, doc[nc.root.left_edge.i:nc.root.right_edge.i + 1]]:\n",
    "                noun_phrases.add(nop.text.strip())\n",
    "                # get VPs\n",
    "        verb_phrases = matcher(doc)\n",
    "        verb_phrases = [doc[start:end] for _, start, end in verb_phrases]\n",
    "\n",
    "        try:\n",
    "            ratio = len(noun_phrases) / len(verb_phrases)\n",
    "        except:\n",
    "            return False, aphasic_utt\n",
    "\n",
    "        X = np.random.uniform(0, 1)\n",
    "        \n",
    "        if ratio > 2 and X < 0.8:\n",
    "            # skip sentence if np/vp too big with prob of 80%\n",
    "            return False, aphasic_utt\n",
    "        else:\n",
    "            # dont skip sentence\n",
    "            for tok in doc:\n",
    "                \n",
    "                if tok.pos_ == \"NOUN\":\n",
    "                    if m0sa_prob >= m0sa_lim or ms_prob >= ms_lim:\n",
    "                        if \"Plur\" in tok.morph.get(\"Number\"):\n",
    "                            aphasic_utt += singularize(tok.text) + ' '\n",
    "                        elif \"Sing\" in tok.morph.get(\"Number\"):\n",
    "                            aphasic_utt += pluralize(tok.text) + ' ' \n",
    "                    else:\n",
    "                        aphasic_utt += tok.text + \" \"\n",
    "                        \n",
    "                if tok.pos_ in [\"DET\", \"PART\"] or tok.dep_ in [\"prep\"]:\n",
    "                    # determiners, prepositions, particle discard with 60-70%\n",
    "                    Y = np.random.uniform(0, 1)\n",
    "                    prob = np.random.uniform(0.6,0.7)\n",
    "                    if Y > prob:\n",
    "                        aphasic_utt += tok.text + \" \"\n",
    "                        \n",
    "                elif tok.pos_ in [\"ADJ\", \"ADV\"]:\n",
    "                    # adjectives, adverbs discard with 50%\n",
    "                    Z = np.random.uniform(0, 1)\n",
    "                    if Z < 0.5:\n",
    "                        aphasic_utt += tok.text + \" \"\n",
    "                        \n",
    "                elif tok.pos_ == \"VERB\":\n",
    "                    # verbs lemmatize with 50%\n",
    "                    Z = np.random.uniform(0, 1)\n",
    "                    if Z < 0.5:\n",
    "                        aphasic_utt += tok.lemma_ + \" \"\n",
    "                        \n",
    "                elif tok.pos == \"INTJ\":\n",
    "                    # close class from PC analysis \n",
    "                    if add:\n",
    "                        aphasic_utt += tok.text + \" \"\n",
    "                    aphasic_utt += tok.text + \" \"\n",
    "                \n",
    "                else:\n",
    "                    # all other pos remain\n",
    "                    aphasic_utt += tok.text + \" \"\n",
    "                \n",
    "        # exclusion criterias\n",
    "        lower = round(length * (1/3))\n",
    "        higher = round(length * (2/3))\n",
    "        aphasic_utt = postprocess(aphasic_utt)\n",
    "        new_length = len(re.findall(\"[a-zA-Z_]+\", aphasic_utt))\n",
    "        \n",
    "        if lower <= new_length <= higher:\n",
    "            return True, aphasic_utt\n",
    "        else:\n",
    "            return False, aphasic_utt\n",
    "    \n",
    "    else:\n",
    "        return False, aphasic_utt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T11:51:59.797037300Z",
     "start_time": "2024-05-06T11:51:59.746255300Z"
    }
   },
   "id": "a662af15af5493c7"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000 sentences, broca utts: 212\n",
      "Processed 2000 sentences, broca utts: 426\n",
      "Processed 3000 sentences, broca utts: 617\n",
      "Processed 4000 sentences, broca utts: 838\n",
      "Processed 5000 sentences, broca utts: 1058\n",
      "Processed 6000 sentences, broca utts: 1250\n",
      "Processed 7000 sentences, broca utts: 1473\n",
      "Processed 8000 sentences, broca utts: 1707\n",
      "Processed 9000 sentences, broca utts: 1896\n",
      "Processed 10000 sentences, broca utts: 2123\n",
      "Processed 11000 sentences, broca utts: 2354\n",
      "Processed 12000 sentences, broca utts: 2591\n",
      "Processed 13000 sentences, broca utts: 2784\n",
      "Processed 14000 sentences, broca utts: 2954\n",
      "Processed 15000 sentences, broca utts: 3165\n",
      "Processed 16000 sentences, broca utts: 3335\n",
      "Processed 17000 sentences, broca utts: 3563\n",
      "Processed 18000 sentences, broca utts: 3829\n",
      "Processed 19000 sentences, broca utts: 4033\n",
      "Processed 20000 sentences, broca utts: 4235\n",
      "Processed 21000 sentences, broca utts: 4446\n",
      "Processed 22000 sentences, broca utts: 4724\n",
      "Processed 23000 sentences, broca utts: 4945\n"
     ]
    }
   ],
   "source": [
    "aphasic_sents = []\n",
    "normal_sents = []\n",
    "new_sents = []\n",
    "\n",
    "import inflect\n",
    "from preprocess import postprocess\n",
    "p = inflect.engine()\n",
    "count = 0\n",
    "num_sents = 9000 # how many aphasic sentences?\n",
    "\n",
    "for sent in test_sents:\n",
    "    # no digits like in aphasiabank\n",
    "    # print(sent)\n",
    "    b = re.findall(\"[0-9]+\", sent)\n",
    "    for i in b:\n",
    "        sent = sent.replace(i, p.number_to_words(i))\n",
    "    \n",
    "    s = preprocess(sent)\n",
    "    \n",
    "    if len(aphasic_sents) >= num_sents:\n",
    "        changed = False\n",
    "        aphasic = \"\"\n",
    "    else:\n",
    "        changed, aphasic = aphasic_speech(s)\n",
    "        count += 1\n",
    "    \n",
    "    if len(aphasic_sents) >= num_sents and len(new_sents) >= num_sents * 2.5911:\n",
    "        break\n",
    "        \n",
    "    if count % 1000 == 0:\n",
    "        print(f\"Processed {count} sentences, broca utts: {len(aphasic_sents)}\")\n",
    "    \n",
    "    # min length is 3?\n",
    "    if changed and aphasic !=\".\" and (postprocess(s) != aphasic): #and 3 <= len(re.findall(\"[a-zA-Z_]+\", aphasic)):\n",
    "        # print(sent)\n",
    "        # print(postprocess(s))\n",
    "        # print(postprocess(aphasic))\n",
    "        # print()\n",
    "        normal_sents.append(postprocess(s))\n",
    "        aphasic_sents.append(aphasic)\n",
    "    if not changed:\n",
    "        new_sents.append(postprocess(s))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T11:53:41.846720Z",
     "start_time": "2024-05-06T11:51:59.758675600Z"
    }
   },
   "id": "22912266114be251"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "sentences = aphasic_sents\n",
    "original = normal_sents"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T11:53:41.859430500Z",
     "start_time": "2024-05-06T11:53:41.848834100Z"
    }
   },
   "id": "48d2a87faba3ccf0"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from preprocess import postprocess\n",
    "broca_sents = []\n",
    "original_sents = []\n",
    "for sent, o in zip(sentences, original):\n",
    "    x = postprocess(sent)\n",
    "    if x != \"\":\n",
    "        broca_sents.append(x)\n",
    "        original_sents.append(o)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T11:53:41.935034900Z",
     "start_time": "2024-05-06T11:53:41.863826Z"
    }
   },
   "id": "c51b6447aa272856"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "pd.DataFrame(data={\"modified\": broca_sents, \"original\": original_sents}).to_csv(\"data/new_1.csv\", sep=\",\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T11:53:41.950126500Z",
     "start_time": "2024-05-06T11:53:41.937490800Z"
    }
   },
   "id": "5ad1eafde2b154dd"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "broca_sents = pd.read_csv(\"data/new_1.csv\", sep=\",\")[\"modified\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T11:53:41.995119100Z",
     "start_time": "2024-05-06T11:53:41.953132Z"
    }
   },
   "id": "46bc092a7122b513"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "broca_sents = broca_sents[:num_sents]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T11:53:42.007271100Z",
     "start_time": "2024-05-06T11:53:41.999119Z"
    }
   },
   "id": "9bd0b1671dcaa5d1"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "12937"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(len(broca_sents)*2.5911)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T11:53:42.060509100Z",
     "start_time": "2024-05-06T11:53:42.012247400Z"
    }
   },
   "id": "4b54c89b88e9dd27"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "control_sents = new_sents[:round(len(broca_sents)*2.5911)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T11:53:42.066386400Z",
     "start_time": "2024-05-06T11:53:42.023800700Z"
    }
   },
   "id": "8ec64bced1906d27"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "broca_data = pd.DataFrame(data={\"preprocessed_text\": broca_sents, \"label\": [1]*len(broca_sents)})\n",
    "control_data = pd.DataFrame(data={\"preprocessed_text\": control_sents, \"label\": [0]*len(control_sents)})\n",
    "data_full_scenario = pd.concat([broca_data, control_data], ignore_index=True)\n",
    "data_full_scenario = data_full_scenario.sample(frac=1).reset_index(drop=True)\n",
    "data_full_scenario[\"preprocessed_text\"] = [re.sub(r'[^\\w\\s]','',x) for x in data_full_scenario[\"preprocessed_text\"]]\n",
    "data_full_scenario.to_csv(\"data/new_1_merge.csv\", sep=\",\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T11:53:42.094808900Z",
     "start_time": "2024-05-06T11:53:42.040826Z"
    }
   },
   "id": "6b6635d33f70b4a8"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "                                       preprocessed_text  label\n0      eighty percent of that would make it eighty tw...      0\n1                              molecules are move slowly      1\n2                                          and many more      0\n3                                       did you enjoy it      0\n4                           if you are dad it is another      0\n...                                                  ...    ...\n17925    i she would she probably me up almost take some      1\n17926   oh we can also start to see bubbles in the water      0\n17927                                          i want go      1\n17928                           but it was kind of crazy      0\n17929  i have had had a lotta practice at this sorta ...      0\n\n[17930 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>preprocessed_text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>eighty percent of that would make it eighty tw...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>molecules are move slowly</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>and many more</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>did you enjoy it</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>if you are dad it is another</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>17925</th>\n      <td>i she would she probably me up almost take some</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>17926</th>\n      <td>oh we can also start to see bubbles in the water</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17927</th>\n      <td>i want go</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>17928</th>\n      <td>but it was kind of crazy</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17929</th>\n      <td>i have had had a lotta practice at this sorta ...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>17930 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_full_scenario"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T11:53:42.114861Z",
     "start_time": "2024-05-06T11:53:42.094808900Z"
    }
   },
   "id": "7675b36856541d1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
