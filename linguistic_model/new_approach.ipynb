{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# TODO\n",
    "run for closed class words feature and check results for both classifiers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4b7abeb946bf588"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import random\n",
    "import re\n",
    "from pattern.text.en import singularize, pluralize, conjugate\n",
    "import enchant\n",
    "from preprocess import preprocess\n",
    "from spacy.matcher import Matcher\n",
    "from string import printable\n",
    "from preprocess import postprocess\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "d = enchant.Dict(\"en_US\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-03T15:53:53.176964800Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "extra = '!\"#$%&\\'()*+-/:;<=>?@[\\\\]^_`{|}~'\n",
    "\n",
    "for x in extra:\n",
    "    printable = printable.replace(x,'',)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T15:53:53.472100200Z",
     "start_time": "2024-05-03T15:53:53.456579900Z"
    }
   },
   "id": "bd77d6afd505a31c"
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "data": {
      "text/plain": "'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ,. \\t\\n\\r\\x0b\\x0c'"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "printable"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T15:53:53.518987100Z",
     "start_time": "2024-05-03T15:53:53.472100200Z"
    }
   },
   "id": "cf038f52952eb41c"
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "dataset_filename = \"../linguistic_model/data/spoken corpus/preprocessed_test_merge.csv\"\n",
    "ds = pd.read_csv(dataset_filename, encoding='utf8', index_col=False).drop(['Unnamed: 0',\"label\"], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T15:53:53.534348700Z",
     "start_time": "2024-05-03T15:53:53.487665800Z"
    }
   },
   "id": "ba74d2f1ffdf9890"
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "texts = ds[\"preprocessed_text\"]\n",
    "sents = []\n",
    "for text in texts:\n",
    "    text = re.sub(r'\\<.*?\\>', \" \", text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    for sent in sentences:\n",
    "        if isinstance(sent, str):\n",
    "            sent = re.sub(r'\\<.*?\\>', \" \", sent)\n",
    "            if not sent.rstrip().isdigit() and len(sent.rstrip()) >= 1:\n",
    "                sents.append(sent)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T15:53:53.777093500Z",
     "start_time": "2024-05-03T15:53:53.602439500Z"
    }
   },
   "id": "26d6b39a368e9ca0"
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "data": {
      "text/plain": "23207"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sents)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T15:53:53.793746700Z",
     "start_time": "2024-05-03T15:53:53.778099Z"
    }
   },
   "id": "dcd1727f7d680616"
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "total_num_sents = len(sents) # 1000\n",
    "test_sents = sents[:total_num_sents]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T15:53:53.838280300Z",
     "start_time": "2024-05-03T15:53:53.793746700Z"
    }
   },
   "id": "d012d17dee191198"
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "def aphasic_speech(text):\n",
    "    doc = nlp(text)\n",
    "    vp_pattern = [[{'POS': 'VERB', 'OP': '?'},\n",
    "                   {'POS': 'ADV', 'OP': '*'},\n",
    "                   {'POS': 'AUX', 'OP': '*'},\n",
    "                   {'POS': 'VERB', 'OP': '+'}]]\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add(\"Verb phrase\", vp_pattern)\n",
    "    n = 15\n",
    "    aphasic_utt = \"\"\n",
    "    length = len(re.findall(\"[a-zA-Z_]+\", text))\n",
    "    \n",
    "    nouns = []\n",
    "    verbs = []\n",
    "    determiners = []\n",
    "    prepositions = []\n",
    "    adjectives = []\n",
    "    adverbs = []\n",
    "    interjections = []\n",
    "    nv_control = np.random.gamma(shape=49.65696, scale=1/50.32268)\n",
    "    open_close = np.random.gamma(shape=4.99415, scale=1/3.558095)\n",
    "    add = False\n",
    "    \n",
    "    # count no. of respective POS\n",
    "    for tok in doc:\n",
    "        if tok.pos_ == \"NOUN\":\n",
    "            nouns.append(tok.text)\n",
    "        elif tok.pos_ == \"VERB\" or tok.dep_ == \"cop\" or tok.tag_ in [\"VBD\", \"VBN\"]:\n",
    "            verbs.append(tok.text)\n",
    "        # det:art and det:dem only\n",
    "        elif tok.dep_ == \"det\" and (\"Dem\" in tok.morph.get('PronType') or \"Art\" in tok.morph.get('PronType')):\n",
    "            determiners.append(tok.text)\n",
    "        elif tok.dep_ == \"prep\":\n",
    "            prepositions.append(tok.text)\n",
    "        elif tok.pos_ == \"ADJ\":\n",
    "            adjectives.append(tok.text)\n",
    "        elif tok.pos_ == \"ADV\":\n",
    "            adverbs.append(tok.text)\n",
    "        elif tok.pos_ == \"INTJ\":\n",
    "            interjections.append(tok.pos_)\n",
    "            \n",
    "    open_class_num = len(nouns) + len(verbs) + len(adjectives) + len(adverbs)\n",
    "    closed_class_num = length - open_class_num - len(interjections)\n",
    "    \n",
    "    if open_close > open_class_num/closed_class_num:\n",
    "        add = True\n",
    "    \n",
    "    # discard sentences of 15 and above length\n",
    "    # and with symbols\n",
    "    if length <= n and not set(text).difference(printable):\n",
    "        # get NPs\n",
    "        noun_phrases = set()\n",
    "\n",
    "        for nc in doc.noun_chunks:\n",
    "            for nop in [nc, doc[nc.root.left_edge.i:nc.root.right_edge.i + 1]]:\n",
    "                noun_phrases.add(nop.text.strip())\n",
    "                # get VPs\n",
    "        verb_phrases = matcher(doc)\n",
    "        verb_phrases = [doc[start:end] for _, start, end in verb_phrases]\n",
    "\n",
    "        try:\n",
    "            ratio = len(noun_phrases) / len(verb_phrases)\n",
    "        except:\n",
    "            return False, aphasic_utt\n",
    "\n",
    "        X = np.random.uniform(0, 1)\n",
    "        \n",
    "        if ratio > 2 and X < 0.8:\n",
    "            # skip sentence if np/vp too big with prob of 80%\n",
    "            return False, aphasic_utt\n",
    "        else:\n",
    "            # dont skip sentence\n",
    "            for tok in doc:\n",
    "                if tok.pos_ in [\"DET\", \"PART\"] or tok.dep_ in [\"prep\"]:\n",
    "                    # determiners, prepositions, particle discard with 60-70%\n",
    "                    Y = np.random.uniform(0, 1)\n",
    "                    prob = np.random.uniform(0.6,0.7)\n",
    "                    if Y < prob:\n",
    "                        aphasic_utt += tok.text + \" \"\n",
    "                elif tok.pos_ in [\"ADJ\", \"ADV\"]:\n",
    "                    # adjectives, adverbs discard with 50%\n",
    "                    Z = np.random.uniform(0, 1)\n",
    "                    if Z < 0.5:\n",
    "                        aphasic_utt += tok.text + \" \"\n",
    "                elif tok.pos_ == \"VERB\":\n",
    "                    # verbs lemmatize with 50%\n",
    "                    Z = np.random.uniform(0, 1)\n",
    "                    if Z < 0.5:\n",
    "                        aphasic_utt += tok.lemma_ + \" \"\n",
    "                elif tok.pos == \"INTJ\":\n",
    "                    # close class from PC analysis \n",
    "                    if add:\n",
    "                        aphasic_utt += tok.text + \" \"\n",
    "                    aphasic_utt += tok.text + \" \"\n",
    "                else:\n",
    "                    # all other pos remain\n",
    "                    aphasic_utt += tok.text + \" \"\n",
    "        # print(\"Text: \" + text + \"Aphasic: \" + aphasic_utt)\n",
    "        # exclusion criterias\n",
    "        lower = round(length * (1/3))\n",
    "        higher = round(length * (2/3))\n",
    "        aphasic_utt = postprocess(aphasic_utt)\n",
    "        new_length = len(re.findall(\"[a-zA-Z_]+\", aphasic_utt))\n",
    "        # print(higher, new_length, lower)\n",
    "        # print(higher <= new_length <= lower)\n",
    "        # print()\n",
    "        if lower <= new_length <= higher:\n",
    "            return True, aphasic_utt\n",
    "        else:\n",
    "            return False, aphasic_utt\n",
    "    \n",
    "    else:\n",
    "        return False, aphasic_utt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T15:53:53.838280300Z",
     "start_time": "2024-05-03T15:53:53.806696300Z"
    }
   },
   "id": "a662af15af5493c7"
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000 sentences, broca utts: 151\n",
      "Processed 2000 sentences, broca utts: 331\n",
      "Processed 3000 sentences, broca utts: 494\n",
      "Processed 4000 sentences, broca utts: 650\n",
      "Processed 5000 sentences, broca utts: 795\n",
      "Processed 6000 sentences, broca utts: 944\n",
      "Processed 7000 sentences, broca utts: 1119\n",
      "Processed 8000 sentences, broca utts: 1305\n",
      "Processed 9000 sentences, broca utts: 1458\n",
      "Processed 10000 sentences, broca utts: 1630\n",
      "Processed 11000 sentences, broca utts: 1818\n",
      "Processed 12000 sentences, broca utts: 1992\n",
      "Processed 13000 sentences, broca utts: 2152\n",
      "Processed 14000 sentences, broca utts: 2270\n",
      "Processed 15000 sentences, broca utts: 2438\n",
      "Processed 16000 sentences, broca utts: 2568\n",
      "Processed 17000 sentences, broca utts: 2742\n",
      "Processed 18000 sentences, broca utts: 2953\n",
      "Processed 19000 sentences, broca utts: 3117\n",
      "Processed 20000 sentences, broca utts: 3272\n",
      "Processed 21000 sentences, broca utts: 3440\n",
      "Processed 22000 sentences, broca utts: 3638\n",
      "Processed 23000 sentences, broca utts: 3808\n"
     ]
    }
   ],
   "source": [
    "aphasic_sents = []\n",
    "normal_sents = []\n",
    "new_sents = []\n",
    "\n",
    "import inflect\n",
    "from preprocess import postprocess\n",
    "p = inflect.engine()\n",
    "count = 0\n",
    "num_sents = 9000 # how many aphasic sentences?\n",
    "\n",
    "for sent in test_sents:\n",
    "    # no digits like in aphasiabank\n",
    "    # print(sent)\n",
    "    b = re.findall(\"[0-9]+\", sent)\n",
    "    for i in b:\n",
    "        sent = sent.replace(i, p.number_to_words(i))\n",
    "    \n",
    "    s = preprocess(sent)\n",
    "    \n",
    "    if len(aphasic_sents) >= num_sents:\n",
    "        changed = False\n",
    "        aphasic = \"\"\n",
    "    else:\n",
    "        changed, aphasic = aphasic_speech(s)\n",
    "        count += 1\n",
    "    \n",
    "    if len(aphasic_sents) >= num_sents and len(new_sents) >= num_sents * 2.5911:\n",
    "        break\n",
    "        \n",
    "    if count % 1000 == 0:\n",
    "        print(f\"Processed {count} sentences, broca utts: {len(aphasic_sents)}\")\n",
    "    \n",
    "    # min length is 3?\n",
    "    if changed and aphasic !=\".\" and (postprocess(s) != aphasic): #and 3 <= len(re.findall(\"[a-zA-Z_]+\", aphasic)):\n",
    "        # print(sent)\n",
    "        # print(postprocess(s))\n",
    "        # print(postprocess(aphasic))\n",
    "        # print()\n",
    "        normal_sents.append(postprocess(s))\n",
    "        aphasic_sents.append(aphasic)\n",
    "    if not changed:\n",
    "        new_sents.append(postprocess(s))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T15:55:10.270258700Z",
     "start_time": "2024-05-03T15:53:53.824762400Z"
    }
   },
   "id": "22912266114be251"
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "sentences = aphasic_sents\n",
    "original = normal_sents"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T15:55:10.285882100Z",
     "start_time": "2024-05-03T15:55:10.270258700Z"
    }
   },
   "id": "48d2a87faba3ccf0"
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from preprocess import postprocess\n",
    "broca_sents = []\n",
    "original_sents = []\n",
    "for sent, o in zip(sentences, original):\n",
    "    x = postprocess(sent)\n",
    "    if x != \"\":\n",
    "        broca_sents.append(x)\n",
    "        original_sents.append(o)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T15:55:10.332758900Z",
     "start_time": "2024-05-03T15:55:10.285882100Z"
    }
   },
   "id": "c51b6447aa272856"
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "pd.DataFrame(data={\"modified\": broca_sents, \"original\": original_sents}).to_csv(\"data/new.csv\", sep=\",\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T15:55:10.348385Z",
     "start_time": "2024-05-03T15:55:10.332758900Z"
    }
   },
   "id": "5ad1eafde2b154dd"
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "broca_sents = pd.read_csv(\"data/new.csv\", sep=\",\")[\"modified\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T15:55:10.379657700Z",
     "start_time": "2024-05-03T15:55:10.348385Z"
    }
   },
   "id": "46bc092a7122b513"
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "broca_sents = broca_sents[:num_sents]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T15:55:10.395260600Z",
     "start_time": "2024-05-03T15:55:10.379657700Z"
    }
   },
   "id": "9bd0b1671dcaa5d1"
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "data": {
      "text/plain": "9947"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(len(broca_sents)*2.5911)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T15:55:10.442570300Z",
     "start_time": "2024-05-03T15:55:10.395260600Z"
    }
   },
   "id": "4b54c89b88e9dd27"
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "control_sents = new_sents[:round(len(broca_sents)*2.5911)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T15:55:10.442570300Z",
     "start_time": "2024-05-03T15:55:10.414883800Z"
    }
   },
   "id": "8ec64bced1906d27"
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "broca_data = pd.DataFrame(data={\"preprocessed_text\": broca_sents, \"label\": [1]*len(broca_sents)})\n",
    "control_data = pd.DataFrame(data={\"preprocessed_text\": control_sents, \"label\": [0]*len(control_sents)})\n",
    "data_full_scenario = pd.concat([broca_data, control_data], ignore_index=True)\n",
    "data_full_scenario = data_full_scenario.sample(frac=1).reset_index(drop=True)\n",
    "data_full_scenario[\"preprocessed_text\"] = [re.sub(r'[^\\w\\s]','',x) for x in data_full_scenario[\"preprocessed_text\"]]\n",
    "data_full_scenario.to_csv(\"data/new_merge.csv\", sep=\",\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T15:55:10.473799500Z",
     "start_time": "2024-05-03T15:55:10.426916Z"
    }
   },
   "id": "6b6635d33f70b4a8"
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "data": {
      "text/plain": "                               preprocessed_text  label\n0                     and it is just like rubble      0\n1                                     dress nice      1\n2            i mean you tried him a couple times      0\n3            and that is the right kind of beads      0\n4                                    him flutter      1\n...                                          ...    ...\n13781        he had been in the in the three twe      0\n13782                            i laura in yeah      1\n13783  it is a pretty good sized area right here      0\n13784                       what is what is that      0\n13785                          add more water it      1\n\n[13786 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>preprocessed_text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>and it is just like rubble</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>dress nice</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>i mean you tried him a couple times</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>and that is the right kind of beads</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>him flutter</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>13781</th>\n      <td>he had been in the in the three twe</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>13782</th>\n      <td>i laura in yeah</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13783</th>\n      <td>it is a pretty good sized area right here</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>13784</th>\n      <td>what is what is that</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>13785</th>\n      <td>add more water it</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>13786 rows Ã— 2 columns</p>\n</div>"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_full_scenario"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T15:55:10.489427400Z",
     "start_time": "2024-05-03T15:55:10.473799500Z"
    }
   },
   "id": "7675b36856541d1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
