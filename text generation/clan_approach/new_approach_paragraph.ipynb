{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import random\n",
    "import re\n",
    "from pattern.text.en import singularize, pluralize, conjugate\n",
    "import enchant\n",
    "from preprocess import preprocess\n",
    "from spacy.matcher import Matcher\n",
    "from string import printable\n",
    "from preprocess import postprocess\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "d = enchant.Dict(\"en_US\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T17:19:42.781612400Z",
     "start_time": "2024-06-19T17:19:42.478030400Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "extra = '!\"#$%&\\'()*+-/:;<=>?@[\\\\]^_`{|}~'\n",
    "\n",
    "for x in extra:\n",
    "    printable = printable.replace(x,'',)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T17:19:42.797313200Z",
     "start_time": "2024-06-19T17:19:42.787144800Z"
    }
   },
   "id": "bd77d6afd505a31c"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ,. \\t\\n\\r\\x0b\\x0c'"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "printable"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T17:19:42.844456500Z",
     "start_time": "2024-06-19T17:19:42.797313200Z"
    }
   },
   "id": "cf038f52952eb41c"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "dets = {'Art': ['a', 'an', 'the'],\n",
    "           'Dem': ['this', 'that', 'these', 'those'],\n",
    "           'Poss': ['my', 'your', 'his', 'her', 'its', 'our', 'their']}\n",
    "\n",
    "def det_sub(x):\n",
    "    for _, det in dets.items():\n",
    "        if x.lower() in det:\n",
    "            y = [j for j in det if x!=j]\n",
    "            return random.choice(y)\n",
    "    return \"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T17:19:42.844456500Z",
     "start_time": "2024-06-19T17:19:42.813195300Z"
    }
   },
   "id": "93aae4fad150933"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# broca_save = \"../datafiles/generated output/bnc_broca.csv\"\n",
    "# generated_save = \"../datafiles/generated output/bnc_all.csv\"\n",
    "# dataset_filename = \"../datafiles/spoken corpus/bnc/preprocessed_bnc.csv\"\n",
    "\n",
    "broca_save = \"../datafiles/generated output/boston_broca.csv\"\n",
    "generated_save = \"../datafiles/generated output/boston_all.csv\"\n",
    "dataset_filename = \"../datafiles/spoken corpus/boston/preprocessed_boston.csv\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T17:19:42.844456500Z",
     "start_time": "2024-06-19T17:19:42.828833500Z"
    }
   },
   "id": "bc6afeadba535d0e"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "ds = pd.read_csv(dataset_filename, encoding='utf8', index_col=False).drop(['Unnamed: 0'], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T17:19:42.876876500Z",
     "start_time": "2024-06-19T17:19:42.844456500Z"
    }
   },
   "id": "ba74d2f1ffdf9890"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "     source_file speaker                                  preprocessed_text  \\\n0     SBC008.cha   *REBE  the way that your testimony is coming in i do ...   \n1     SBC008.cha   *REBE  to expose himself to a person for sexual arous...   \n2     SBC008.cha   *REBE  in at least one of the cases. then we are allo...   \n3     SBC008.cha   *REBE  things like that that is why we are able to ha...   \n4     SBC008.cha   *REBE  well i have made two one a year from almost a ...   \n...          ...     ...                                                ...   \n7713  SBC060.cha   *ALAN  on areas score score higher on the test than t...   \n7714  SBC060.cha   *ALAN  i am told i have not seen any of them but the ...   \n7715  SBC060.cha    *JON  but they are well educated people down there. ...   \n7716  SBC060.cha   *ALAN  i am s. i know that this fellow goldstone who ...   \n7717  SBC060.cha   *ALAN  pretty good indication. pretty good indication...   \n\n      label  \n0         0  \n1         0  \n2         0  \n3         0  \n4         0  \n...     ...  \n7713      0  \n7714      0  \n7715      0  \n7716      0  \n7717      0  \n\n[7718 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source_file</th>\n      <th>speaker</th>\n      <th>preprocessed_text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>SBC008.cha</td>\n      <td>*REBE</td>\n      <td>the way that your testimony is coming in i do ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>SBC008.cha</td>\n      <td>*REBE</td>\n      <td>to expose himself to a person for sexual arous...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>SBC008.cha</td>\n      <td>*REBE</td>\n      <td>in at least one of the cases. then we are allo...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>SBC008.cha</td>\n      <td>*REBE</td>\n      <td>things like that that is why we are able to ha...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>SBC008.cha</td>\n      <td>*REBE</td>\n      <td>well i have made two one a year from almost a ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7713</th>\n      <td>SBC060.cha</td>\n      <td>*ALAN</td>\n      <td>on areas score score higher on the test than t...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7714</th>\n      <td>SBC060.cha</td>\n      <td>*ALAN</td>\n      <td>i am told i have not seen any of them but the ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7715</th>\n      <td>SBC060.cha</td>\n      <td>*JON</td>\n      <td>but they are well educated people down there. ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7716</th>\n      <td>SBC060.cha</td>\n      <td>*ALAN</td>\n      <td>i am s. i know that this fellow goldstone who ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7717</th>\n      <td>SBC060.cha</td>\n      <td>*ALAN</td>\n      <td>pretty good indication. pretty good indication...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>7718 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T17:19:42.891384Z",
     "start_time": "2024-06-19T17:19:42.876876500Z"
    }
   },
   "id": "790e882230dda953"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "m0sa_lim = 0.7      # m:0s:a             (30%) done\n",
    "ms_lim = 0.7        # m:+s(:a)           (30%) done\n",
    "sgc_lim = 0.6       # s:r:gc             (40%) done \n",
    "rep_lim = 0.98       # repetition         (10%) done"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T17:19:42.907019200Z",
     "start_time": "2024-06-19T17:19:42.891384Z"
    }
   },
   "id": "1e38b4281b3f84b4"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def aphasic_speech(text):\n",
    "    # print('Sentence: ', text)\n",
    "\n",
    "    doc = nlp(text)\n",
    "    vp_pattern = [[{'POS': 'VERB', 'OP': '?'},\n",
    "                   {'POS': 'ADV', 'OP': '*'},\n",
    "                   {'POS': 'AUX', 'OP': '*'},\n",
    "                   {'POS': 'VERB', 'OP': '+'}]]\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add(\"Verb phrase\", vp_pattern)\n",
    "    n = 15\n",
    "    aphasic_utt = \"\"\n",
    "    length = len(re.findall(\"[a-zA-Z_]+\", text))\n",
    "    nouns = []\n",
    "    verbs = []\n",
    "    determiners = []\n",
    "    prepositions = []\n",
    "    adjectives = []\n",
    "    adverbs = []\n",
    "    interjections = []\n",
    "    open_close = np.random.gamma(shape=4.99415, scale=1/3.558095)\n",
    "    add = False\n",
    "    \n",
    "    # count no. of respective POS\n",
    "    for tok in doc:\n",
    "        if tok.pos_ == \"NOUN\":\n",
    "            nouns.append(tok.text)\n",
    "        elif tok.pos_ == \"VERB\" or tok.dep_ == \"cop\" or tok.tag_ in [\"VBD\", \"VBN\"]:\n",
    "            verbs.append(tok.text)\n",
    "        # det:art and det:dem only\n",
    "        elif tok.dep_ == \"det\" and (\"Dem\" in tok.morph.get('PronType') or \"Art\" in tok.morph.get('PronType')):\n",
    "            determiners.append(tok.text)\n",
    "        elif tok.dep_ == \"prep\":\n",
    "            prepositions.append(tok.text)\n",
    "        elif tok.pos_ == \"ADJ\":\n",
    "            adjectives.append(tok.text)\n",
    "        elif tok.pos_ == \"ADV\":\n",
    "            adverbs.append(tok.text)\n",
    "        elif tok.pos_ == \"INTJ\":\n",
    "            interjections.append(tok.pos_)\n",
    "            \n",
    "    open_class_num = len(nouns) + len(verbs) + len(adjectives) + len(adverbs)\n",
    "    closed_class_num = length - open_class_num - len(interjections)\n",
    "    \n",
    "    # acc to frank, not removing only adding\n",
    "    if closed_class_num != 0:\n",
    "        if open_close > open_class_num/closed_class_num:\n",
    "            add = True\n",
    "    \n",
    "    # discard sentences of 15 and above length\n",
    "    # and with symbols\n",
    "    if length <= n and not set(text).difference(printable):       \n",
    "        # get NPs\n",
    "        noun_phrases = set()\n",
    "\n",
    "        for nc in doc.noun_chunks:\n",
    "            for nop in [nc, doc[nc.root.left_edge.i:nc.root.right_edge.i + 1]]:\n",
    "                noun_phrases.add(nop.text.strip())\n",
    "                # get VPs\n",
    "        verb_phrases = matcher(doc)\n",
    "        verb_phrases = [doc[start:end] for _, start, end in verb_phrases]\n",
    "        \n",
    "\n",
    "        try:\n",
    "            ratio = len(noun_phrases) / len(verb_phrases)\n",
    "        except:\n",
    "            ratio = 1\n",
    "            # print('No verb phrases')\n",
    "            # return False, aphasic_utt\n",
    "\n",
    "        X = np.random.uniform(0, 1)\n",
    "        \n",
    "        if ratio > 2 and X < 0.8:\n",
    "            # skip sentence if np/vp too big with prob of 80%\n",
    "            # print(\"Skipped sentence\")\n",
    "            return False, aphasic_utt\n",
    "        else:\n",
    "            # dont skip sentence\n",
    "            \n",
    "            # Handle nouns\n",
    "            for tok in doc:\n",
    "                # print(tok.pos_, tok.text)\n",
    "                if tok.pos_ == \"NOUN\":\n",
    "                    # m:0s:a and m:+s(:a) errors\n",
    "                    m0sa_prob = random.uniform(0,1)     # m:0s:a\n",
    "                    ms_prob = random.uniform(0,1)       # m:+s(:a)\n",
    "                    if m0sa_prob >= m0sa_lim or ms_prob >= ms_lim:\n",
    "                        if \"Plur\" in tok.morph.get(\"Number\"):\n",
    "                            aphasic_utt += singularize(tok.text) + ' '\n",
    "                        elif \"Sing\" in tok.morph.get(\"Number\"):\n",
    "                            aphasic_utt += pluralize(tok.text) + ' ' \n",
    "                    # keep noun as is\n",
    "                    else:\n",
    "                        aphasic_utt += tok.text + \" \"\n",
    "                \n",
    "                # Handle pronouns\n",
    "                elif tok.pos_ == \"PRON\":\n",
    "                    sgc_prob = random.uniform(0,1)      # s:r:gc\n",
    "                    rep_prob = random.uniform(0,1)      # repetitions\n",
    "                    # s:r:gc:pro error \n",
    "                    if sgc_prob >= sgc_lim:\n",
    "                        if tok.pos_ == \"DET\" or \"Dem\" in tok.morph.get('PronType') or \"Yes\" in tok.morph.get('Poss'):\n",
    "                            sub = det_sub(tok.text) \n",
    "                            aphasic_utt += sub + \" \"\n",
    "                            # repetition of s:r:gc:pro error\n",
    "                            if rep_prob >= rep_lim:\n",
    "                                aphasic_utt += sub + \" \"\n",
    "                        else:\n",
    "                            # repetition or keep as is\n",
    "                            if rep_prob >= rep_lim:\n",
    "                                aphasic_utt += tok.text + \" \"\n",
    "                            aphasic_utt += tok.text + \" \"\n",
    "                    else:\n",
    "                        # repetition or keep as is\n",
    "                        aphasic_utt += tok.text + \" \"\n",
    "                        if rep_prob >= rep_lim:\n",
    "                            aphasic_utt += tok.text + \" \"\n",
    "                            \n",
    "                # Handle determiners, prepositions, particle, aux       \n",
    "                elif tok.pos_ in [\"DET\", \"PART\", \"AUX\"] or tok.dep_ in [\"prep\"]:\n",
    "                    # keep with 30-40%\n",
    "                    Y = np.random.uniform(0, 1)\n",
    "                    prob = np.random.uniform(0.6, 0.7)\n",
    "                    if Y > prob:\n",
    "                        # keep if not discarding\n",
    "                        aphasic_utt += tok.text + \" \"\n",
    "    \n",
    "                # Handle adjectives, adverbs \n",
    "                elif tok.pos_ in [\"ADJ\", \"ADV\"]:\n",
    "                    # keep with 70%\n",
    "                    # TODO: maybe not drop it at all\n",
    "                    Z = np.random.uniform(0, 1)\n",
    "                    if Z > 0.3:\n",
    "                        # keep if not discarding\n",
    "                        aphasic_utt += tok.text + \" \"\n",
    "                        \n",
    "                # Handle verbs     \n",
    "                elif tok.pos_ == \"VERB\":\n",
    "                    # lemmatize with 50%\n",
    "                    Z = np.random.uniform(0, 1)\n",
    "                    if Z <= 0.5:\n",
    "                        aphasic_utt += tok.lemma_ + \" \"\n",
    "                    else:\n",
    "                        # keep as is\n",
    "                        aphasic_utt += tok.text + \" \"\n",
    "                 \n",
    "                # Handle interjections        \n",
    "                elif tok.pos == \"INTJ\":\n",
    "                    rep_prob = random.uniform(0,1)      # repetitions\n",
    "                    # close class from PC analysis OR repetition error\n",
    "                    if rep_prob >= rep_lim or add:\n",
    "                        aphasic_utt += tok.text + \" \"\n",
    "                    aphasic_utt += tok.text + \" \"\n",
    "                \n",
    "                else:\n",
    "                    # all other POS remain\n",
    "                    aphasic_utt += tok.text + \" \"\n",
    "                \n",
    "        # exclusion criterias\n",
    "        lower = round(length * (1/3))\n",
    "        higher = round(length * (2/3))\n",
    "        aphasic_utt = postprocess(aphasic_utt)\n",
    "        new_length = len(re.findall(\"[a-zA-Z_]+\", aphasic_utt))\n",
    "        \n",
    "        if lower <= new_length <= higher:\n",
    "            return True, aphasic_utt\n",
    "        else:\n",
    "            # print(aphasic_utt, lower, higher, new_length, length)\n",
    "            # print(\"Too much/too little removed\")\n",
    "            return False, aphasic_utt\n",
    "    \n",
    "    else:\n",
    "        # print('removed', length, not set(text).difference(printable))\n",
    "        return False, aphasic_utt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T17:19:42.985163700Z",
     "start_time": "2024-06-19T17:19:42.922642500Z"
    }
   },
   "id": "a662af15af5493c7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Remove repetition in data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7f112c172d086fe"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def tuple_to_str(tuple):\n",
    "    output_str = \"\"\n",
    "    for tup in tuple:\n",
    "        try:\n",
    "            output_str += \" \" + tup[0]\n",
    "            output_str += \" \" + tup[1]\n",
    "        except:\n",
    "            continue\n",
    "    return output_str[1:]\n",
    "\n",
    "\n",
    "def remove_single_repetitions(text):\n",
    "    \"\"\"\n",
    "    Removes duplicated words (stuttering) and duplicated pauses from utterance.\n",
    "    e.g: I I I I I I wanted --> I wanted.\n",
    "    :param line: input text containing dupes.\n",
    "    :return: unduped string containing text.\n",
    "    \"\"\"\n",
    "    utterance = text.split(\" \")\n",
    "\n",
    "    newlist = []\n",
    "    newlist.append(utterance[0])\n",
    "    for i, element in enumerate(utterance):\n",
    "        if i > 0 and utterance[i - 1] != element:\n",
    "            newlist.append(element)\n",
    "\n",
    "    return ' '.join(newlist)\n",
    "\n",
    "\n",
    "def remove_bigram_repetitions(text):\n",
    "    \"\"\"\n",
    "    Removes bigram stuttering from text. I went I went to the to the doctor --> I went to the doctor.\n",
    "    :param text: input text containing a string\n",
    "    :return: string without duplicates.\n",
    "    \"\"\"\n",
    "    bigram = list(nltk.bigrams(text.split()))\n",
    "    grams = []\n",
    "\n",
    "    for i in range(0, len(bigram)):\n",
    "        if i % 2 == 0:\n",
    "            grams.append(bigram[i])\n",
    "\n",
    "    result = []\n",
    "    prev_item = None\n",
    "    for item in grams:\n",
    "        if item != prev_item:\n",
    "            result.append(item)\n",
    "            prev_item = item\n",
    "\n",
    "    if result[-1][-1] != bigram[-1][-1]:\n",
    "        result.append(tuple((bigram[-1][-1]).split(\" \")))\n",
    "\n",
    "    return tuple_to_str(result)\n",
    "\n",
    "def remove_all_repetitions(text):\n",
    "    \"\"\"\n",
    "    Removes bigram repetitions and stuttering from text.\n",
    "    :return: Clean text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        output_text = remove_single_repetitions(text)\n",
    "        output_text2 = remove_bigram_repetitions(output_text)\n",
    "    except:\n",
    "        return text\n",
    "    return output_text2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T17:19:43.000814200Z",
     "start_time": "2024-06-19T17:19:42.938272500Z"
    }
   },
   "id": "21ea34f840000dfe"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000 sentences, broca paras: 32\n",
      "Processed 2000 sentences, broca paras: 62\n",
      "Processed 3000 sentences, broca paras: 98\n",
      "Processed 4000 sentences, broca paras: 130\n",
      "Processed 5000 sentences, broca paras: 153\n",
      "Processed 6000 sentences, broca paras: 177\n",
      "Processed 7000 sentences, broca paras: 209\n"
     ]
    }
   ],
   "source": [
    "#### PARAGRAPH LEVEL ####\n",
    "texts = ds[\"preprocessed_text\"]\n",
    "aphasic_texts = []\n",
    "original_texts = []\n",
    "control_texts = []\n",
    "\n",
    "total_paras = 10000\n",
    "import inflect\n",
    "from preprocess import postprocess\n",
    "count = 0\n",
    "\n",
    "for text in texts:\n",
    "    count += 1\n",
    "    text = re.sub(r'\\<.*?\\>', \" \", text)\n",
    "    text = text.replace('?', '.')\n",
    "    text = text.replace('!', '.')\n",
    "    sentences = sent_tokenize(text)\n",
    "    aphasic_text = \"\"\n",
    "    original_text = \"\"\n",
    "    # print(\"Original: \", text)\n",
    "    if count % 1000 == 0:\n",
    "        print(f\"Processed {count} sentences, broca paras: {len(aphasic_texts)}\")\n",
    "    \n",
    "    if len(aphasic_texts) > total_paras:\n",
    "        break\n",
    "        \n",
    "    for sent in sentences:\n",
    "        p = inflect.engine()   \n",
    "    \n",
    "        # no digits like in aphasiabank\n",
    "        # print(sent)\n",
    "        b = re.findall(\"[0-9]+\", sent)\n",
    "        for i in b:\n",
    "            sent = sent.replace(i, p.number_to_words(i))\n",
    "        \n",
    "        s = preprocess(sent)\n",
    "        \n",
    "        changed, aphasic = aphasic_speech(s)\n",
    "        \n",
    "        # print(count)\n",
    "        if changed:\n",
    "            aphasic = postprocess(aphasic)\n",
    "            aphasic_text += aphasic.rstrip().lstrip() + \" \"\n",
    "            s = postprocess(s)\n",
    "            s = remove_all_repetitions(s)\n",
    "            original_text += s.rstrip().lstrip() + \" \"\n",
    "            \n",
    "        else:\n",
    "            # only get example where all sentences are modified\n",
    "            aphasic_text = ''\n",
    "            control_texts.append(text.rstrip().lstrip())\n",
    "            break\n",
    "                    \n",
    "              \n",
    "    \n",
    "    if len(re.findall(\"[a-zA-Z_]+\", aphasic_text)) != 0:\n",
    "        # print(\"Original: \", original_text)\n",
    "        # print(\"Aphasic: \", aphasic_text)  \n",
    "        aphasic_texts.append(aphasic_text.rstrip().lstrip()) \n",
    "        original_texts.append(original_text.rstrip().lstrip())\n",
    "        \n",
    "      "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T17:20:30.013742700Z",
     "start_time": "2024-06-19T17:19:42.953893900Z"
    }
   },
   "id": "2d7e82abfce53196"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "pd.DataFrame(data={\"modified\": aphasic_texts, \"original\": original_texts}).to_csv(broca_save, sep=\",\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T17:20:30.029662300Z",
     "start_time": "2024-06-19T17:20:30.013742700Z"
    }
   },
   "id": "5ad1eafde2b154dd"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "control_sents = control_texts[:round(len(aphasic_texts)*2.5911)]\n",
    "\n",
    "broca_data = pd.DataFrame(data={\"preprocessed_text\": aphasic_texts, \"label\": [1]*len(aphasic_texts)})\n",
    "control_data = pd.DataFrame(data={\"preprocessed_text\": control_sents, \"label\": [0]*len(control_sents)})\n",
    "data_full_scenario = pd.concat([broca_data, control_data], ignore_index=True)\n",
    "# data_full_scenario = data_full_scenario.sample(frac=1).reset_index(drop=True)\n",
    "#data_full_scenario[\"preprocessed_text\"] = [re.sub(r'[^\\w\\s]','',x) for x in data_full_scenario[\"preprocessed_text\"]]\n",
    "data_full_scenario.to_csv(generated_save, sep=\",\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T17:20:30.045505400Z",
     "start_time": "2024-06-19T17:20:30.036774Z"
    }
   },
   "id": "4bf3ea3035df92f1"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "                                     preprocessed_text  label\n0              even for one. for one. i talk this one.      1\n1    less or exes. or equal. which xx uh less or to...      1\n2    i know this one so. you know how to do ones. w...      1\n3    so get math over. i got it over out of high sc...      1\n4                         to side. to sides. to sides.      1\n..                                                 ...    ...\n799  how mad were you. oops that will not work. oop...      0\n800  what is everybody waiting for. do not do it. y...      0\n801  well we are. i will just take my gifts up to m...      0\n802  there is more to tonight. well i have f a fun ...      0\n803  i thought i would throw kendie off the s off t...      0\n\n[804 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>preprocessed_text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>even for one. for one. i talk this one.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>less or exes. or equal. which xx uh less or to...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>i know this one so. you know how to do ones. w...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>so get math over. i got it over out of high sc...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>to side. to sides. to sides.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>799</th>\n      <td>how mad were you. oops that will not work. oop...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>800</th>\n      <td>what is everybody waiting for. do not do it. y...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>801</th>\n      <td>well we are. i will just take my gifts up to m...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>802</th>\n      <td>there is more to tonight. well i have f a fun ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>803</th>\n      <td>i thought i would throw kendie off the s off t...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>804 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_full_scenario"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T17:20:30.061323200Z",
     "start_time": "2024-06-19T17:20:30.045505400Z"
    }
   },
   "id": "be8a487fdb957791"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "                                     preprocessed_text  label\n0              even for one. for one. i talk this one.      1\n1    less or exes. or equal. which xx uh less or to...      1\n2    i know this one so. you know how to do ones. w...      1\n3    so get math over. i got it over out of high sc...      1\n4                         to side. to sides. to sides.      1\n..                                                 ...    ...\n219  i did person do like big piece. oh it good. th...      1\n220         where our toffee. the basements. i get up.      1\n221      she dead after we marry. a year. how old she.      1\n222  but he real. yeah he. well you know he go to d...      1\n223  you know all artist put their. well they must ...      1\n\n[224 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>preprocessed_text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>even for one. for one. i talk this one.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>less or exes. or equal. which xx uh less or to...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>i know this one so. you know how to do ones. w...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>so get math over. i got it over out of high sc...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>to side. to sides. to sides.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>219</th>\n      <td>i did person do like big piece. oh it good. th...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>220</th>\n      <td>where our toffee. the basements. i get up.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>221</th>\n      <td>she dead after we marry. a year. how old she.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>222</th>\n      <td>but he real. yeah he. well you know he go to d...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>223</th>\n      <td>you know all artist put their. well they must ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>224 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broca_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T17:20:41.687953900Z",
     "start_time": "2024-06-19T17:20:41.671263700Z"
    }
   },
   "id": "c60d3509f750e3bb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# merge"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "beb7ca9eafb493ed"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "# df = pd.concat([pd.read_csv(\"../datafiles/generated output/boston_all.csv\"), pd.read_csv(\"../datafiles/generated output/bnc_all.csv\")])\n",
    "df = pd.concat([pd.read_csv(\"../datafiles/generated output/bnc_broca.csv\"), pd.read_csv(\"../datafiles/generated output/boston_broca.csv\")])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T19:00:53.425557400Z",
     "start_time": "2024-06-19T19:00:53.394177700Z"
    }
   },
   "id": "436686b0e4b81d18"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "df.to_csv(\"../datafiles/generated output/merge_broca.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T19:00:55.702545800Z",
     "start_time": "2024-06-19T19:00:55.673564800Z"
    }
   },
   "id": "71865f4bc4af4779"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "35"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = []\n",
    "for i in range(100):\n",
    "    Y = np.random.uniform(0, 1)\n",
    "    prob = np.random.uniform(0.6, 0.7)\n",
    "    if Y > prob:\n",
    "        # keep if not discarding\n",
    "        ans.append(1)\n",
    "len(ans)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T17:20:30.092602900Z",
     "start_time": "2024-06-19T17:20:30.076960100Z"
    }
   },
   "id": "6611ec730199037b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
