{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "from pattern.text.en import singularize, pluralize\n",
    "import enchant\n",
    "from preprocess import preprocess\n",
    "from spacy.matcher import Matcher\n",
    "from string import printable\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "d = enchant.Dict(\"en_US\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T10:29:08.189624Z",
     "start_time": "2024-07-17T10:29:07.746557100Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "broca_save = \"../datafiles/generated output/generated_broca1.csv\"\n",
    "generated_save = \"../datafiles/generated output/generated_all1.csv\"\n",
    "dataset_filename1 = \"../datafiles/spoken corpus/preprocessed_bnc1.csv\"\n",
    "dataset_filename2 = \"../datafiles/spoken corpus/preprocessed_boston1.csv\"\n",
    "\n",
    "# broca_save = \"../datafiles/generated output/aphasiabank_broca.csv\"\n",
    "# generated_save = \"../datafiles/generated output/aphasiabank_all.csv\"\n",
    "# dataset_filename = \"../datafiles/clan_data/preprocessed_control.csv\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T10:29:08.210733700Z",
     "start_time": "2024-07-17T10:29:08.195466400Z"
    }
   },
   "id": "64f15e9cd1670e2e"
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "extra = '!\"#$%&\\'()*+-/:;<=>?@[\\\\]^_`{|}~'\n",
    "\n",
    "for x in extra:\n",
    "    printable = printable.replace(x,'',)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T10:29:08.259182600Z",
     "start_time": "2024-07-17T10:29:08.210733700Z"
    }
   },
   "id": "bd77d6afd505a31c"
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "data": {
      "text/plain": "'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ,. \\t\\n\\r\\x0b\\x0c'"
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "printable"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T10:29:08.259182600Z",
     "start_time": "2024-07-17T10:29:08.224333900Z"
    }
   },
   "id": "cf038f52952eb41c"
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "dets = {'Art': ['a', 'an', 'the'],\n",
    "           'Dem': ['this', 'that', 'these', 'those'],\n",
    "           'Poss': ['my', 'your', 'his', 'her', 'its', 'our', 'their']}\n",
    "\n",
    "def det_sub(x):\n",
    "    for _, det in dets.items():\n",
    "        if x.lower() in det:\n",
    "            y = [j for j in det if x!=j]\n",
    "            return random.choice(y)\n",
    "    return \"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T10:29:08.259182600Z",
     "start_time": "2024-07-17T10:29:08.245177700Z"
    }
   },
   "id": "93aae4fad150933"
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [
    "ds1 = pd.read_csv(dataset_filename1, encoding='utf8', index_col=False).drop(['Unnamed: 0'], axis=1)\n",
    "ds2 = pd.read_csv(dataset_filename2, encoding='utf8', index_col=False).drop(['Unnamed: 0'], axis=1)\n",
    "ds2.drop(axis=1, inplace=True, columns=['source_file','speaker'])\n",
    "ds = pd.concat([ds1, ds2])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T10:29:09.693534400Z",
     "start_time": "2024-07-17T10:29:08.256247700Z"
    }
   },
   "id": "10a7a289af5de590"
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "ds = pd.concat([ds1, ds2])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T10:29:09.773676600Z",
     "start_time": "2024-07-17T10:29:09.693534400Z"
    }
   },
   "id": "c5d15c6c81fb1fbc"
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "data": {
      "text/plain": "                                       preprocessed_text  lens\n0                                     it's a games word?   NaN\n1                            like a computer games word?   NaN\n2                                        oh that's nice.   NaN\n3      I it's something I have really heard z-buffer ...   NaN\n4                                   the old z-buffering.   NaN\n...                                                  ...   ...\n23202                          you had to look close xx.   6.0\n23203                            pretty good indication.   3.0\n23204                            pretty good indication.   3.0\n23205                                     well you know.   3.0\n23206   what astounds me is and this goes back into n...  32.0\n\n[988466 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>preprocessed_text</th>\n      <th>lens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>it's a games word?</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>like a computer games word?</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>oh that's nice.</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>I it's something I have really heard z-buffer ...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>the old z-buffering.</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>23202</th>\n      <td>you had to look close xx.</td>\n      <td>6.0</td>\n    </tr>\n    <tr>\n      <th>23203</th>\n      <td>pretty good indication.</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>23204</th>\n      <td>pretty good indication.</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>23205</th>\n      <td>well you know.</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>23206</th>\n      <td>what astounds me is and this goes back into n...</td>\n      <td>32.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>988466 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T10:29:09.785455800Z",
     "start_time": "2024-07-17T10:29:09.773676600Z"
    }
   },
   "id": "31d820f949f3d278"
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "texts = ds[\"preprocessed_text\"]\n",
    "sents = []\n",
    "for text in texts:\n",
    "    text = re.sub(r'\\<.*?\\>', \" \", text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    for sent in sentences:\n",
    "        if isinstance(sent, str):\n",
    "            sent = re.sub(r'\\<.*?\\>', \" \", sent)\n",
    "            if not sent.rstrip().isdigit() and len(sent.rstrip()) >= 1:\n",
    "                sents.append(sent)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T10:29:22.453290200Z",
     "start_time": "2024-07-17T10:29:09.864757300Z"
    }
   },
   "id": "26d6b39a368e9ca0"
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "data": {
      "text/plain": "988466"
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sents)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T10:29:22.487013600Z",
     "start_time": "2024-07-17T10:29:22.445612700Z"
    }
   },
   "id": "dcd1727f7d680616"
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "total_num_sents = len(sents) # 1000\n",
    "test_sents = sents[:total_num_sents]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T10:29:22.550894900Z",
     "start_time": "2024-07-17T10:29:22.459134900Z"
    }
   },
   "id": "d012d17dee191198"
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [],
   "source": [
    "m0sa_lim = 0.7      # m:0s:a             (30%) done\n",
    "ms_lim = 0.7        # m:+s(:a)           (30%) done\n",
    "sgc_lim = 0.6       # s:r:gc             (40%) done \n",
    "rep_lim = 0.9       # repetition         (10%) done"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T10:29:22.562986600Z",
     "start_time": "2024-07-17T10:29:22.490401700Z"
    }
   },
   "id": "1e38b4281b3f84b4"
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "def aphasic_speech(text):\n",
    "    # print('Sentence: ', text)\n",
    "\n",
    "    doc = nlp(text)\n",
    "    vp_pattern = [[{'POS': 'VERB', 'OP': '?'},\n",
    "                   {'POS': 'ADV', 'OP': '*'},\n",
    "                   {'POS': 'AUX', 'OP': '*'},\n",
    "                   {'POS': 'VERB', 'OP': '+'}]]\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add(\"Verb phrase\", vp_pattern)\n",
    "    n = 15\n",
    "    aphasic_utt = \"\"\n",
    "    length = len(re.findall(\"[a-zA-Z_]+\", text))\n",
    "    nouns = []\n",
    "    verbs = []\n",
    "    determiners = []\n",
    "    prepositions = []\n",
    "    adjectives = []\n",
    "    adverbs = []\n",
    "    interjections = []\n",
    "    open_close = np.random.gamma(shape=4.99415, scale=1/3.558095)\n",
    "    add = False\n",
    "    \n",
    "    # count no. of respective POS\n",
    "    for tok in doc:\n",
    "        if tok.pos_ == \"NOUN\":\n",
    "            nouns.append(tok.text)\n",
    "        elif tok.pos_ == \"VERB\" or tok.dep_ == \"cop\" or tok.tag_ in [\"VBD\", \"VBN\"]:\n",
    "            verbs.append(tok.text)\n",
    "        # det:art and det:dem only\n",
    "        elif tok.dep_ == \"det\" and (\"Dem\" in tok.morph.get('PronType') or \"Art\" in tok.morph.get('PronType')):\n",
    "            determiners.append(tok.text)\n",
    "        elif tok.dep_ == \"prep\":\n",
    "            prepositions.append(tok.text)\n",
    "        elif tok.pos_ == \"ADJ\":\n",
    "            adjectives.append(tok.text)\n",
    "        elif tok.pos_ == \"ADV\":\n",
    "            adverbs.append(tok.text)\n",
    "        elif tok.pos_ == \"INTJ\":\n",
    "            interjections.append(tok.pos_)\n",
    "            \n",
    "    open_class_num = len(nouns) + len(verbs) + len(adjectives) + len(adverbs)\n",
    "    closed_class_num = length - open_class_num - len(interjections)\n",
    "    \n",
    "    # acc to frank, not removing only adding\n",
    "    if closed_class_num != 0:\n",
    "        if open_close > open_class_num/closed_class_num:\n",
    "            add = True\n",
    "    \n",
    "    # discard sentences of 15 and above length\n",
    "    # and with symbols\n",
    "    if length <= n and not set(text).difference(printable):       \n",
    "        # get NPs\n",
    "        noun_phrases = set()\n",
    "\n",
    "        for nc in doc.noun_chunks:\n",
    "            for nop in [nc, doc[nc.root.left_edge.i:nc.root.right_edge.i + 1]]:\n",
    "                noun_phrases.add(nop.text.strip())\n",
    "                # get VPs\n",
    "        verb_phrases = matcher(doc)\n",
    "        verb_phrases = [doc[start:end] for _, start, end in verb_phrases]\n",
    "        \n",
    "\n",
    "        try:\n",
    "            ratio = len(noun_phrases) / len(verb_phrases)\n",
    "        except:\n",
    "            ratio = 1\n",
    "            # print('No verb phrases')\n",
    "            # return False, aphasic_utt\n",
    "\n",
    "        X = np.random.uniform(0, 1)\n",
    "        \n",
    "        if ratio > 2 and X < 0.8:\n",
    "            # skip sentence if np/vp too big with prob of 80%\n",
    "            # print(\"Skipped sentence\")\n",
    "            return False, aphasic_utt\n",
    "        else:\n",
    "            # dont skip sentence\n",
    "            \n",
    "            # Handle nouns\n",
    "            for tok in doc:\n",
    "                # print(tok.pos_, tok.text)\n",
    "                if tok.pos_ == \"NOUN\":\n",
    "                    # m:0s:a and m:+s(:a) errors\n",
    "                    m0sa_prob = random.uniform(0,1)     # m:0s:a\n",
    "                    ms_prob = random.uniform(0,1)       # m:+s(:a)\n",
    "                    if m0sa_prob >= m0sa_lim or ms_prob >= ms_lim:\n",
    "                        if \"Plur\" in tok.morph.get(\"Number\"):\n",
    "                            aphasic_utt += singularize(tok.text) + ' '\n",
    "                        elif \"Sing\" in tok.morph.get(\"Number\"):\n",
    "                            aphasic_utt += pluralize(tok.text) + ' ' \n",
    "                    # keep noun as is\n",
    "                    else:\n",
    "                        aphasic_utt += tok.text + \" \"\n",
    "                \n",
    "                # Handle pronouns\n",
    "                elif tok.pos_ == \"PRON\":\n",
    "                    sgc_prob = random.uniform(0,1)      # s:r:gc\n",
    "                    rep_prob = random.uniform(0,1)      # repetitions\n",
    "                    # s:r:gc:pro error \n",
    "                    if sgc_prob >= sgc_lim:\n",
    "                        if tok.pos_ == \"DET\" or \"Dem\" in tok.morph.get('PronType') or \"Yes\" in tok.morph.get('Poss'):\n",
    "                            sub = det_sub(tok.text) \n",
    "                            aphasic_utt += sub + \" \"\n",
    "                            # repetition of s:r:gc:pro error\n",
    "                            if rep_prob >= rep_lim:\n",
    "                                aphasic_utt += sub + \" \"\n",
    "                        else:\n",
    "                            # repetition or keep as is\n",
    "                            if rep_prob >= rep_lim:\n",
    "                                aphasic_utt += tok.text + \" \"\n",
    "                            aphasic_utt += tok.text + \" \"\n",
    "                    else:\n",
    "                        # repetition or keep as is\n",
    "                        aphasic_utt += tok.text + \" \"\n",
    "                        if rep_prob >= rep_lim:\n",
    "                            aphasic_utt += tok.text + \" \"\n",
    "                            \n",
    "                # Handle determiners, prepositions, particle, aux       \n",
    "                elif tok.pos_ in [\"DET\", \"PART\", \"AUX\"] or tok.dep_ in [\"prep\"]:\n",
    "                    # keep with 30-40%\n",
    "                    Y = np.random.uniform(0, 1)\n",
    "                    prob = np.random.uniform(0.6, 0.7)\n",
    "                    if Y > prob:\n",
    "                        # keep if not discarding\n",
    "                        aphasic_utt += tok.text + \" \"\n",
    "    \n",
    "                # Handle adjectives, adverbs \n",
    "                elif tok.pos_ in [\"ADJ\", \"ADV\"]:\n",
    "                    # keep with 70%\n",
    "                    # TODO: maybe not drop it at all\n",
    "                    Z = np.random.uniform(0, 1)\n",
    "                    if Z > 0.3:\n",
    "                        # keep if not discarding\n",
    "                        aphasic_utt += tok.text + \" \"\n",
    "                        \n",
    "                # Handle verbs     \n",
    "                elif tok.pos_ == \"VERB\":\n",
    "                    # lemmatize with 50%\n",
    "                    Z = np.random.uniform(0, 1)\n",
    "                    if Z <= 0.5:\n",
    "                        aphasic_utt += tok.lemma_ + \" \"\n",
    "                    else:\n",
    "                        # keep as is\n",
    "                        aphasic_utt += tok.text + \" \"\n",
    "                 \n",
    "                # Handle interjections        \n",
    "                elif tok.pos == \"INTJ\":\n",
    "                    rep_prob = random.uniform(0,1)      # repetitions\n",
    "                    # close class from PC analysis OR repetition error\n",
    "                    if rep_prob >= rep_lim or add:\n",
    "                        aphasic_utt += tok.text + \" \"\n",
    "                    aphasic_utt += tok.text + \" \"\n",
    "                \n",
    "                else:\n",
    "                    # all other POS remain\n",
    "                    aphasic_utt += tok.text + \" \"\n",
    "                \n",
    "        # exclusion criterias\n",
    "        lower = round(length * (1/3))\n",
    "        higher = round(length * (2/3))\n",
    "        aphasic_utt = postprocess(aphasic_utt)\n",
    "        new_length = len(re.findall(\"[a-zA-Z_]+\", aphasic_utt))\n",
    "        \n",
    "        if lower <= new_length <= higher:\n",
    "            return True, aphasic_utt\n",
    "        else:\n",
    "            # print(aphasic_utt, lower, higher, new_length, length)\n",
    "            # print(\"Too much/too little removed\")\n",
    "            return False, aphasic_utt\n",
    "    \n",
    "    else:\n",
    "        # print('removed', length, not set(text).difference(printable))\n",
    "        return False, aphasic_utt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T10:29:22.564275900Z",
     "start_time": "2024-07-17T10:29:22.544642800Z"
    }
   },
   "id": "a662af15af5493c7"
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def tuple_to_str(tuple):\n",
    "    output_str = \"\"\n",
    "    for tup in tuple:\n",
    "        try:\n",
    "            output_str += \" \" + tup[0]\n",
    "            output_str += \" \" + tup[1]\n",
    "        except:\n",
    "            continue\n",
    "    return output_str[1:]\n",
    "\n",
    "\n",
    "def remove_single_repetitions(text):\n",
    "    \"\"\"\n",
    "    Removes duplicated words (stuttering) and duplicated pauses from utterance.\n",
    "    e.g: I I I I I I wanted --> I wanted.\n",
    "    :param line: input text containing dupes.\n",
    "    :return: unduped string containing text.\n",
    "    \"\"\"\n",
    "    utterance = text.split(\" \")\n",
    "\n",
    "    newlist = []\n",
    "    newlist.append(utterance[0])\n",
    "    for i, element in enumerate(utterance):\n",
    "        if i > 0 and utterance[i - 1] != element:\n",
    "            newlist.append(element)\n",
    "\n",
    "    return ' '.join(newlist)\n",
    "\n",
    "\n",
    "def remove_bigram_repetitions(text):\n",
    "    \"\"\"\n",
    "    Removes bigram stuttering from text. I went I went to the to the doctor --> I went to the doctor.\n",
    "    :param text: input text containing a string\n",
    "    :return: string without duplicates.\n",
    "    \"\"\"\n",
    "    bigram = list(nltk.bigrams(text.split()))\n",
    "    grams = []\n",
    "\n",
    "    for i in range(0, len(bigram)):\n",
    "        if i % 2 == 0:\n",
    "            grams.append(bigram[i])\n",
    "\n",
    "    result = []\n",
    "    prev_item = None\n",
    "    for item in grams:\n",
    "        if item != prev_item:\n",
    "            result.append(item)\n",
    "            prev_item = item\n",
    "\n",
    "    if result[-1][-1] != bigram[-1][-1]:\n",
    "        result.append(tuple((bigram[-1][-1]).split(\" \")))\n",
    "\n",
    "    return tuple_to_str(result)\n",
    "\n",
    "def remove_all_repetitions(text):\n",
    "    \"\"\"\n",
    "    Removes bigram repetitions and stuttering from text.\n",
    "    :return: Clean text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        output_text = remove_single_repetitions(text)\n",
    "        output_text2 = remove_bigram_repetitions(output_text)\n",
    "    except:\n",
    "        return text\n",
    "    return output_text2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T10:29:22.579355600Z",
     "start_time": "2024-07-17T10:29:22.560664600Z"
    }
   },
   "id": "935f757a10e16329"
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000 sentences, broca utts: 271\n",
      "Processed 2000 sentences, broca utts: 531\n",
      "Processed 3000 sentences, broca utts: 766\n",
      "Processed 4000 sentences, broca utts: 993\n",
      "Processed 5000 sentences, broca utts: 1240\n",
      "Processed 6000 sentences, broca utts: 1421\n",
      "Processed 7000 sentences, broca utts: 1660\n",
      "Processed 8000 sentences, broca utts: 1807\n",
      "Processed 9000 sentences, broca utts: 2023\n",
      "Processed 10000 sentences, broca utts: 2220\n",
      "Processed 11000 sentences, broca utts: 2426\n",
      "Processed 12000 sentences, broca utts: 2606\n",
      "Processed 13000 sentences, broca utts: 2770\n",
      "Processed 14000 sentences, broca utts: 2967\n",
      "Processed 15000 sentences, broca utts: 3180\n",
      "Processed 16000 sentences, broca utts: 3430\n",
      "Processed 17000 sentences, broca utts: 3630\n",
      "Processed 18000 sentences, broca utts: 3843\n",
      "Processed 19000 sentences, broca utts: 4032\n",
      "Processed 20000 sentences, broca utts: 4241\n",
      "Processed 21000 sentences, broca utts: 4437\n",
      "Processed 22000 sentences, broca utts: 4622\n",
      "Processed 23000 sentences, broca utts: 4812\n",
      "Processed 24000 sentences, broca utts: 5016\n",
      "Processed 25000 sentences, broca utts: 5212\n",
      "Processed 26000 sentences, broca utts: 5398\n",
      "Processed 27000 sentences, broca utts: 5618\n",
      "Processed 28000 sentences, broca utts: 5851\n",
      "Processed 29000 sentences, broca utts: 6074\n",
      "Processed 30000 sentences, broca utts: 6287\n",
      "Processed 31000 sentences, broca utts: 6448\n",
      "Processed 32000 sentences, broca utts: 6709\n",
      "Processed 33000 sentences, broca utts: 6932\n",
      "Processed 34000 sentences, broca utts: 7146\n",
      "Processed 35000 sentences, broca utts: 7333\n",
      "Processed 36000 sentences, broca utts: 7560\n",
      "Processed 37000 sentences, broca utts: 7792\n",
      "Processed 38000 sentences, broca utts: 8003\n",
      "Processed 39000 sentences, broca utts: 8222\n",
      "Processed 40000 sentences, broca utts: 8423\n",
      "Processed 41000 sentences, broca utts: 8607\n",
      "Processed 42000 sentences, broca utts: 8845\n",
      "Processed 43000 sentences, broca utts: 9075\n",
      "Processed 44000 sentences, broca utts: 9328\n",
      "Processed 45000 sentences, broca utts: 9617\n",
      "Processed 46000 sentences, broca utts: 9909\n",
      "Processed 47000 sentences, broca utts: 10087\n",
      "Processed 48000 sentences, broca utts: 10348\n",
      "Processed 49000 sentences, broca utts: 10612\n",
      "Processed 50000 sentences, broca utts: 10898\n",
      "Processed 51000 sentences, broca utts: 11142\n",
      "Processed 52000 sentences, broca utts: 11327\n",
      "Processed 53000 sentences, broca utts: 11566\n",
      "Processed 54000 sentences, broca utts: 11778\n",
      "Processed 55000 sentences, broca utts: 12008\n",
      "Processed 56000 sentences, broca utts: 12215\n",
      "Processed 57000 sentences, broca utts: 12387\n",
      "Processed 58000 sentences, broca utts: 12494\n",
      "Processed 59000 sentences, broca utts: 12710\n",
      "Processed 60000 sentences, broca utts: 12896\n",
      "Processed 61000 sentences, broca utts: 13056\n",
      "Processed 62000 sentences, broca utts: 13209\n",
      "Processed 63000 sentences, broca utts: 13414\n",
      "Processed 64000 sentences, broca utts: 13650\n",
      "Processed 65000 sentences, broca utts: 13855\n",
      "Processed 66000 sentences, broca utts: 14089\n",
      "Processed 67000 sentences, broca utts: 14326\n",
      "Processed 68000 sentences, broca utts: 14541\n",
      "Processed 69000 sentences, broca utts: 14756\n",
      "Processed 70000 sentences, broca utts: 14954\n",
      "Processed 71000 sentences, broca utts: 15144\n",
      "Processed 72000 sentences, broca utts: 15368\n",
      "Processed 73000 sentences, broca utts: 15550\n",
      "Processed 74000 sentences, broca utts: 15778\n",
      "Processed 75000 sentences, broca utts: 16030\n",
      "Processed 76000 sentences, broca utts: 16261\n",
      "Processed 77000 sentences, broca utts: 16461\n",
      "Processed 78000 sentences, broca utts: 16651\n",
      "Processed 79000 sentences, broca utts: 16848\n",
      "Processed 80000 sentences, broca utts: 17077\n",
      "Processed 81000 sentences, broca utts: 17303\n",
      "Processed 82000 sentences, broca utts: 17533\n",
      "Processed 83000 sentences, broca utts: 17763\n",
      "Processed 84000 sentences, broca utts: 17990\n",
      "Processed 85000 sentences, broca utts: 18188\n",
      "Processed 86000 sentences, broca utts: 18371\n",
      "Processed 87000 sentences, broca utts: 18585\n",
      "Processed 88000 sentences, broca utts: 18780\n",
      "Processed 89000 sentences, broca utts: 18940\n",
      "Processed 90000 sentences, broca utts: 19102\n",
      "Processed 91000 sentences, broca utts: 19248\n",
      "Processed 92000 sentences, broca utts: 19437\n",
      "Processed 93000 sentences, broca utts: 19683\n",
      "Processed 94000 sentences, broca utts: 19916\n"
     ]
    }
   ],
   "source": [
    "aphasic_sents = []\n",
    "normal_sents = []\n",
    "new_sents = []\n",
    "\n",
    "import inflect\n",
    "from preprocess import postprocess\n",
    "p = inflect.engine()\n",
    "count = 0\n",
    "num_sents = 20000 # 20000 # how many aphasic sentences?\n",
    "\n",
    "for sent in test_sents:\n",
    "    # no digits like in aphasiabank\n",
    "    # print(sent)\n",
    "    b = re.findall(\"[0-9]+\", sent)\n",
    "    for i in b:\n",
    "        sent = sent.replace(i, p.number_to_words(i))\n",
    "    \n",
    "    s = preprocess(sent)\n",
    "    s = remove_all_repetitions(s)\n",
    "    \n",
    "    if len(aphasic_sents) >= num_sents:\n",
    "        changed = False\n",
    "        aphasic = \"\"\n",
    "    else:\n",
    "        changed, aphasic = aphasic_speech(s)\n",
    "        count += 1\n",
    "    \n",
    "    if len(aphasic_sents) >= num_sents and len(new_sents) >= num_sents * 2.5911:\n",
    "        break\n",
    "        \n",
    "    if count % 1000 == 0:\n",
    "        print(f\"Processed {count} sentences, broca utts: {len(aphasic_sents)}\")\n",
    "    \n",
    "    # min length is 3?\n",
    "    if changed and aphasic !=\".\" and (postprocess(s) != aphasic): #and 3 <= len(re.findall(\"[a-zA-Z_]+\", aphasic)):\n",
    "        # print(sent)\n",
    "        # print(postprocess(s))\n",
    "        # print(postprocess(aphasic))\n",
    "        # print()\n",
    "        normal_sents.append(remove_all_repetitions(postprocess(s)))\n",
    "        aphasic_sents.append(aphasic)\n",
    "    if not changed:\n",
    "        new_sents.append(postprocess(s))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T10:39:51.555736300Z",
     "start_time": "2024-07-17T10:29:22.574677500Z"
    }
   },
   "id": "22912266114be251"
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [],
   "source": [
    "sentences = aphasic_sents\n",
    "original = normal_sents"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T10:39:51.573636100Z",
     "start_time": "2024-07-17T10:39:51.562389600Z"
    }
   },
   "id": "48d2a87faba3ccf0"
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from preprocess import postprocess\n",
    "broca_sents = []\n",
    "original_sents = []\n",
    "for sent, o in zip(sentences, original):\n",
    "    x = postprocess(sent)\n",
    "    if x != \"\":\n",
    "        broca_sents.append(x)\n",
    "        original_sents.append(o)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T10:39:52.000163100Z",
     "start_time": "2024-07-17T10:39:51.573636100Z"
    }
   },
   "id": "c51b6447aa272856"
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [],
   "source": [
    "pd.DataFrame(data={\"modified\": broca_sents, \"original\": original_sents}).to_csv(broca_save, sep=\",\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T10:39:52.063274700Z",
     "start_time": "2024-07-17T10:39:52.000163100Z"
    }
   },
   "id": "5ad1eafde2b154dd"
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "broca_sents = pd.read_csv(broca_save, sep=\",\")[\"modified\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T10:39:52.109636300Z",
     "start_time": "2024-07-17T10:39:52.069782700Z"
    }
   },
   "id": "46bc092a7122b513"
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [
    "broca_sents = broca_sents[:num_sents]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T10:39:52.129882800Z",
     "start_time": "2024-07-17T10:39:52.112618500Z"
    }
   },
   "id": "9bd0b1671dcaa5d1"
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "data": {
      "text/plain": "20000"
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(broca_sents)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T10:39:52.149781500Z",
     "start_time": "2024-07-17T10:39:52.133654100Z"
    }
   },
   "id": "20d7d0eb413f94b3"
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [
    {
     "data": {
      "text/plain": "51822"
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(len(broca_sents)*2.5911)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T10:39:52.160053400Z",
     "start_time": "2024-07-17T10:39:52.149781500Z"
    }
   },
   "id": "4b54c89b88e9dd27"
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [],
   "source": [
    "control_sents = new_sents[:round(len(broca_sents)*2.5911)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T10:39:52.174650200Z",
     "start_time": "2024-07-17T10:39:52.160053400Z"
    }
   },
   "id": "8ec64bced1906d27"
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "broca_data = pd.DataFrame(data={\"preprocessed_text\": broca_sents, \"label\": [1]*len(broca_sents)})\n",
    "control_data = pd.DataFrame(data={\"preprocessed_text\": control_sents, \"label\": [0]*len(control_sents)})\n",
    "data_full_scenario = pd.concat([broca_data, control_data], ignore_index=True)\n",
    "# data_full_scenario = data_full_scenario.sample(frac=1).reset_index(drop=True)\n",
    "# data_full_scenario[\"preprocessed_text\"] = [re.sub(r'[^\\w\\s]','',x) for x in data_full_scenario[\"preprocessed_text\"]]\n",
    "data_full_scenario.to_csv(generated_save, sep=\",\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T10:39:52.337353100Z",
     "start_time": "2024-07-17T10:39:52.179660600Z"
    }
   },
   "id": "6b6635d33f70b4a8"
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [
    {
     "data": {
      "text/plain": "                                       preprocessed_text  label\n0                                      old zs buffering.      1\n1                                     thank you cooking.      1\n2                                        thank you much.      1\n3                                           see what it.      1\n4                                   i half an hour cook.      1\n...                                                  ...    ...\n71817  and hers did not so she said to the trainer oh...      0\n71818  he said alright then and then he was happy the...      0\n71819                                     he carried on.      0\n71820  she said they saw some of the elephants who ar...      0\n71821                        part of the trip i suppose.      0\n\n[71822 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>preprocessed_text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>old zs buffering.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>thank you cooking.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>thank you much.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>see what it.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>i half an hour cook.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>71817</th>\n      <td>and hers did not so she said to the trainer oh...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>71818</th>\n      <td>he said alright then and then he was happy the...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>71819</th>\n      <td>he carried on.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>71820</th>\n      <td>she said they saw some of the elephants who ar...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>71821</th>\n      <td>part of the trip i suppose.</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>71822 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_full_scenario"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T10:39:52.354088100Z",
     "start_time": "2024-07-17T10:39:52.337353100Z"
    }
   },
   "id": "7675b36856541d1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
