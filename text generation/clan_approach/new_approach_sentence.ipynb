{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "from pattern.text.en import singularize, pluralize\n",
    "import enchant\n",
    "from preprocess import preprocess\n",
    "from spacy.matcher import Matcher\n",
    "from string import printable\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "d = enchant.Dict(\"en_US\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T10:01:10.356481200Z",
     "start_time": "2024-06-07T10:01:10.080463200Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "broca_save = \"../datafiles/generated_output/bnc_broca.csv\"\n",
    "generated_save = \"../datafiles/generated_output/bnc_all.csv\"\n",
    "dataset_filename = \"../datafiles/spoken corpus/bnc/preprocessed_bnc.csv\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64f15e9cd1670e2e"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "extra = '!\"#$%&\\'()*+-/:;<=>?@[\\\\]^_`{|}~'\n",
    "\n",
    "for x in extra:\n",
    "    printable = printable.replace(x,'',)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T10:01:10.372682400Z",
     "start_time": "2024-06-07T10:01:10.357780500Z"
    }
   },
   "id": "bd77d6afd505a31c"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ,. \\t\\n\\r\\x0b\\x0c'"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "printable"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T10:01:10.414951800Z",
     "start_time": "2024-06-07T10:01:10.373682900Z"
    }
   },
   "id": "cf038f52952eb41c"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "dets = {'Art': ['a', 'an', 'the'],\n",
    "           'Dem': ['this', 'that', 'these', 'those'],\n",
    "           'Poss': ['my', 'your', 'his', 'her', 'its', 'our', 'their']}\n",
    "\n",
    "def det_sub(x):\n",
    "    for _, det in dets.items():\n",
    "        if x.lower() in det:\n",
    "            y = [j for j in det if x!=j]\n",
    "            return random.choice(y)\n",
    "    return \"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T10:01:10.425953200Z",
     "start_time": "2024-06-07T10:01:10.389312700Z"
    }
   },
   "id": "93aae4fad150933"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "ds = pd.read_csv(dataset_filename, encoding='utf8', index_col=False).drop(['Unnamed: 0'], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T10:01:11.125111400Z",
     "start_time": "2024-06-07T10:01:10.404951500Z"
    }
   },
   "id": "ba74d2f1ffdf9890"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "                                        preprocessed_text\n0                                  thank you for cooking.\n1                                           rice is good.\n2                                    it's a mighty green.\n3            misses the greens. I have been missing them.\n4            that wasn't even me that said that that was.\n...                                                   ...\n819390  yeah. it was weird and like coming back and be...\n819391  Sha Li yeah er but then when I've like. I don'...\n819392  but. anyway. erm he was like Sha Li that's rea...\n819393  I'll just stick with it yeah. this was like ha...\n819394                           yeah we probably should.\n\n[819395 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>preprocessed_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>thank you for cooking.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>rice is good.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>it's a mighty green.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>misses the greens. I have been missing them.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>that wasn't even me that said that that was.</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>819390</th>\n      <td>yeah. it was weird and like coming back and be...</td>\n    </tr>\n    <tr>\n      <th>819391</th>\n      <td>Sha Li yeah er but then when I've like. I don'...</td>\n    </tr>\n    <tr>\n      <th>819392</th>\n      <td>but. anyway. erm he was like Sha Li that's rea...</td>\n    </tr>\n    <tr>\n      <th>819393</th>\n      <td>I'll just stick with it yeah. this was like ha...</td>\n    </tr>\n    <tr>\n      <th>819394</th>\n      <td>yeah we probably should.</td>\n    </tr>\n  </tbody>\n</table>\n<p>819395 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T10:01:11.145115100Z",
     "start_time": "2024-06-07T10:01:11.126112700Z"
    }
   },
   "id": "31d820f949f3d278"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "texts = ds[\"preprocessed_text\"]\n",
    "sents = []\n",
    "for text in texts:\n",
    "    text = re.sub(r'\\<.*?\\>', \" \", text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    for sent in sentences:\n",
    "        if isinstance(sent, str):\n",
    "            sent = re.sub(r'\\<.*?\\>', \" \", sent)\n",
    "            if not sent.rstrip().isdigit() and len(sent.rstrip()) >= 1:\n",
    "                sents.append(sent)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T10:01:24.568158100Z",
     "start_time": "2024-06-07T10:01:11.143112900Z"
    }
   },
   "id": "26d6b39a368e9ca0"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "1219315"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sents)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T10:01:24.584195300Z",
     "start_time": "2024-06-07T10:01:24.560056400Z"
    }
   },
   "id": "dcd1727f7d680616"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "total_num_sents = len(sents) # 1000\n",
    "test_sents = sents[:total_num_sents]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T10:01:24.641761800Z",
     "start_time": "2024-06-07T10:01:24.574719100Z"
    }
   },
   "id": "d012d17dee191198"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "m0sa_lim = 0.7      # m:0s:a             (30%) done\n",
    "ms_lim = 0.7        # m:+s(:a)           (30%) done\n",
    "sgc_lim = 0.6       # s:r:gc             (40%) done \n",
    "rep_lim = 0.9       # repetition         (10%) done"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T10:01:24.653242800Z",
     "start_time": "2024-06-07T10:01:24.635980700Z"
    }
   },
   "id": "1e38b4281b3f84b4"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "def aphasic_speech(text):\n",
    "    doc = nlp(text)\n",
    "    vp_pattern = [[{'POS': 'VERB', 'OP': '?'},\n",
    "                   {'POS': 'ADV', 'OP': '*'},\n",
    "                   {'POS': 'AUX', 'OP': '*'},\n",
    "                   {'POS': 'VERB', 'OP': '+'}]]\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add(\"Verb phrase\", vp_pattern)\n",
    "    n = 15\n",
    "    aphasic_utt = \"\"\n",
    "    length = len(re.findall(\"[a-zA-Z_]+\", text))\n",
    "    \n",
    "    nouns = []\n",
    "    verbs = []\n",
    "    determiners = []\n",
    "    prepositions = []\n",
    "    adjectives = []\n",
    "    adverbs = []\n",
    "    interjections = []\n",
    "    open_close = np.random.gamma(shape=4.99415, scale=1/3.558095)\n",
    "    add = False\n",
    "    \n",
    "    # count no. of respective POS\n",
    "    for tok in doc:\n",
    "        if tok.pos_ == \"NOUN\":\n",
    "            nouns.append(tok.text)\n",
    "        elif tok.pos_ == \"VERB\" or tok.dep_ == \"cop\" or tok.tag_ in [\"VBD\", \"VBN\"]:\n",
    "            verbs.append(tok.text)\n",
    "        # det:art and det:dem only\n",
    "        elif tok.dep_ == \"det\" and (\"Dem\" in tok.morph.get('PronType') or \"Art\" in tok.morph.get('PronType')):\n",
    "            determiners.append(tok.text)\n",
    "        elif tok.dep_ == \"prep\":\n",
    "            prepositions.append(tok.text)\n",
    "        elif tok.pos_ == \"ADJ\":\n",
    "            adjectives.append(tok.text)\n",
    "        elif tok.pos_ == \"ADV\":\n",
    "            adverbs.append(tok.text)\n",
    "        elif tok.pos_ == \"INTJ\":\n",
    "            interjections.append(tok.pos_)\n",
    "            \n",
    "    open_class_num = len(nouns) + len(verbs) + len(adjectives) + len(adverbs)\n",
    "    closed_class_num = length - open_class_num - len(interjections)\n",
    "    \n",
    "    # acc to frank, not removing only adding\n",
    "    if closed_class_num != 0:\n",
    "        if open_close > open_class_num/closed_class_num:\n",
    "            add = True\n",
    "    \n",
    "    # discard sentences of 15 and above length\n",
    "    # and with symbols\n",
    "    if length <= n and not set(text).difference(printable):\n",
    "        m0sa_prob = random.uniform(0,1)     # m:0s:a\n",
    "        ms_prob = random.uniform(0,1)       # m:+s(:a)\n",
    "        sgc_prob = random.uniform(0,1)      # s:r:gc\n",
    "        rep_prob = random.uniform(0,1)      # repetitions\n",
    "        \n",
    "        # get NPs\n",
    "        noun_phrases = set()\n",
    "\n",
    "        for nc in doc.noun_chunks:\n",
    "            for nop in [nc, doc[nc.root.left_edge.i:nc.root.right_edge.i + 1]]:\n",
    "                noun_phrases.add(nop.text.strip())\n",
    "                # get VPs\n",
    "        verb_phrases = matcher(doc)\n",
    "        verb_phrases = [doc[start:end] for _, start, end in verb_phrases]\n",
    "\n",
    "        try:\n",
    "            ratio = len(noun_phrases) / len(verb_phrases)\n",
    "        except:\n",
    "            return False, aphasic_utt\n",
    "\n",
    "        X = np.random.uniform(0, 1)\n",
    "        \n",
    "        if ratio > 2 and X < 0.8:\n",
    "            # skip sentence if np/vp too big with prob of 80%\n",
    "            return False, aphasic_utt\n",
    "        else:\n",
    "            # dont skip sentence\n",
    "            \n",
    "            # Handle nouns\n",
    "            for tok in doc:\n",
    "                if tok.pos_ == \"NOUN\":\n",
    "                    # m:0s:a and m:+s(:a) errors\n",
    "                    if m0sa_prob >= m0sa_lim or ms_prob >= ms_lim:\n",
    "                        if \"Plur\" in tok.morph.get(\"Number\"):\n",
    "                            aphasic_utt += singularize(tok.text) + ' '\n",
    "                        elif \"Sing\" in tok.morph.get(\"Number\"):\n",
    "                            aphasic_utt += pluralize(tok.text) + ' ' \n",
    "                    # keep noun as is\n",
    "                    else:\n",
    "                        aphasic_utt += tok.text + \" \"\n",
    "                \n",
    "                # Handle pronouns\n",
    "                elif tok.pos_ == \"PRON\":\n",
    "                    # s:r:gc:pro error \n",
    "                    if sgc_prob >= sgc_lim:\n",
    "                        if tok.pos_ == \"DET\" or \"Dem\" in tok.morph.get('PronType') or \"Yes\" in tok.morph.get('Poss'):\n",
    "                            sub = det_sub(tok.text) \n",
    "                            aphasic_utt += sub + \" \"\n",
    "                            # repetition of s:r:gc:pro error\n",
    "                            if rep_prob >= rep_lim:\n",
    "                                aphasic_utt += sub + \" \"\n",
    "                        else:\n",
    "                            # repetition or keep as is\n",
    "                            if rep_prob >= rep_lim:\n",
    "                                aphasic_utt += tok.text + \" \"\n",
    "                            aphasic_utt += tok.text + \" \"\n",
    "                    else:\n",
    "                        # repetition or keep as is\n",
    "                        aphasic_utt += tok.text + \" \"\n",
    "                        if rep_prob >= rep_lim:\n",
    "                            aphasic_utt += tok.text + \" \"\n",
    "                            \n",
    "                # Handle determiners, prepositions, particle       \n",
    "                elif tok.pos_ in [\"DET\", \"PART\"] or tok.dep_ in [\"prep\"]:\n",
    "                    # discard with 60-70%\n",
    "                    Y = np.random.uniform(0, 1)\n",
    "                    prob = np.random.uniform(0.6,0.7)\n",
    "                    if Y > prob:\n",
    "                        # keep if not discarding\n",
    "                        aphasic_utt += tok.text + \" \"\n",
    "    \n",
    "                # Handle adjectives, adverbs \n",
    "                elif tok.pos_ in [\"ADJ\", \"ADV\"]:\n",
    "                    # discard with 30%\n",
    "                    # TODO: maybe not drop it at all\n",
    "                    Z = np.random.uniform(0, 1)\n",
    "                    if Z < 0.3:\n",
    "                        # keep if not discarding\n",
    "                        aphasic_utt += tok.text + \" \"\n",
    "                        \n",
    "                # Handle verbs     \n",
    "                elif tok.pos_ == \"VERB\":\n",
    "                    # lemmatize with 50%\n",
    "                    Z = np.random.uniform(0, 1)\n",
    "                    if Z < 0.5:\n",
    "                        aphasic_utt += tok.lemma_ + \" \"\n",
    "                    else:\n",
    "                        # keep as is\n",
    "                        aphasic_utt += tok.text + \" \"\n",
    "                 \n",
    "                # Handle interjections        \n",
    "                elif tok.pos == \"INTJ\":\n",
    "                    # close class from PC analysis OR repetition error\n",
    "                    if rep_prob >= rep_lim or add:\n",
    "                        aphasic_utt += tok.text + \" \"\n",
    "                    aphasic_utt += tok.text + \" \"\n",
    "                \n",
    "                else:\n",
    "                    # all other POS remain\n",
    "                    aphasic_utt += tok.text + \" \"\n",
    "                \n",
    "        # exclusion criterias\n",
    "        lower = round(length * (1/3))\n",
    "        higher = round(length * (2/3))\n",
    "        aphasic_utt = postprocess(aphasic_utt)\n",
    "        new_length = int(len(re.findall(\"[a-zA-Z_]+\", aphasic_utt)))\n",
    "        \n",
    "        if lower <= new_length <= higher:\n",
    "            return True, aphasic_utt\n",
    "        else:\n",
    "            return False, aphasic_utt\n",
    "    \n",
    "    else:\n",
    "        return False, aphasic_utt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T10:01:24.708494800Z",
     "start_time": "2024-06-07T10:01:24.696810700Z"
    }
   },
   "id": "a662af15af5493c7"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def tuple_to_str(tuple):\n",
    "    output_str = \"\"\n",
    "    for tup in tuple:\n",
    "        try:\n",
    "            output_str += \" \" + tup[0]\n",
    "            output_str += \" \" + tup[1]\n",
    "        except:\n",
    "            continue\n",
    "    return output_str[1:]\n",
    "\n",
    "\n",
    "def remove_single_repetitions(text):\n",
    "    \"\"\"\n",
    "    Removes duplicated words (stuttering) and duplicated pauses from utterance.\n",
    "    e.g: I I I I I I wanted --> I wanted.\n",
    "    :param line: input text containing dupes.\n",
    "    :return: unduped string containing text.\n",
    "    \"\"\"\n",
    "    utterance = text.split(\" \")\n",
    "\n",
    "    newlist = []\n",
    "    newlist.append(utterance[0])\n",
    "    for i, element in enumerate(utterance):\n",
    "        if i > 0 and utterance[i - 1] != element:\n",
    "            newlist.append(element)\n",
    "\n",
    "    return ' '.join(newlist)\n",
    "\n",
    "\n",
    "def remove_bigram_repetitions(text):\n",
    "    \"\"\"\n",
    "    Removes bigram stuttering from text. I went I went to the to the doctor --> I went to the doctor.\n",
    "    :param text: input text containing a string\n",
    "    :return: string without duplicates.\n",
    "    \"\"\"\n",
    "    bigram = list(nltk.bigrams(text.split()))\n",
    "    grams = []\n",
    "\n",
    "    for i in range(0, len(bigram)):\n",
    "        if i % 2 == 0:\n",
    "            grams.append(bigram[i])\n",
    "\n",
    "    result = []\n",
    "    prev_item = None\n",
    "    for item in grams:\n",
    "        if item != prev_item:\n",
    "            result.append(item)\n",
    "            prev_item = item\n",
    "\n",
    "    if result[-1][-1] != bigram[-1][-1]:\n",
    "        result.append(tuple((bigram[-1][-1]).split(\" \")))\n",
    "\n",
    "    return tuple_to_str(result)\n",
    "\n",
    "def remove_all_repetitions(text):\n",
    "    \"\"\"\n",
    "    Removes bigram repetitions and stuttering from text.\n",
    "    :return: Clean text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        output_text = remove_single_repetitions(text)\n",
    "        output_text2 = remove_bigram_repetitions(output_text)\n",
    "    except:\n",
    "        return text\n",
    "    return output_text2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T10:01:24.725838Z",
     "start_time": "2024-06-07T10:01:24.716391Z"
    }
   },
   "id": "935f757a10e16329"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000 sentences, broca utts: 151\n",
      "Processed 2000 sentences, broca utts: 303\n",
      "Processed 3000 sentences, broca utts: 463\n",
      "Processed 4000 sentences, broca utts: 621\n",
      "Processed 5000 sentences, broca utts: 791\n",
      "Processed 6000 sentences, broca utts: 953\n",
      "Processed 7000 sentences, broca utts: 1097\n",
      "Processed 8000 sentences, broca utts: 1305\n",
      "Processed 9000 sentences, broca utts: 1432\n",
      "Processed 10000 sentences, broca utts: 1582\n",
      "Processed 11000 sentences, broca utts: 1766\n",
      "Processed 12000 sentences, broca utts: 1912\n",
      "Processed 13000 sentences, broca utts: 2047\n",
      "Processed 14000 sentences, broca utts: 2195\n",
      "Processed 15000 sentences, broca utts: 2314\n",
      "Processed 16000 sentences, broca utts: 2455\n",
      "Processed 17000 sentences, broca utts: 2619\n",
      "Processed 18000 sentences, broca utts: 2797\n",
      "Processed 19000 sentences, broca utts: 2962\n",
      "Processed 20000 sentences, broca utts: 3130\n",
      "Processed 21000 sentences, broca utts: 3288\n",
      "Processed 22000 sentences, broca utts: 3485\n",
      "Processed 23000 sentences, broca utts: 3666\n",
      "Processed 24000 sentences, broca utts: 3806\n",
      "Processed 25000 sentences, broca utts: 3973\n",
      "Processed 26000 sentences, broca utts: 4140\n",
      "Processed 27000 sentences, broca utts: 4294\n",
      "Processed 28000 sentences, broca utts: 4428\n",
      "Processed 29000 sentences, broca utts: 4578\n",
      "Processed 30000 sentences, broca utts: 4741\n",
      "Processed 31000 sentences, broca utts: 4900\n",
      "Processed 32000 sentences, broca utts: 5064\n",
      "Processed 33000 sentences, broca utts: 5238\n",
      "Processed 34000 sentences, broca utts: 5420\n",
      "Processed 35000 sentences, broca utts: 5623\n",
      "Processed 36000 sentences, broca utts: 5799\n",
      "Processed 37000 sentences, broca utts: 5969\n",
      "Processed 38000 sentences, broca utts: 6132\n",
      "Processed 39000 sentences, broca utts: 6289\n",
      "Processed 40000 sentences, broca utts: 6450\n",
      "Processed 41000 sentences, broca utts: 6603\n",
      "Processed 42000 sentences, broca utts: 6764\n",
      "Processed 43000 sentences, broca utts: 6921\n",
      "Processed 44000 sentences, broca utts: 7099\n",
      "Processed 45000 sentences, broca utts: 7239\n",
      "Processed 46000 sentences, broca utts: 7402\n",
      "Processed 47000 sentences, broca utts: 7547\n",
      "Processed 48000 sentences, broca utts: 7691\n",
      "Processed 49000 sentences, broca utts: 7845\n",
      "Processed 50000 sentences, broca utts: 8001\n",
      "Processed 51000 sentences, broca utts: 8145\n",
      "Processed 52000 sentences, broca utts: 8324\n",
      "Processed 53000 sentences, broca utts: 8477\n",
      "Processed 54000 sentences, broca utts: 8654\n",
      "Processed 55000 sentences, broca utts: 8817\n",
      "Processed 56000 sentences, broca utts: 9001\n",
      "Processed 57000 sentences, broca utts: 9168\n",
      "Processed 58000 sentences, broca utts: 9347\n",
      "Processed 59000 sentences, broca utts: 9469\n",
      "Processed 60000 sentences, broca utts: 9619\n",
      "Processed 61000 sentences, broca utts: 9770\n",
      "Processed 62000 sentences, broca utts: 9935\n",
      "Processed 63000 sentences, broca utts: 10111\n",
      "Processed 64000 sentences, broca utts: 10261\n",
      "Processed 65000 sentences, broca utts: 10437\n",
      "Processed 66000 sentences, broca utts: 10609\n",
      "Processed 67000 sentences, broca utts: 10793\n",
      "Processed 68000 sentences, broca utts: 10962\n",
      "Processed 69000 sentences, broca utts: 11119\n",
      "Processed 70000 sentences, broca utts: 11277\n",
      "Processed 71000 sentences, broca utts: 11429\n",
      "Processed 72000 sentences, broca utts: 11539\n",
      "Processed 73000 sentences, broca utts: 11676\n",
      "Processed 74000 sentences, broca utts: 11849\n",
      "Processed 75000 sentences, broca utts: 11988\n",
      "Processed 76000 sentences, broca utts: 12145\n",
      "Processed 77000 sentences, broca utts: 12261\n",
      "Processed 78000 sentences, broca utts: 12425\n",
      "Processed 79000 sentences, broca utts: 12587\n",
      "Processed 80000 sentences, broca utts: 12723\n",
      "Processed 81000 sentences, broca utts: 12870\n",
      "Processed 82000 sentences, broca utts: 13053\n",
      "Processed 83000 sentences, broca utts: 13226\n",
      "Processed 84000 sentences, broca utts: 13380\n",
      "Processed 85000 sentences, broca utts: 13544\n",
      "Processed 86000 sentences, broca utts: 13700\n",
      "Processed 87000 sentences, broca utts: 13847\n",
      "Processed 88000 sentences, broca utts: 13981\n",
      "Processed 89000 sentences, broca utts: 14134\n",
      "Processed 90000 sentences, broca utts: 14298\n",
      "Processed 91000 sentences, broca utts: 14433\n",
      "Processed 92000 sentences, broca utts: 14617\n",
      "Processed 93000 sentences, broca utts: 14768\n",
      "Processed 94000 sentences, broca utts: 14948\n",
      "Processed 95000 sentences, broca utts: 15099\n",
      "Processed 96000 sentences, broca utts: 15253\n",
      "Processed 97000 sentences, broca utts: 15409\n",
      "Processed 98000 sentences, broca utts: 15563\n",
      "Processed 99000 sentences, broca utts: 15722\n",
      "Processed 100000 sentences, broca utts: 15891\n",
      "Processed 101000 sentences, broca utts: 16072\n",
      "Processed 102000 sentences, broca utts: 16255\n",
      "Processed 103000 sentences, broca utts: 16426\n",
      "Processed 104000 sentences, broca utts: 16594\n",
      "Processed 105000 sentences, broca utts: 16762\n",
      "Processed 106000 sentences, broca utts: 16903\n",
      "Processed 107000 sentences, broca utts: 17057\n",
      "Processed 108000 sentences, broca utts: 17229\n",
      "Processed 109000 sentences, broca utts: 17381\n",
      "Processed 110000 sentences, broca utts: 17538\n",
      "Processed 111000 sentences, broca utts: 17684\n",
      "Processed 112000 sentences, broca utts: 17810\n",
      "Processed 113000 sentences, broca utts: 17994\n",
      "Processed 114000 sentences, broca utts: 18132\n",
      "Processed 115000 sentences, broca utts: 18330\n",
      "Processed 116000 sentences, broca utts: 18501\n",
      "Processed 117000 sentences, broca utts: 18646\n",
      "Processed 118000 sentences, broca utts: 18795\n",
      "Processed 119000 sentences, broca utts: 18978\n",
      "Processed 120000 sentences, broca utts: 19143\n",
      "Processed 121000 sentences, broca utts: 19326\n",
      "Processed 122000 sentences, broca utts: 19506\n",
      "Processed 123000 sentences, broca utts: 19686\n",
      "Processed 124000 sentences, broca utts: 19865\n"
     ]
    }
   ],
   "source": [
    "aphasic_sents = []\n",
    "normal_sents = []\n",
    "new_sents = []\n",
    "\n",
    "import inflect\n",
    "from preprocess import postprocess\n",
    "p = inflect.engine()\n",
    "count = 0\n",
    "num_sents = 20000 # how many aphasic sentences?\n",
    "\n",
    "for sent in test_sents:\n",
    "    # no digits like in aphasiabank\n",
    "    # print(sent)\n",
    "    b = re.findall(\"[0-9]+\", sent)\n",
    "    for i in b:\n",
    "        sent = sent.replace(i, p.number_to_words(i))\n",
    "    \n",
    "    s = preprocess(sent)\n",
    "    \n",
    "    if len(aphasic_sents) >= num_sents:\n",
    "        changed = False\n",
    "        aphasic = \"\"\n",
    "    else:\n",
    "        changed, aphasic = aphasic_speech(s)\n",
    "        count += 1\n",
    "    \n",
    "    if len(aphasic_sents) >= num_sents and len(new_sents) >= num_sents * 2.5911:\n",
    "        break\n",
    "        \n",
    "    if count % 1000 == 0:\n",
    "        print(f\"Processed {count} sentences, broca utts: {len(aphasic_sents)}\")\n",
    "    \n",
    "    # min length is 3?\n",
    "    if changed and aphasic !=\".\" and (postprocess(s) != aphasic): #and 3 <= len(re.findall(\"[a-zA-Z_]+\", aphasic)):\n",
    "        # print(sent)\n",
    "        # print(postprocess(s))\n",
    "        # print(postprocess(aphasic))\n",
    "        # print()\n",
    "        normal_sents.append(remove_all_repetitions(postprocess(s)))\n",
    "        aphasic_sents.append(aphasic)\n",
    "    if not changed:\n",
    "        new_sents.append(remove_all_repetitions(postprocess(s)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T10:08:22.602908800Z",
     "start_time": "2024-06-07T10:01:24.729837900Z"
    }
   },
   "id": "22912266114be251"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "sentences = aphasic_sents\n",
    "original = normal_sents"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T10:08:22.618407Z",
     "start_time": "2024-06-07T10:08:22.605164500Z"
    }
   },
   "id": "48d2a87faba3ccf0"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from preprocess import postprocess\n",
    "broca_sents = []\n",
    "original_sents = []\n",
    "for sent, o in zip(sentences, original):\n",
    "    x = postprocess(sent)\n",
    "    if x != \"\":\n",
    "        broca_sents.append(x)\n",
    "        original_sents.append(o)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T10:08:22.830516200Z",
     "start_time": "2024-06-07T10:08:22.661986300Z"
    }
   },
   "id": "c51b6447aa272856"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "pd.DataFrame(data={\"modified\": broca_sents, \"original\": original_sents}).to_csv(broca_save, sep=\",\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T10:08:22.875851600Z",
     "start_time": "2024-06-07T10:08:22.835609100Z"
    }
   },
   "id": "5ad1eafde2b154dd"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "broca_sents = pd.read_csv(broca_save, sep=\",\")[\"modified\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T10:08:22.910699600Z",
     "start_time": "2024-06-07T10:08:22.877856900Z"
    }
   },
   "id": "46bc092a7122b513"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "broca_sents = broca_sents[:num_sents]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T10:08:22.921796600Z",
     "start_time": "2024-06-07T10:08:22.907421200Z"
    }
   },
   "id": "9bd0b1671dcaa5d1"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "51503"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(len(broca_sents)*2.5911)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T10:08:22.965681600Z",
     "start_time": "2024-06-07T10:08:22.923796300Z"
    }
   },
   "id": "4b54c89b88e9dd27"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "control_sents = new_sents[:round(len(broca_sents)*2.5911)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T10:08:22.966685500Z",
     "start_time": "2024-06-07T10:08:22.939531400Z"
    }
   },
   "id": "8ec64bced1906d27"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "broca_data = pd.DataFrame(data={\"preprocessed_text\": broca_sents, \"label\": [1]*len(broca_sents)})\n",
    "control_data = pd.DataFrame(data={\"preprocessed_text\": control_sents, \"label\": [0]*len(control_sents)})\n",
    "data_full_scenario = pd.concat([broca_data, control_data], ignore_index=True)\n",
    "data_full_scenario = data_full_scenario.sample(frac=1).reset_index(drop=True)\n",
    "data_full_scenario[\"preprocessed_text\"] = [re.sub(r'[^\\w\\s]','',x) for x in data_full_scenario[\"preprocessed_text\"]]\n",
    "data_full_scenario.to_csv(generated_save, sep=\",\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T10:08:23.134931200Z",
     "start_time": "2024-06-07T10:08:22.958684700Z"
    }
   },
   "id": "6b6635d33f70b4a8"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "                                       preprocessed_text  label\n0      and then there were two girls that fancied him...      0\n1                                yeah they are nice yeah      0\n2                         mm yeah it is not strong is it      0\n3                                          it is mums go      1\n4      he is his fathers son you see in certain eleme...      0\n...                                                  ...    ...\n71375                                           not sure      0\n71376                            i do not like up fusses      1\n71377                                   i do not it that      1\n71378                                          mm yes it      0\n71379  and i have literally just like it is just got ...      0\n\n[71380 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>preprocessed_text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>and then there were two girls that fancied him...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>yeah they are nice yeah</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>mm yeah it is not strong is it</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>it is mums go</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>he is his fathers son you see in certain eleme...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>71375</th>\n      <td>not sure</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>71376</th>\n      <td>i do not like up fusses</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>71377</th>\n      <td>i do not it that</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>71378</th>\n      <td>mm yes it</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>71379</th>\n      <td>and i have literally just like it is just got ...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>71380 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_full_scenario"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-07T10:08:23.157367Z",
     "start_time": "2024-06-07T10:08:23.140453900Z"
    }
   },
   "id": "7675b36856541d1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
