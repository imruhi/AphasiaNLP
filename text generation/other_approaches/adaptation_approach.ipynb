{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Based on results from Adaptation theory and non-fluent aphasia in english by Salis and Edwards (2004)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T16:47:02.192827400Z",
     "start_time": "2024-02-29T16:47:02.174665100Z"
    }
   },
   "id": "18232d89896a30ae"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "from pattern.en import conjugate, lemma, lexeme, PRESENT, SG\n",
    "from spacy.matcher import Matcher\n",
    "from datasets import load_dataset\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "from preprocess import preprocess, postprocess"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T16:47:02.618680900Z",
     "start_time": "2024-02-29T16:47:02.177171100Z"
    }
   },
   "id": "f237ea2055319fba"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "dets = {'Art': ['a', 'an', 'the', ''],\n",
    "           'Dem': ['this', 'that', 'these', 'those', ''],\n",
    "           'Poss': ['my', 'your', 'his', 'her', 'its', 'our', 'their', '']}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T16:47:02.634287100Z",
     "start_time": "2024-02-29T16:47:02.618680900Z"
    }
   },
   "id": "a8681817efd09f21"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def det_sub(det):\n",
    "    for _, detrms in dets.items():\n",
    "        if det.lower() in detrms:\n",
    "            y = [j for j in detrms if det!=j]\n",
    "            return random.choice(y)\n",
    "    return \"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T16:47:02.649911100Z",
     "start_time": "2024-02-29T16:47:02.634287100Z"
    }
   },
   "id": "66f4ce3548abe7ee"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def handle_determiner(tok):\n",
    "    x = np.random.uniform(0,1)\n",
    "    utt = tok.text + \" \"\n",
    "    \n",
    "    # 3% of determiners were substituted\n",
    "    if x >= 0.97:\n",
    "        if tok.pos_ == \"DET\" or \"Dem\" in tok.morph.get('PronType') or \"Yes\" in tok.morph.get('Poss'):\n",
    "            utt = det_sub(tok.text) + \" \"\n",
    "        \n",
    "    # 19% determiners were omitted\n",
    "    if x >= 0.81:\n",
    "        utt = \" \"\n",
    "        \n",
    "    return utt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T16:47:02.665532800Z",
     "start_time": "2024-02-29T16:47:02.649911100Z"
    }
   },
   "id": "54c008b5de8d4253"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def handle_verb(tok):\n",
    "    x = np.random.uniform(0,1)\n",
    "    utt = tok.text + \" \"\n",
    "\n",
    "    # 5% of copula were substituted\n",
    "    if x >= 0.95 and tok.pos_ == \"AUX\" and tok.dep_ == \"ROOT\":\n",
    "        utt = conjugate(verb=tok.text,tense=PRESENT,number=SG) + \" \" \n",
    "        \n",
    "    # 6% of lexical bound morphemes were removed\n",
    "    if x >= 0.94 and tok.pos_ == \"VERB\":\n",
    "        utt = tok.lemma_ + \" \"\n",
    "    \n",
    "    # 8% of auxiliary verbs were substituted\n",
    "    if x >= 0.92 and tok.pos_ == \"AUX\" and tok.dep_ == \"aux\":\n",
    "        utt = conjugate(verb=tok.text,tense=PRESENT,number=SG) + \" \" \n",
    "        \n",
    "    # 12% of lexical verbs were substituted (tense error) \n",
    "    if x >= 0.88 and tok.pos_ == \"VERB\":\n",
    "        utt = conjugate(verb=tok.text,tense=PRESENT,number=SG) + \" \" \n",
    "    \n",
    "    # 14% of auxiliary verbs were omitted\n",
    "    if x >= 0.86 and tok.pos_ == \"AUX\" and tok.dep_ == \"aux\":\n",
    "        utt = \" \" \n",
    "    \n",
    "    # 17% verbs were omitted\n",
    "    if x >= 0.83:\n",
    "        utt = \" \"\n",
    "    return utt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T16:47:02.685784300Z",
     "start_time": "2024-02-29T16:47:02.665532800Z"
    }
   },
   "id": "68fa2e55a62234fb"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def handle_preposition(tok):\n",
    "    x = np.random.uniform(0,1)\n",
    "    utt = tok.text + \" \"\n",
    "    # 2% of prepositions were substituted\n",
    "    if x >= 0.98:\n",
    "        # TODO substitute\n",
    "        pass\n",
    "    # 37% of prepositions were omitted\n",
    "    if x >= 0.63:\n",
    "        utt = \" \"\n",
    "    return utt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T16:47:02.698198Z",
     "start_time": "2024-02-29T16:47:02.681158400Z"
    }
   },
   "id": "ac336a16bcbfa7ee"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def handle_person_pron(tok):\n",
    "    utt = tok.text + \" \"\n",
    "    x = np.random.uniform(0,1)\n",
    "    # 27% of personal pronouns were omitted\n",
    "    if x >= 0.73:\n",
    "        utt = \" \"\n",
    "    return utt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T16:47:02.715676700Z",
     "start_time": "2024-02-29T16:47:02.698198Z"
    }
   },
   "id": "63c9ea6356f3bc60"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def aphasic_speech(sentence):\n",
    "    vp_pattern = [[{'POS': 'VERB', 'OP': '?'},\n",
    "                   {'POS': 'ADV', 'OP': '*'},\n",
    "                   {'POS': 'AUX', 'OP': '*'},\n",
    "                   {'POS': 'VERB', 'OP': '+'}]]\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add(\"Verb phrase\", vp_pattern)\n",
    "    n = 15\n",
    "    aphasic_utt = \"\"\n",
    "    doc = nlp(sentence)\n",
    "    # print(sentence, len(sentence.split()))\n",
    "    if len(sentence.split()) <= n:\n",
    "        # get NPs\n",
    "        noun_phrases = set()\n",
    "\n",
    "        for nc in doc.noun_chunks:\n",
    "            for nop in [nc, doc[nc.root.left_edge.i:nc.root.right_edge.i + 1]]:\n",
    "                noun_phrases.add(nop.text.strip())\n",
    "                # get VPs\n",
    "        verb_phrases = matcher(doc)\n",
    "        verb_phrases = [doc[start:end] for _, start, end in verb_phrases]\n",
    "\n",
    "        try:\n",
    "            ratio = len(noun_phrases) / len(verb_phrases)\n",
    "        except:\n",
    "            # print(\"Division by zero\")\n",
    "            return aphasic_utt\n",
    "\n",
    "        X = np.random.uniform(0, 1)\n",
    "        # print(ratio, X)\n",
    "        if ratio > 2 and X <= 0.8:\n",
    "            # skip sentence\n",
    "            return aphasic_utt\n",
    "    \n",
    "        else:\n",
    "            # don't skip sentence\n",
    "            for tok in doc:\n",
    "                if tok.pos_ in [\"DET\", \"PRON\"]:\n",
    "                    aphasic_utt += handle_determiner(tok)      \n",
    "                elif tok.pos_ in [\"VERB\", \"AUX\"]:\n",
    "                    aphasic_utt += handle_verb(tok)\n",
    "                elif tok.dep_ == \"prep\" or tok.pos_ == \"ADP\":\n",
    "                    aphasic_utt += handle_preposition(tok)\n",
    "                elif tok.morph.get('Case') == 'Nom' and tok.morph.get('Person') == 1:\n",
    "                    aphasic_utt += handle_person_pron(tok)\n",
    "                else:\n",
    "                    aphasic_utt += tok.text + \" \"\n",
    "    \n",
    "    return aphasic_utt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T16:47:02.729687800Z",
     "start_time": "2024-02-29T16:47:02.715676700Z"
    }
   },
   "id": "908fae5d5c2840bd"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting data\n",
      "Sentences retained after augmentation: 2884.4444444444443\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"imdb\")\n",
    "texts = ds[\"train\"][\"text\"]\n",
    "sents = []\n",
    "save_path = \"data/synthetic_salis.csv\"\n",
    "\n",
    "for text in texts:\n",
    "    t = re.sub(r'\\<.*?\\>', \" \", text)\n",
    "    sentences = sent_tokenize(t)\n",
    "    for sent in sentences:\n",
    "        sent = preprocess(sent)\n",
    "        sents.append(\" \".join(sent.split()))\n",
    "\n",
    "broca_ds = sents[500:1000]\n",
    "control_ds = sents[1000:1500]\n",
    "\n",
    "print(\"Augmenting data\")\n",
    "\n",
    "augmented_sentences = []\n",
    "control_sentences = []\n",
    "\n",
    "for x in broca_ds:\n",
    "    broca_sentence = aphasic_speech(x)\n",
    "    if isinstance(broca_sentence, str):\n",
    "        broca_sentence = postprocess(broca_sentence)\n",
    "        if broca_sentence:\n",
    "            augmented_sentences.append(broca_sentence)\n",
    "\n",
    "for x in control_ds:\n",
    "    control_sentence = postprocess(x)\n",
    "    if control_sentence:\n",
    "        control_sentences.append(control_sentence)\n",
    "\n",
    "broca_data = pd.DataFrame(data={\"modified\": augmented_sentences, \"label\": [1] * len(augmented_sentences)})\n",
    "control_data = pd.DataFrame(data={\"modified\": control_sentences, \"label\": [0] * len(control_sentences)})\n",
    "data_full_scenario = pd.concat([broca_data, control_data], ignore_index=True)\n",
    "data_full_scenario = data_full_scenario.sample(frac=1).reset_index(drop=True)\n",
    "data_full_scenario.to_csv(save_path, sep=\",\", index=False)\n",
    "\n",
    "print(f\"Sentences retained after augmentation: {len(sents) / len(augmented_sentences)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T16:47:30.954251400Z",
     "start_time": "2024-02-29T16:47:02.729687800Z"
    }
   },
   "id": "7cd19f299b26d5ce"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
